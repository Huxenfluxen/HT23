% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{multirow}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\usepackage[normalem]{ulem}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\includegraphics[width=1.18056in,height=1.39583in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image1.png}

AN INTRODUCTION TO

MATHEMATICAL SYSTEMS THEORY

Lecture notes by

Anders Lindquist and Janne Sand

revised by Xiaoming Hu

Optimization and Systems Theory

Royal institute of technology

SE-100 44 Stockholm, Sweden

\textbf{Contents}

\begin{quote}
Preface v

Chapter 1. Linear Control Systems 1

1.1. Introduction 1

1.2. Input-output Description of a System 2

1.3. State Space Description of a System 3

1.4. The Concept of State in Mathematical Systems Theory\emph{∗} 5

Chapter 2. Linear Dynamical Systems 9

2.1. Solutions of linear differential equations 9

2.2. Time-invariant systems 12

2.3. Systems of linear difference equations 13

Chapter 3. Reachability and Observability 15

3.1. Reachability 15

3.2. Reachability for Time-Invariant Systems 19

3.3. Reachability for Discrete-Time Systems 22

3.4. Observability 23

3.5. Observability for Time-Invariant Systems 25

3.6. Observability for Discrete-Time Systems 27

3.7. Duality between reachability and observability 27

Chapter 4. Stability 29

4.1. Stability of a dynamical system 29

4.2. Input-output stability 31

4.3. The Lyapunov equation 32

4.4. Stability of discrete-time systems 35

Chapter 5. Realization theory 37

5.1. Realizability and rationality 38

5.2. Minimality and McMillan degree 44

5.3. Characteristic polynomial and minimal realization 49

5.4. Ho's algorithm1 50

Chapter 6. State Feedback and Observers 55

6.1. Feedback with Complete State Information 55

6.2. Observers 61

Chapter 7. Linear-Quadratic Optimal Control 65
\end{quote}

iii

\begin{quote}
iv CONTENTS

7.1. Linear-Quadratic regulator 65

7.2. Solving the Riccati equation 69

7.3. Fixed end-point problems 71

Chapter 8. LQ Control over Infinite Time Interval and ARE 73

8.1. Existence of a positive definite solution 73

8.2. The optimal control law and the question of uniqueness 76

Chapter 9. Kalman Filtering 81

9.1. The discrete-time filter 82

9.2. The continuous-time Kalman filter 93

Index 101
\end{quote}

\textbf{Preface}

\begin{quote}
This is a set of lecture notes for the course ``Mathematical Systems
The-ory'' given at Kungliga Tekniska H¨ogskolan, Stockholm. The
compendium is initially written by Anders Lindquist and Janne Sand, and
is partially revised by Xiaoming Hu in 2010. Some minor updates are made
constantly.
\end{quote}

v

\includegraphics[width=1.72222in,height=0.25in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image2.png}

CHAPTER 1

\textbf{Linear Control Systems}

\textbf{1.1. Introduction}

\begin{quote}
``System'' as defined in the Webster is ``a regularly interacting or
inter-dependent group of items forming a unified whole''. In this course
the class of systems we study typically has some input signals and
output signals as shown below.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
Σ
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
y
\end{minipage} \\
\begin{minipage}[b]{\linewidth}\raggedright
input
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
output
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
When an input \emph{u} is applied to the system Σ, we assume that a
\emph{unique} output \emph{y} is produced. Note that the uniqueness of
the output is an essential property of the systems we will study. A
system is called a single-input single-output (SISO) system if both the
input and the output are scalars. Otherwise it is called a multi-input
multi-output (MIMO) system.

Now let us assume that there is a given set \emph{T} of times which is
either continuous (\emph{T ⊂} R and the system is called a
\emph{continuous-time system} ac-accordingly), \emph{u} and \emph{y} are
vector-valued functions of time belonging to vector cordingly) or
discrete (\emph{T ⊂} Z and the system is called a \emph{discrete-time
system} spaces U and Y, and the box that defines the system is a
function \emph{f}Σ : U \emph{→} Y. A system is called \emph{memoryless}
if its output signal at time \emph{t} depends only on the input signal
applied at time \emph{t}. However, most practical engineering systems do
have memory. Thus if an input \emph{u} is applied to the system over
{[}\emph{t, ∞}{]}, the output \emph{y} is in general not determinable
unless we know the input applied before \emph{t}. As we will see later,
the key properties of the system will be the same (provided it is a
linear system!) no matter what past input was applied to the system.
Thus in the input-output modeling of a system, we assume that the system
is \emph{relaxed} before any input is applied, namely \emph{f}Σ(0) = 0.

The system is called a \emph{linear} system if
\end{quote}

\emph{f}Σ(\emph{αu}1 + \emph{βu}2) = \emph{αf}Σ(\emph{u}1) +
\emph{βf}Σ(\emph{u}2)

\begin{quote}
for all \emph{u}1\emph{, u}2 \emph{∈} U and \emph{α, β ∈} R
(superposition), and \emph{causal} if\\
\emph{u}1(\emph{t}) = \emph{u}2(\emph{t}) for \emph{t \textless{} t}1
=\emph{⇒} \emph{f}Σ(\emph{u}1) = \emph{f}Σ(\emph{u}2) for \emph{t
\textless{} t}1\emph{,}

namely the current output depends only on the past and current input,
but not the future input.
\end{quote}

1

\begin{quote}
2 1. LINEAR CONTROL SYSTEMS
\end{quote}

\textbf{1.2. Input-output Description of a System}

\begin{quote}
A suitable model for continuous-time linear systems is

(1) \emph{y}(\emph{t}) =∫ \emph{t G}(\emph{t,
s})\emph{u}(\emph{s})\emph{ds} + \emph{D}(\emph{t})\emph{u}(\emph{t})

where the functions \emph{G} and \emph{D} take values in the space
R\emph{m×k}of (real) \emph{m × k} matrices. The set \emph{T} of times
could be one of the intervals {[}\emph{t}0\emph{, t}1{]},
{[}\emph{t}0\emph{, ∞}), or, setting \emph{t}0 = \emph{−∞}, (\emph{−∞,
t}1{]}, or (\emph{−∞, ∞}). Since \emph{G}(\emph{t, s}) is the output
response to the impulse input, it is called the \emph{impulse response}.
Obviously being causal implies that \emph{G}(\emph{t, s}) = 0\emph{, ∀t
\textless{} s} and being memoryless implies that \emph{G}(\emph{t, s})
\emph{≡} 0. A linear system Σ is called \emph{time invariant} if

\emph{y} = \emph{f}Σ(\emph{u}) =\emph{⇒} \emph{yT} = \emph{f}Σ(\emph{uT}
)

where \emph{wT} denotes the shift function: \emph{wT} (\emph{t}) =
\emph{w}(\emph{t − T})\emph{, t ≥ t}0 + \emph{T} and \emph{wT}
(\emph{t}) = 0\emph{, t \textless{} t}0 + \emph{T}. In this case,

\emph{f}Σ(\emph{uT} ) =∫ \emph{t G}(\emph{t, s})\emph{uT}
(\emph{s})\emph{ds} + \emph{D}(\emph{t})\emph{u}(\emph{t − T})

(2) =∫ \emph{t−T} \emph{G}(\emph{t, r} + \emph{T})\emph{uT} (\emph{r} +
\emph{T})\emph{dr} + \emph{D}(\emph{t})\emph{u}(\emph{t − T})

=∫ \emph{t−T} \emph{G}(\emph{t, r} +
\emph{T})\emph{u}(\emph{r})\emph{dr} +
\emph{D}(\emph{t})\emph{u}(\emph{t − T})

Comparing this with (1), time invariance requires that

\emph{G}(\emph{t, s} + \emph{T}) = \emph{G}(\emph{t − T, s})\emph{,
D}(\emph{t}) = \emph{D}(\emph{t − T})\emph{, ∀T ≥} 0\emph{.} It is easy
to see that this will be satisfied if
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
and \emph{D} is constant.
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{G}(\emph{t, s}) = \emph{G}(\emph{t − s,} 0)
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
It is known in the literature that in order to obtain a finite
dimensional internal description of the black box, which shall be our
next topic, the function \emph{G} must be factorizable as

(3) \emph{G}(\emph{t, s}) = \emph{H}(\emph{t})\emph{K}(\emph{s})

where \emph{H} and \emph{K} take values in R\emph{m×n}and
R\emph{n×k}respectively for some integer \emph{n}. One problem with this
description is that it does not allow for modeling of so-called
\emph{autonomous dynamics} producing a nonzero output even when the
input is identically zero, which would be easily taken care by the state
space description as well.

The above discussion can be easily adopted to discrete-time linear
sys-tems which can be modeled as
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(4)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{y}(\emph{t}) =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{t}\\
∑\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{G}(\emph{t, s})\emph{u}(\emph{s}) +
\emph{D}(\emph{t})\emph{u}(\emph{t})
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
1.3. STATE SPACE DESCRIPTION OF A SYSTEM 3
\end{quote}

\textbf{1.3. State Space Description of a System}

\begin{quote}
The basic concept of systems theory is that of \emph{state}. Loosely
speaking, at each time \emph{t}, we would like to collect all the
\emph{current} information that is relevant for future outputs and is
represented by a so-called state vector

If at any time \emph{t x}(\emph{t}) together with the current input
\emph{u}(\emph{t}) uniquely deter- \emph{x}(\emph{t}) =
\emph{xn}(\emph{t}) \emph{x}1(\emph{t}) \emph{x}2(\emph{t}) ... 
 \emph{.}

mines the output \emph{y}(\emph{t}), the time evolution of \emph{x}
defines an \emph{internal} description of the black box. If this can be
done so that the number \emph{n} of state vari-ables, \emph{x}1\emph{,
x}2\emph{, . . . , xn}, is finite, we say that the system is
finite-dimensional, or lumped. Condition (3) is precisely what is
required for this. In fact, as we shall see in Chapter 5, condition (3)
allows us to describe the system Σ in \emph{state space form}, i.e. as
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(5)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
˙\emph{x}(\emph{t}) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{u}(\emph{t})\\
\emph{y}(\emph{t}) = \emph{C}(\emph{t})\emph{x}(\emph{t}) +
\emph{D}(\emph{t})\emph{u}(\emph{t})
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
in continuous time, and
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(6)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t} + 1) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{u}(\emph{t}) \emph{y}(\emph{t}) =
\emph{C}(\emph{t})\emph{x}(\emph{t}) +
\emph{D}(\emph{t})\emph{u}(\emph{t})
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
in discrete time. Here \emph{A}, \emph{B}, \emph{C}, \emph{D} take
values in R\emph{n×n}, R\emph{n×k}, R\emph{m×n}and
R\emph{m×k}respectively, and it is easy to see that the system Σ is time
invariant if and only if \emph{A},\emph{B}, \emph{C} and \emph{D} are
constant matrices. The problem to determine (5) or (6) from (1) and (4)
respectively is the so-called \emph{realization problem} to be discussed
in Chapter 5.

Example 1.3.1. A particle of mass 1 moves along the \emph{y}-axis under
in-
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
fluence of the force \emph{u}(\emph{t}) for \emph{t ≥} 0. 1 u
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
y
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\includegraphics[width=2.65278in,height=\textheight]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image3.png}

\begin{quote}
Newton's law yields
\end{quote}

¨\emph{y} = \emph{u}

\begin{quote}
so the external description (1) of the system with input \emph{u} and
output \emph{y} is
\end{quote}

\emph{y}(\emph{t}) =∫ \emph{t} (\emph{t − s})\emph{u}(\emph{s})\emph{ds}

i.e. \emph{G}(\emph{t, s}) = \emph{t − s}. The system is time invariant.
To obtain an internal description define the state

\begin{quote}
\emph{x} = {[}˙\emph{y} {]}\\
\emph{y}
\end{quote}

\includegraphics[width=2.04167in,height=1.75in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image7.png}

\begin{quote}
4 1. LINEAR CONTROL SYSTEMS

Then ˙\emph{x}1 = \emph{x}2 and ˙\emph{x}2 = \emph{u}, i.e.\\
 ˙\emph{x} =

\emph{y} = (1\emph{,} 0)\emph{x}\\
{[} 0 0 1 0 {]} \emph{x} + {[} 0 1 {]} \emph{u}

However, we could also have chosen the state

\emph{x} = {[} \emph{y −} ˙\emph{y} {]}\\
\emph{y} + ˙\emph{y}

yielding the system\\
 ˙\emph{x} =

\emph{y} = (1\emph{/}2\emph{,} 1\emph{/}2)\emph{x}\\
{[} 1\emph{/}2 1\emph{/}2 \emph{−}1\emph{/}2 \emph{−}1\emph{/}2 {]}
\emph{x} + {[}\emph{−}1 1 {]} \emph{u}

or in infinitely many other ways. The relation between these internal
descrip-\\
tions is merely a simple change of coordinates in the state space
\emph{X} = R2. □

Example 1.3.2. Describe the electrical network in the figure as a linear
control system with the voltage \emph{u} as an input and current
\emph{y} as an output.
\end{quote}

y

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
x
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
1
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
R
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

R

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
L
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
x2
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
C
\end{quote}
\end{minipage} \\
& & & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Cx 2
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\textbf{Figure 1.1}

\begin{quote}
From the theory of electricity we know that the elements in Figure 1.1
produce the dynamical behaviors of the following chart
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule()
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
v
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Resistor
\end{quote}
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
v
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Inductor
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
v
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Capacitor
\end{quote}
\end{minipage} \\
& \multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\includegraphics[width=0.36111in,height=1.19444in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image4.png}R
\end{quote}

i
\end{minipage}} & & & &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\includegraphics[width=0.375in,height=1.19444in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image5.png}i

Cdv dt = i
\end{quote}
\end{minipage}} \\
& & & \multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\includegraphics[width=0.33333in,height=1.19444in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image6.png}i

v=L di
\end{quote}

dt
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
v=Ri
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\includegraphics[width=1.45833in,height=1.44444in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image9.png}

\begin{quote}
1.4. THE CONCEPT OF STATE 5
\end{quote}

Therefore, choosing state variables as in Figure 1.1, we obtain the
state

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
space description

for the system. Clearly the system is time invariant. Let us, for
simplicity 

\\
˙\emph{x} =

\emph{y} = (1\emph{, −}1\\
{[}\emph{−R}
\end{quote}

0

\begin{quote}
\emph{R})\emph{x} + 1\\
\emph{−}1\\
0
\end{quote}

\emph{RC}

\begin{quote}
\emph{Ru}\\
{]} \emph{x} + {[} \emph{RC} \emph{L}
\end{quote}

\uline{1}

\begin{quote}
1 {]} \emph{u}
\end{quote}

and without regard to physical reasonability, set \emph{R} = \emph{L} =
\emph{C} = 1. Then, the\strut
\end{minipage}} \\
\midrule()
\endhead
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
system becomes

(7)

that is,

and
\end{quote}
\end{minipage} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
˙\emph{x} =

\emph{y} = (1\emph{, −}1)\emph{x} + \emph{u}\\
{[}\emph{−}1

0 \emph{−}1 0 {]} \emph{x} + {[} 1
\end{quote}

1 {]} \emph{u}

\begin{quote}
\emph{x}1(\emph{t}) = \emph{x}2(\emph{t}) =∫
\emph{t−∞e−}(\emph{t−s})\emph{u}(\emph{s})\emph{ds}
\end{quote}\strut
\end{minipage} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
\emph{y} = \emph{x}1 \emph{− x}2 + \emph{u} = \emph{u.}

Consequently, \emph{G}(\emph{t, s}) \emph{≡} 0 and \emph{D} = 1, and
hence the two-dimensional exter-nal description (7) can be replaced by
the zero-dimensional system} \\
\bottomrule()
\end{longtable}

\emph{y} = \emph{u}

\begin{quote}
A natural question then is: what is wrong with the representation (7)?
The answer is that the dynamics is confined to the subspace
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{M} = \emph{\{x ∈} R2\emph{\textbar{} x}1 = \emph{x}2\emph{\}}
\end{minipage} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
x2
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
x1
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
of the state space \emph{X} = R2. We say that Σ is not
\end{quote}
\end{minipage} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{completely reachability}. Similarly, we cannot ob-
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
serve what is going on in the subspace \emph{M}, i.e.\\
Σ is not \emph{completely observable}. With a termi-\\
nology to be established in Chapter 4, we say\\
that \emph{M} is both the reachable and unobservable\\
subspace of \emph{X}. □
\end{quote}

\textbf{1.4. The Concept of State in Mathematical Systems
Theory}\emph{∗}

\begin{quote}
To spread some further light on the abstract concept of state, let us
consider the following experiment.

\emph{Kalman's experiment:} Control the system
\end{quote}

\includegraphics[width=1.72222in,height=0.25in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image8.png}

\begin{quote}
6 1. LINEAR CONTROL SYSTEMS

up to time \emph{t} (after which the control \emph{u} is identically
zero) and then observe the resulting output after time \emph{t}.

For the purpose of this discussion, define \emph{U} and \emph{Y} to be
the (possibly infinite-dimensional) vector spaces of the inputs
respectively the outputs admissible in this experiment. Then Kalman's
experiment defines a linear mapping
\end{quote}

\emph{F} : \emph{U → Y.}

\begin{quote}
The basic idea of the state space construction is to determine a
\emph{state space X}, preferably of finite dimension, over which
\emph{F} can be factored as in the commutative diagram

\emph{F}\\
\emph{U−→ Y}\\
(8) R \emph{↘ ↗}O\\
\emph{X}
\end{quote}

In other words, find a state space \emph{X} and two maps R : \emph{U →
X} and O : \emph{X →Y} such that

\emph{F} = OR\emph{.}

\begin{quote}
If, for example, we choose \emph{X} := \emph{U} so that R = \emph{I}
(identity) and O = \emph{F}, we have a valid factorization, but it is of
little use, since we obtain no data reduction. Indeed, we need to
remember the whole past history of the input. The state space may not
even be finite-dimensional.

Therefore, let us reduce the state space by identifying inputs which
produce the same output. Hence, we say that \emph{u}1\emph{, u}2 \emph{∈
U} are equivalent (\emph{u}1 \emph{∼ u}2) if \emph{F}(\emph{u}1) =
\emph{F}(\emph{u}2). Then define \emph{X} to be the set of all
equivalence classes under this equivalence. In algebraic terms this is a
quotient space denoted \emph{X} = \emph{U/ ∼}, and, in the present
context, a vector space. Using this \emph{X} as a state space it is
clear that the factorization (8) has the following properties.

(1) R is \emph{surjective} (onto), i.e. Im R = \emph{X}\\
(2) O is \emph{injective} (one-one), i.e. O\emph{x}1 = O\emph{x}2
=\emph{⇒ x}1 = \emph{x}2, or equiva-lently, ker O = 0.

Remark 1.4.1. We recall that for a linear map \emph{A} : \emph{X → Y} ,
the \emph{range space}(or image) Im \emph{A} is defined as \emph{\{Ax
\textbar{} x ∈ X\}} and the \emph{kernel} ker \emph{A} as \emph{\{x ∈ X
\textbar{} Ax} = 0\emph{\}}. □

A state space representation is said to be \emph{completely reachable}
if prop-erty (1) holds and \emph{completely observable} if property (2)
holds.

It is not hard to convince oneself that such a construction leads to a
state space which is \emph{minimal} in the sense of subspace inclusion,
i.e. \emph{X} contains no proper subspace which can also serve as a
state space. This fact can be illustrated (in Venn diagram form) by
Figure 1.2.
\end{quote}

\includegraphics[width=2.95833in,height=1.86111in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image11.png}

\begin{quote}
1.4. THE CONCEPT OF STATE 7

F\\
Im F
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
U
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Im R
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
O
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Y
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

R

X

ker O (shaded)

\textbf{Figure 1.2}

\begin{quote}
In Figure 1.2 we discard from \emph{X} the part which is not in Im R; it
serves no purpose, since it corresponds to unreachable states. Next,
from the remaining state space we discard the unobservable part ker O,
choosing as the final state space only its complement, the black part of
\emph{X}. (Beware of the fact that the Venn diagram representation is
just an illustration and could be misleading! How?) The remaining
\emph{X} satisfies both property (1) and property (2). Compare this with
the discussion in Example 1.3.2.

Next, let us illustrate this by means of a time invariant system in
discrete
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
time, namely
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\{ \emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t})

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t})
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Time invariance implies that the state space construction is identical
for all \emph{t}. Hence, choose the last time of control to be \emph{t}
= \emph{−}1 and first time of observation to be \emph{t} = 0:\\
PAST FUTURE
\end{quote}

\includegraphics[width=1.23611in,height=0.26389in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image10.png}t

\begin{quote}
control observation

Then, the map \emph{F} : \emph{U → Y} can be describe as

i.e. \emph{F} has a matrix representation which is a block \emph{Hankel}
matrix. Now   \emph{y}(0) \emph{y}(1) \emph{y}(2) ...   = 
\\
\emph{CA}2\emph{B} \emph{CAB} \emph{CB}
\end{quote}

...

\begin{quote}
\emph{CA}2\emph{B}\\
\emph{CA}3\emph{B}\\
\emph{CAB}
\end{quote}

...

\begin{quote}
\emph{CA}3\emph{B}\\
\emph{CA}4\emph{B}\\
\emph{CA}2\emph{B}

... \emph{· · ·} \emph{· · ·} \emph{· · ·}    
\emph{u}(\emph{−}1) \emph{u}(\emph{−}2) \emph{u}(\emph{−}3) ...  

since   \emph{CA}2\emph{B} \emph{CAB} \emph{CB}

...

\emph{CA}2\emph{B}\\
\emph{CA}3\emph{B}\\
\emph{CAB}
\end{quote}

...

\begin{quote}
\emph{CA}3\emph{B}\\
\emph{CA}4\emph{B}\\
\emph{CA}2\emph{B}

... \emph{· · ·} \emph{· · ·} \emph{· · ·}   =   \emph{CA}2
\emph{CA} \emph{C}

...  {[}\emph{B} \emph{AB} \emph{A}2\emph{B} \emph{· · ·}{]}

8 1. LINEAR CONTROL SYSTEMS

which we can identify as the factorization
\end{quote}

\emph{F} = OR\emph{.}

\begin{quote}
In fact,
\end{quote}

\emph{x}(0) = R\emph{u}(\emph{−}1)

\emph{u}(\emph{−}2)

\begin{quote}
In order to insure that \emph{X} = R\emph{n}is minimal, we should
choose\emph{u}(\emph{−}3)

...  \emph{.}
\end{quote}

R = {[}\emph{B, AB, A}2\emph{B, · · ·} {]}

\begin{quote}
surjective and
\end{quote}

O =\emph{CA}2

\emph{CA}

\emph{C}

\begin{quote}
injective. This amounts to requiring that both R and O are full rank.
This ... 
\end{quote}

whole matter will be discussed in detail in Chapter 3 and Chapter 5. The

reader is encouraged to return to the discussion above while studying
Chap-

ter 3 and Chapter 5 to reinforce his/her understanding of the concept of

\begin{quote}
state.
\end{quote}

CHAPTER 2

\textbf{Linear Dynamical Systems}

\textbf{2.1. Solutions of linear differential equations}

\begin{quote}
Let us consider an n-dimensional linear time-varying control system:
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(9)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
˙\emph{x} = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{u}(\emph{t})\\
\emph{y} = \emph{C}(\emph{t})\emph{x}(\emph{t}) +
\emph{D}(\emph{t})\emph{u}(\emph{t})\emph{,}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

where \emph{x, u, y} are the state, input and output respectively, and
\emph{A, B, C, D} are

respectively \emph{n× n, n × m, p ×n, p× m} matrices whose entries are
continuous

\begin{quote}
functions of time \emph{t} over (\emph{−∞, ∞}). Consider first the
homogeneous system

(10) ˙\emph{x}(\emph{t}) = \emph{A}(\emph{t})\emph{x}(\emph{t})
\end{quote}

One can show that (10) has a unique solution for every initial state
\emph{x}(\emph{t}0) = \emph{a}

on every bounded interval containing \emph{t}0 and for all \emph{a ∈}
R\emph{n}. Then the set of solutions forms a linear space and the
dimension of the space is \emph{n}.

\begin{quote}
Proposition 2.1.1. \emph{The set of all solutions of} (10) \emph{is a
linear space of}

\emph{dimension n over the real field.}
\end{quote}

\textbf{Proof:} Let Φ\emph{k}(\emph{t, t}0), \emph{k} = 1\emph{,}
2\emph{, . . . , n}, be the unique solutions of (10) with

\begin{quote}
linearly independent initial conditions

0 1 0\\
1 0 0

If we can show every solution of (10) can be expressed as a linear
combination \emph{x}(\emph{t}0) = \\
0

0 ... \emph{, x}(\emph{t}0) = \\
0

0 ...\emph{, · · · , x}(\emph{t}0) =\\
0

1 ...\\
\emph{.}
\end{quote}

of Φ\emph{k}(\emph{t, t}0), then the proposition is proven. Define the
\emph{n×n} transition matrix

Φ(\emph{t, t}0) = {[}Φ1(\emph{t, t}0)\emph{,} Φ2(\emph{t, t}))\emph{, .
. . ,} Φ\emph{n}(\emph{t, t}0){]}\emph{.}

We claim that Φ(\emph{t, t}0) is nonsingular for all \emph{t}. This can
be easily seen by the

fact that if Φ(\emph{t, t}0) is singular at some time \emph{t}1, it
would remain singular for

all finite \emph{t}. Otherwise the uniqueness property of the solution
(with respect

\begin{quote}
to a given initial state) will not hold.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Then we have
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\{ Φ(\emph{s, s}) = \emph{I} \emph{∂t}(\emph{t, s}) =
\emph{A}(\emph{t})Φ(\emph{t, s}) \emph{∂}Φ
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

9

\begin{quote}
10 2. LINEAR DYNAMICAL SYSTEMS

Since\\
1 0 0 1 0 0

by the superposition principle, the system (10) with \emph{x}(\emph{t}0)
= \emph{a} has the \emph{a} = \emph{a}1 \\
0

0 ... + \emph{a}2 \\
0

0 ...+ \emph{· · ·} + \emph{an}\\
0

1 ...\\
\emph{,}

solution
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
that is
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t}) = \emph{a}1Φ1(\emph{t, t}0) + \emph{a}2Φ2(\emph{t,
t}0) + \emph{· · ·} + \emph{an}Φ\emph{n}(\emph{t, t}0)
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\emph{x}(\emph{t}) = Φ(\emph{t, t}0)\emph{a.}\\
□

In the above derivation we have chosen some special initial states to
obtain Φ(\emph{t, t}0). Now suppose Ψ(\emph{t}) be a matrix whose
columns are linearly independent solutions to (10). Then the solution
\emph{x}(\emph{t}) with \emph{x}(\emph{t}0) = \emph{a} can be
expressed\\
\emph{x}(\emph{t}) = Ψ(\emph{t})\emph{w,}

and \emph{a} = Ψ(\emph{t}0)\emph{w}. Thus \emph{w} =
Ψ\emph{−}1(\emph{t}0)\emph{a}, and
\end{quote}

\emph{x}(\emph{t}) = Ψ(\emph{t})Ψ\emph{−}1(\emph{t}0)\emph{a,}

\begin{quote}
which implies Φ(\emph{t, t}0) = Ψ(\emph{t})Ψ\emph{−}1(\emph{t}0).

Proposition 2.1.2. \emph{Let} Ψ \emph{be an arbitrary n × n matrix
solution of} ˙Ψ(\emph{t}) = \emph{A}(\emph{t})Ψ(\emph{t}) ; Ψ(\emph{t}0)
= \emph{C}

\emph{where C is nonsingular, then} Ψ(\emph{t}) \emph{is nonsingular for
all t and}
\end{quote}

Φ(\emph{t, s}) = Ψ(\emph{t})Ψ(\emph{s})\emph{−}1\emph{.}

\begin{quote}
\emph{Such a} Ψ(\emph{t}) \emph{is called a} fundamental matrix\emph{,
and} Φ(\emph{t, s}) \emph{is called the} state transition matrix\emph{.}

Let us list some properties of the transition matrix Φ:

(1) Φ(\emph{t, s}) = Φ(\emph{t, τ})Φ(\emph{τ, s}) for all (\emph{t, s,
τ}) as illustrated by the commu- tative diagram

Φ(\emph{τ,s})\\
\emph{X} \emph{−→ X}\\
Φ(\emph{t, s}) \emph{↘} \emph{↓}Φ(\emph{t, τ})\\
\emph{X}

\textbf{Proof:} Consider the unique solution of\\
\{˙\emph{x}(\emph{σ}) = \emph{A}(\emph{σ})\emph{x}(\emph{σ})
\end{quote}

\emph{x}(\emph{s}) = \emph{a}

\begin{quote}
2.1. SOLUTIONS OF LINEAR DIFFERENTIAL EQUATIONS 11

Then, as illustrated in diagram \emph{x}(\emph{t}) = Φ(\emph{t,
s})\emph{a,} \emph{x}(\emph{t}) = Φ(\emph{t, τ})\emph{x}(\emph{τ}) and
\emph{x}(\emph{τ}) = Φ(\emph{τ, s})\emph{a}, and hence\\
Φ(\emph{t, τ})Φ(\emph{τ, s})\emph{a} = Φ(\emph{t, s})\emph{a.}

that is
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[}Φ(\emph{t, τ})Φ(\emph{τ, s}) \emph{−} Φ(\emph{t, s}){]}\emph{a} = 0\\
(2) Φ(\emph{t, s}) is nonsingular, and Φ(\emph{t, s})\emph{−}1=
Φ(\emph{s, t}). Since this must hold for all \emph{a ∈}
R\emph{n}property (1) follows.
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\textbf{Proof:} This follows immediately from
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
(11)
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
Φ(\emph{t, s})Φ(\emph{s, t}) = \emph{I}
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
which is a consequence of property (1).
\end{quote}
\end{minipage} \\
& \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
(3)\uline{\emph{∂}Φ}\emph{∂s}(\emph{t, s}) = \emph{−}Φ(\emph{t,
s})\emph{A}(\emph{s}).
\end{quote}
\end{minipage} \\
& \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Proof:} Differentiating (11) with respect to \emph{s} yields
that
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\emph{∂}Φ\\
\emph{∂s}(\emph{t, s})Φ(\emph{s, t}) + Φ(\emph{t,
s})\emph{A}(\emph{s})Φ(\emph{s, t}) = 0\\
Since Φ(\emph{s, t}) is nonsingular, property (3) follows. □

Let us next consider the solution of the control system
\{˙\emph{x}(\emph{t}) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{u}(\emph{t}) \emph{x}(\emph{t}0) = \emph{a}
\emph{.}

Set \emph{z}(\emph{t}) := Φ(\emph{t}0\emph{, t})\emph{x}(\emph{t}), i.e
\emph{x}(\emph{t}) = Φ(\emph{t, t}0)\emph{z}(\emph{t}). Then,
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
and therefore,
\end{minipage}} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.7500} + 4\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
˙\emph{z} = \emph{−}Φ(\emph{t}0\emph{,
t})\emph{A}(\emph{t})\emph{x}(\emph{t}) + Φ(\emph{t}0\emph{, t})
˙\emph{x}(\emph{t}) so that \{˙\emph{z} = Φ(\emph{t}0\emph{,
t})\emph{B}(\emph{t})\emph{u}(\emph{t}) \emph{z}(\emph{t}0) = \emph{a}
\end{quote}
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
\emph{z}(\emph{t}) = \emph{a} +
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Φ(\emph{t}0\emph{, s})\emph{B}(\emph{s})\emph{u}(\emph{s})\emph{ds,}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
or, premultiplying by Φ(\emph{t, t}0),
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{x}(\emph{t}) = Φ(\emph{t, t}0)\emph{a} +
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Φ(\emph{t, s})\emph{B}(\emph{s})\emph{u}(\emph{s})\emph{ds.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Notice that this equality also holds for \emph{t ≤ t}0.

12 2. LINEAR DYNAMICAL SYSTEMS

\textbf{2.2. Time-invariant systems} Now consider the time-invariant
case:\\
˙\emph{x} = \emph{Ax,}\\
where is \emph{A} is a constant matrix.

For time-invariant systems determining the transition matrix function
becomes much simpler in that it can be expressed in terms of the matrix
exponential.

converges. Definition 2.2.1. \emph{eAt}=∑\emph{∞} Note that,
since∑\emph{N} We collect some properties of matrix exponentials. The
proofs are left \emph{k}=0 \emph{\textbar t\textbar k k}!\emph{∥A∥k ≤
e∥A∥\textbar t\textbar{} \textless{} ∞} for any finite \emph{t}, the sum
\emph{k}=0 \emph{k}!\emph{Ak}.\\
\emph{tk}

for readers as exercises.

(1) If \emph{D} = diag (\emph{λ}1\emph{, λ}2\emph{, . . . , λn}),
\emph{eD}= diag (\emph{eλ}1\emph{, eλ}2\emph{, . . . , eλn});\\
(2) \emph{eP −}1\emph{AP}= \emph{P−}1\emph{eAP};\\
(3) If \emph{AB} = \emph{BA}, then, \emph{eAeB}= \emph{eA}+\emph{B};
\emph{Warning:} In general, \emph{eAeB̸}= \emph{eA}+\emph{B}.

(4) (\emph{eA})\emph{−}1= \emph{e−A};\\
(5) \emph{dteAt} = \emph{AeAt} = \emph{eAtA};

It follows from property (5) above that\\
Ψ(\emph{t}) = \emph{eAt}\\
is a fundamental matrix of the time-invariant system\\
˙\emph{x} = \emph{Ax}\\
where \emph{A} is constant. Then, by Proposition 2.1.2\\
Φ(\emph{t, s}) = Ψ(\emph{t})Ψ(\emph{s})\emph{−}1=
\emph{eAt}(\emph{eAs})\emph{−}1= \emph{eAte−As,} that is\\
(12) Φ(\emph{t, s}) = \emph{eA}(\emph{t−s})

Therefore the solution of the time-invariant system ˙\emph{x} =
\emph{Ax} + \emph{Bu}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
becomes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t}) = \emph{eA}(\emph{t−t}0)\emph{x}(\emph{t}0) +
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{eA}(\emph{t−s})\emph{Bu}(\emph{s})\emph{ds.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
2.3. SYSTEMS OF LINEAR DIFFERENCE EQUATIONS 13

Remark 2.2.2. One might ask whether (12) can be generalized to the
time-varying case, i.e. is it true that

(13) Φ(\emph{t, s}) = exp\emph{\{}∫ \emph{t A}(\emph{τ})\emph{dτ\}}\\
when \emph{A}(\emph{t}) is time varying? The answer is that a sufficient
condition for (13) to hold is that \emph{A}(\emph{t}) and ∫ \emph{t
sA}(\emph{τ})\emph{dτ} commute. □

\textbf{2.3. Systems of linear difference equations}\\
The corresponding analysis for discrete-time system is quite analogous.

In fact, considering the following discrete-time system
\emph{x}(\emph{t} + 1) = \emph{A}(\emph{t})\emph{x}(\emph{t})\emph{,}\\
the transition matrix is generated by\\
Φ(\emph{t} + 1\emph{, s}) = \emph{A}(\emph{t})Φ(\emph{t, s}) Φ(\emph{t,
t}) = \emph{I.}

It is not hard to see that\\
Φ(\emph{t, s}) = \emph{A}(\emph{t −} 1)\emph{A}(\emph{t −} 2) \emph{· ·
· A}(\emph{s}) for \emph{t \textgreater{} s}\\
and Φ(\emph{t, s}) is defined for \emph{t \textless{} s} only if
Φ(\emph{s, t}) is invertible, i.e. \emph{A}(\emph{k})\emph{−}1for
\emph{k} = \emph{s, s} + 1\emph{, . . . , t −} 1. In this case,
Φ(\emph{t, s}) = Φ(\emph{s, t})\emph{−}1. In addition Φ(\emph{t, s}) has
the following properties.

(1) Φ(\emph{t, s}) = Φ(\emph{t, τ})Φ(\emph{τ, s});\\
(2) Φ(\emph{t, s −} 1) = Φ(\emph{t, s})\emph{A}(\emph{s −} 1)\\
Hence, the solution of the control system\\
\emph{x}(\emph{t} + 1) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{u}(\emph{t}) becomes
\end{quote}

\emph{x}(\emph{t}) = Φ(\emph{t, s})\emph{x}(\emph{s}) +∑Φ(\emph{t, σ} +
1)\emph{B}(\emph{σ})\emph{u}(\emph{σ})\emph{.}

\begin{quote}
For time-invariant systems, the transfer matrix\\
Φ(\emph{t, s}) = \emph{At−s}\\
is invertible if and only if \emph{A−}1exists. Note that \emph{eAt}in
continuous time cor-responds to \emph{At}in discrete time. Here lies one
of the fundamental differences between the continuous-time and
discrete-time setting. Indeed \emph{eAt}is never singular, whereas
\emph{At}might be.

Example 2.3.1 (Sampling a continuous-time system). Sometimes it is
important to represent a continuous-time system by means of a
discrete-time system, e.g. when implementing a control law on a
computer. Let \emph{h} be the

14 2. LINEAR DYNAMICAL SYSTEMS
\end{quote}

sampling time and integrate the time-invariant system ˙\emph{x} =
\emph{Ax} + \emph{Bu} from

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{kh} to \emph{kh} + \emph{h},
\end{quote}

\emph{x}(\emph{kh} + \emph{h}) = \emph{eAhx}(\emph{kh}) +∫
\emph{kh}+\emph{h eA}(\emph{kh}+\emph{h−s})\emph{Bu}(\emph{s})
\emph{ds.}

\begin{quote}
By restricting the input \emph{u}(\emph{t}) to be a piecewise constant
signal as
\end{quote}

\emph{u}(\emph{t}) = \emph{v}(\emph{k}) for \emph{t ∈} {[}\emph{kh, kh}
+ \emph{h})\emph{, k ∈} Z

\begin{quote}
the integral above can be evaluated as
\end{quote}

∫ \emph{kh}+\emph{h
eA}(\emph{kh}+\emph{h−s})\emph{Bu}(\emph{s})\emph{ds} =∫
\emph{kh}+\emph{h eA}(\emph{kh}+\emph{h−s})\emph{ds Bv}(\emph{k}) =∫
\emph{h eAsds Bv}(\emph{k})\emph{.}

\begin{quote}
Hence, by letting \emph{z}(\emph{k}) ≜ \emph{x}(\emph{kh}) we get the
discrete-time system
\end{quote}

\emph{z}(\emph{k} + 1) = \emph{Fz}(\emph{k}) +
\emph{Gv}(\emph{k})\emph{,}

\begin{quote}
where \emph{F} = \emph{eAh}and \emph{G} =

system in the sampling points \emph{t} = \emph{kh}.

As a numerical example, consider the one-dimensional motion of Exam-∫
\emph{h} 0\emph{eAs ds B}, having the same values as the original

ple 1.3.1 with the state space description

˙\emph{x} = {[} 0 0 {]} \emph{x} + {[} 1 {]} \emph{u,}

0 1 0
\end{quote}

where \emph{x}1 is the position and \emph{x}2 is the velocity of the
particle. Since \emph{A} is

\begin{quote}
nilpotent, \emph{eAh}is easily calculated from the series expansion,
which gives

\emph{F} = \emph{eAh}= {[} 0 1 {]} \emph{.}

1 \emph{h}

To get \emph{G} we integrate

∫ \emph{h eAsds} =∫ \emph{h} {[}\\
1

0 1
\end{quote}

\emph{s}

{]} \emph{ds} = {[} \emph{h}

\begin{quote}
0\\
\emph{h}2
\end{quote}

\emph{h}

2 {]} \emph{,}

\begin{quote}
which gives \emph{G} = {[} \emph{\uline{h}}2 {]} . □
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

CHAPTER 3

\textbf{Reachability and Observability}

\begin{quote}
In this chapter we shall address two basic issues in systems theory.
First, given a control system, it is natural to ask in what way we can
control the state of the system, i.e., which states can be reached by
choice of a suitable input? This question leads to the concept of
reachability.

Second, given the input and the output of a system, can we determine its
state? This question leads to the concept of observability.
\end{quote}

\textbf{3.1. Reachability}

\begin{quote}
Consider the system

(14) ˙\emph{x}(\emph{t}) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{u}(\emph{t}); \emph{x}(\emph{t}0) =
\emph{x}0\emph{,}

where \emph{x ∈} R\emph{n}, and \emph{u ∈} R\emph{m}. A fundamental
issue in systems and control is to understand under what conditions
there is always a continuous (in time) input signal \emph{u} that
transfers the state from any given initial state \emph{x}(\emph{t}0) =
\emph{x}0 to any other final state \emph{x}(\emph{t}1) = \emph{x}1? In
the literature this property is called \emph{completely controllable}.
If we set \emph{x}1 = 0, it is called \emph{(null-) controllable}, and
it is called \emph{reachable} if we set \emph{x}0 = 0. As we will show
all these notions are equivalent for linear continuous time systems.
Note that for nonlinear systems and discrete time systems, these notions
are not equivalent anymore.

We shall answer the question by giving necessary and sufficient
condi-tions in terms of the given data. Furthermore, when such a state
transfer is possible we calculate the input signal ˆ\emph{u} of minimum
``energy'' that achieves this state transfer.

If we solve the state equations explicitly we obtain
\end{quote}

\emph{x}1 \emph{−} Φ(\emph{t}1\emph{, t}0)\emph{x}0 =∫ \emph{t}1
Φ(\emph{t}1\emph{, s})\emph{B}(\emph{s})\emph{u}(\emph{s})\emph{ds.}

\begin{quote}
Let \emph{d} ≜ \emph{x}1 \emph{−} Φ(\emph{t}1\emph{, t}0)\emph{x}0 and
let \emph{U} be the space of input signals. If we define the mapping
\emph{L} : \emph{U →} R\emph{n}as

(15) \emph{Lu} ≜∫ \emph{t}1 Φ(\emph{t}1\emph{,
s})\emph{B}(\emph{s})\emph{u}(\emph{s})\emph{ds,}

we see that the desired state transfer is possible if and only if
\emph{d ∈} Im \emph{L}. Furthermore, if either \emph{x}0 or \emph{x}1 is
arbitrary, \emph{d} will span the whole R\emph{n}. From this we can also
see that why those different notions on controllability are equivalent.
\end{quote}

15

\begin{quote}
16 3. REACHABILITY AND OBSERVABILITY

Thus, the mathematical problem for reachability is to study under what
conditions the equation \emph{Lu} = \emph{d} has a solution for any
\emph{d ∈} R\emph{n}, or the mapping \emph{L} is onto R\emph{n}.

It can be easily verified that \emph{L} is a linear mapping, but since
it does not act between two finite-dimensional vector spaces, \emph{L}
does not have a finite-dimensional matrix representation. As a
consequence, the characterization of Im \emph{L} is not immediate.

Recall that for a constant matrix \emph{P}, the equation \emph{Pu} =
\emph{d} has a solution if \emph{d ∈} Im \emph{P} and \emph{P} is onto
if it has full row rank, namely its row vectors are linearly
independent. From linear algebra we also know that Im \emph{P} = Im
\emph{PPT}.
\end{quote}

time-varying matrix, it is necessary for us to extend the concept of
linear Since our \emph{L} has the form \emph{Lu} =∫ \emph{t}1
\emph{t}0\emph{F}(\emph{s})\emph{u}(\emph{s})\emph{ds}, where
\emph{F}(\emph{t}) is an \emph{n × m}

\begin{quote}
independence to functions of a real variable.

Definition 3.1.1. A set of functions \emph{f}1(\emph{t})\emph{, · · · ,
fN}(\emph{t}) is said to be lin-early dependent on {[}\emph{t}0\emph{,
t}1{]} over the complex field if there exist complex num-bers
\emph{c}1\emph{, · · · , cN} that are not all zero, such that
\end{quote}

\emph{c}1\emph{f}1(\emph{t}) + \emph{· · ·} + \emph{cNfN}(\emph{t}) =
0\emph{, ∀t ∈} {[}\emph{t}0\emph{, t}1{]}\emph{.}

\begin{quote}
Otherwise, the functions are said to be \emph{linearly independent} over
{[}\emph{t}0\emph{, t}1{]}.

Lemma 3.1.2. \emph{A set of real functions f}1(\emph{t})\emph{, · · · ,
fN}(\emph{t}) \emph{is linearly inde-pendent on} {[}\emph{t}0\emph{,
t}1{]} \emph{if and only if}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{is nonsingular, where}
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{W}(\emph{t}0\emph{, t}1) :=
\end{minipage} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4000} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}1
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{F}(\emph{s})\emph{FT}(\emph{s})\emph{ds}
\end{quote}
\end{minipage} \\
&
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4000} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{F}(\emph{t}) =
\end{minipage}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4000} + 2\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{f}1(\emph{t})

\emph{...} 

\emph{fn}(\emph{t}) \emph{.}
\end{quote}
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\textbf{Proof:} We prove first the necessity by contradiction. Suppose
\emph{W}(\emph{t}0\emph{, t}1) is singular, then there exists a row
vector \emph{α ̸}= 0 such that \emph{αW}(\emph{t}0\emph{, t}1) = 0. Thus
\emph{αW}(\emph{t}0\emph{, t}1)\emph{αT}= 0, namely
\end{quote}

\emph{αW}(\emph{t}0\emph{, t}1)\emph{αT}=∫ \emph{t}1
(\emph{αF}(\emph{s}))(\emph{αF}(\emph{s}))\emph{Tds} = 0\emph{.}

\begin{quote}
Since (\emph{αF}(\emph{t}))(\emph{αF}(\emph{t}))\emph{T}is continuous
and nonnegative, the above identity implies that \emph{αF}(\emph{t}) = 0
on {[}\emph{t}0\emph{, t}1{]}, which contradicts with the linear inde-
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
pendence assumption on the rows of \emph{F}(\emph{t}).\\
Proof of the sufficiency is left as an exercise.
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
3.1. REACHABILITY 17

Lemma 3.1.3. \emph{Consider Lu} =

\emph{F}(\emph{t}) = ∫ \emph{t}1
\emph{t}0\emph{F}(\emph{s})\emph{u}(\emph{s})\emph{ds, where}
\emph{f}1(\emph{t}) \emph{...} \\
\\
\emph{fn}(\emph{t}) \emph{, each fi}(\emph{t}) \emph{is} 1 \emph{× m
matrix valued continuous function over} {[}\emph{t}0\emph{,
t}1{]}\emph{.}\\
\emph{The mapping L is onto} R\emph{nif and only if the functions
f}1(\emph{t})\emph{, · · · , fn}(\emph{t}) \emph{are linearly
independent over} {[}\emph{t}0\emph{, t}1{]}\emph{.}

\textbf{Proof:} Sufficiency: for any given \emph{d ∈} R\emph{n}, one can
easily verify that \emph{u}(\emph{t}) =
\emph{FT}(\emph{t})\emph{W−}1(\emph{t}0\emph{, t}1)\emph{d}

solves the equation \emph{Lu} = \emph{d}.

Necessity: we prove it by contradiction. Suppose there exists a row
vector \emph{α ̸}= 0 such that \emph{αF}(\emph{t}) = 0 on
{[}\emph{t}0\emph{, t}1{]}. Since \emph{L} is anyway onto, the following
equation has a solution:\\
∫ \emph{t}1 \emph{F}(\emph{s})\emph{u}(\emph{s})\emph{ds} = \emph{αT.}

Multiplying both sides by \emph{α} we have 0 = \emph{ααT}, which implies
\emph{α} = 0. This is a contradiction. □ Now let us return to the
discussion on reachability. For \emph{L} defined by (15), we have
\emph{F}(\emph{t}) = Φ(\emph{t}1\emph{, t})\emph{B}(\emph{t}).
\end{quote}

Definition 3.1.4. The \emph{reachability Gramian W}(\emph{t}0\emph{,
t}1) is the matrix

\emph{W}(\emph{t}0\emph{, t}1) ≜∫ \emph{t}1 Φ(\emph{t}1\emph{,
s})\emph{B}(\emph{s})\emph{BT}(\emph{s})Φ\emph{T}(\emph{t}1\emph{,
s})\emph{ds.}

semidefinite matrix. Remark 3.1.5. \emph{∀t}0\emph{, t}1 such that
\emph{t}0 \emph{\textless{} t}1\emph{W}(\emph{t}0\emph{, t}1) is a
symmetric, positive□

\begin{quote}
We can now state and prove the main theorem of this section.

Theorem 3.1.6. \emph{Consider the system} (14) \emph{with transition
matrix} Φ(\emph{t, s})\emph{.}

(1) \emph{The system is reachable if and only if the reachability
Gramian is nonsingular. In this case the minimum energy control that
transfers the state from x}0 \emph{to x}1 \emph{is}
\end{quote}

ˆ\emph{u}(\emph{t}) = \emph{BT}(\emph{t})Φ\emph{T}(\emph{t}1\emph{,
t})\emph{W−}1(\emph{t}0\emph{, t}1){[}\emph{x}1 \emph{−}
Φ(\emph{t}1\emph{, t}0)\emph{x}0{]}\emph{.}

\begin{quote}
(2) \emph{If the reachability Gramian is singular, the state transfer
from x}(\emph{t}0) = \emph{x}0 \emph{to x}(\emph{t}1) = \emph{x}1
\emph{is possible if and only if}
\end{quote}

\emph{d} ≜ \emph{x}1 \emph{−} Φ(\emph{t}1\emph{, t}0)\emph{x}0 \emph{∈}
Im \emph{W}(\emph{t}0\emph{, t}1)\emph{.}

\begin{quote}
\emph{and the minimum energy solution} ˆ\emph{u is given by}
\end{quote}

ˆ\emph{u} = \emph{BT}(\emph{t})Φ\emph{T}(\emph{t}1\emph{, t})\emph{a,}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
18
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
3. REACHABILITY AND OBSERVABILITY

\emph{where a solves W}(\emph{t}0\emph{, t}1)\emph{a} = \emph{d and the
energy of u is defined as} \emph{∥u∥} = (∫ \emph{t}1
\emph{uT}(\emph{s})\emph{u}(\emph{s})\emph{ds}) 2 \emph{.}
\end{quote}

\uline{1}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\textbf{Proof:} The first part of the theorem is a direct application of
Lemma 3.1.3. Now we show the second part.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Sufficiency: Suppose first that \emph{d ∈} Im \emph{W}(\emph{t}0\emph{,
t}1), then \emph{d} = \emph{W}(\emph{t}0\emph{, t}1)\emph{a} for

some \emph{a ∈} R\emph{n}. Let \emph{u} ≜
\emph{BT}(\emph{t}))Φ\emph{T}(\emph{t}1\emph{, t})\emph{a} and we get
\emph{Lu} = \emph{W}(\emph{t}0\emph{, t}1)\emph{a} = \emph{d}.
Necessity: Suppose now that the state transfer is possible, i.e.,
\emph{Lu} = \emph{d}

for some \emph{u}(\emph{t}). Furthermore, suppose that \emph{d ̸∈} Im
\emph{W}(\emph{t}0\emph{, t}1). We show that this will give a
contradiction.

Since \emph{rank W}(\emph{t}0\emph{, t}1) \emph{\textless{} n}, there
exists a row vector \emph{α ̸}= 0, such that

\emph{αd ̸}= 0 and \emph{αx} = 0 \emph{∀x ∈} Im \emph{W}. This implies
\end{quote}

∫ \emph{t}1 \emph{α}Φ(\emph{t}1\emph{,
s})\emph{B}(\emph{s})\emph{BT}(\emph{s})Φ\emph{T}(\emph{t}1\emph{,
s})\emph{ds} = 0\emph{.}

\begin{quote}
Thus

implies \emph{αd} = 0. This is a contradiction.∫ \emph{t}1
\emph{t}0(\emph{α}Φ(\emph{t}1\emph{,
s})\emph{B}(\emph{s}))(\emph{α}Φ(\emph{t}1\emph{,
s})\emph{B}(\emph{s}))\emph{T ds} = 0. Therefore \emph{αLu} = 0, which

The final step is to prove the optimality of ˆ\emph{u}. Let \emph{u} be
any solution of

\emph{Lu} = \emph{d}. Then \emph{Lu} = \emph{L}ˆ\emph{u} so
\emph{L}(\emph{u −} ˆ\emph{u}) = 0. This gives

0 =∫ \emph{t}1 (\emph{aT}Φ(\emph{t}1\emph{,
s})\emph{B}(\emph{s})(\emph{u −} ˆ\emph{u})\emph{ds} =∫ \emph{t}1
(ˆ\emph{u})\emph{Tuds − ∥}ˆ\emph{u∥}2\emph{.}
\end{quote}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
1 {]} .

1
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Hence, \emph{∥}ˆ\emph{u∥}2= (\emph{u,} ˆ\emph{u}). By using the
Cauchy-Schwartz inequality we have \emph{∥}ˆ\emph{u∥}2\emph{≤
∥}ˆ\emph{u∥ ∥u∥,}

which yields that \emph{∥}ˆ\emph{u∥ ≤ ∥u∥}. Hence, ˆ\emph{u} is optimal.
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Example 3.1.7. Consider the system ˙\emph{x} = \emph{Ax} + \emph{bu},
with

\emph{A} = {[}\emph{−}1 0 \emph{−}1 0

{]} and \emph{b} = {[}\\
1
\end{quote}

1 {]} \emph{.}

\begin{quote}
The system is time invariant and \emph{φ}(\emph{t, s}) =
\emph{eA}(\emph{t−s}). By series expansion, we

get \emph{eAt}= \emph{e−tI} and the reachability Gramian is
\emph{W}(\emph{t}0\emph{, t}1) =

If \emph{t}1 \emph{\textgreater{} t}0 then the integral is strictly
positive and hence ∫ \emph{t}1
\emph{t}0\emph{e−}2(\emph{t}1\emph{−s})\emph{ds} {[} 1
\end{quote}

1

\begin{quote}
Im \emph{W}(\emph{t}0\emph{, t}1) = \emph{\{λ} {[} 1 {]} ; \emph{λ ∈}
R\emph{\}.}\\
1

Can we control the system from \emph{x}0 = {[} 1 {]} to \emph{x}1 = {[}
4 {]} ? Form \emph{d} as

0 2
\end{quote}

\emph{d} = \emph{x}1 \emph{− e−}(\emph{t}1\emph{−t}0)\emph{x}0 = {[} 4
\emph{− e−}(\emph{t}1\emph{−t}0) {]} \emph{.}

2

\begin{quote}
It is easily seen that \emph{d /∈} Im \emph{W}(\emph{t}0\emph{, t}1) for
every choice of \emph{t}1 \emph{\textgreater{} t}0 and the state
transfer is not possible.
\end{quote}\strut
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
3.2. REACHABILITY FOR TIME-INVARIANT SYSTEMS 19

Can we control the system from \emph{x}0 = {[} 2 {]} to \emph{x}1 = {[}
3 {]} ? We now get

0 2

\emph{d} = \emph{x}1 \emph{− e−}(\emph{t}1\emph{−t}0)\emph{x}0 = {[} 3
\emph{−} 2\emph{e−}(\emph{t}1\emph{−t}0) {]} \emph{.}\\
2

For the particular choice \emph{t}1 \emph{− t}0 = ln 2 we get \emph{d} =

the state transfer is possible. {[} 2 {]}\emph{∈} Im
\emph{W}(\emph{t}0\emph{, t}1) and

□\\
2
\end{quote}

\textbf{3.2. Reachability for Time-Invariant Systems}

\begin{quote}
In this section we shall consider the special case of time-invariant
sys-
\end{quote}

tems and characterize Im \emph{W}(\emph{t}0\emph{, t}1) in terms of the
matrix pair (\emph{A, B}). This

characterization will be the first step towards a geometric theory for
this

\begin{quote}
class of systems.

We shall mainly analyze continuous-time systems, but almost all theo-

rems obtained are valid for discrete-time systems as well.
\end{quote}

\emph{reachability matrix} Γ is defined as Definition 3.2.1. Let
(\emph{A, B}) be a matrix pair, where \emph{A} is \emph{n × n}. The

Γ ≜ {[}\emph{B, AB, A}2\emph{B, .., An−}1\emph{B}{]}\emph{.}

\begin{quote}
Remark 3.2.2. By the Cayley-Hamilton theorem \emph{An}+\emph{j, j ≥} 0
is a linear

combination of \emph{Aj}for \emph{j} = 0\emph{,} 1\emph{, .., n −} 1,
which implies that Im Γ equals the image of the infinite matrix
{[}\emph{B, AB, A}2\emph{B, A}3\emph{B, ....}{]}. □

The reachability properties of a linear system, in the previous section
\end{quote}

characterized in terms of \emph{W}(\emph{t}0\emph{, t}1), can in the
time-invariant case be dis-

played in a simpler form involving the pair (\emph{A, B}). The following
theorem

\begin{quote}
gives a convenient algebraic characterization of Im
\emph{W}(\emph{t}0\emph{, t}1).

Theorem 3.2.3. \emph{Let A be n × n and B be n × k. Then, for all
t}0\emph{, t}1

\emph{such that t}0 \emph{\textless{} t}1 \emph{we have}
\end{quote}

Im \emph{W}(\emph{t}0\emph{, t}1) = Im {[}\emph{B, AB, A}2\emph{B, ..,
An−}1\emph{B}{]}\emph{.}

\textbf{Proof:} Fix (\emph{t}0\emph{, t}1) and let \emph{W} =
\emph{W}(\emph{t}0\emph{, t}1). We first show that Im Γ \emph{⊆} Im
\emph{W}

by showing that ker \emph{WT⊆} ker Γ\emph{T}. To this end, let \emph{a
∈} ker \emph{W} which implies that 0 = \emph{aTWa} =∫ \emph{t}1
\emph{t}0\emph{aT eA}(\emph{t}1\emph{−s})\emph{BBT eAT}
(\emph{t}1\emph{,s})\emph{a ds}, i.e.,

∫ \emph{t}1 \emph{\textbar BTeAT}
(\emph{t}1\emph{−s})\emph{a\textbar{}}2\emph{ds} = 0\emph{.}

\begin{quote}
Since the integrand is continuous and non-negative it follows that

\emph{BTeAT} (\emph{t}1\emph{−s})\emph{a ≡} 0 \emph{∀s ∈}
{[}\emph{t}0\emph{, t}1{]}\emph{,}

i.e.,

∑\emph{j}!(\emph{t}1 \emph{− s})\emph{jBT} (\emph{AT} )\emph{ja ≡}
0\emph{.}\\
1

20 3. REACHABILITY AND OBSERVABILITY

This implies that \emph{BT}(\emph{AT})\emph{ja} = 0 for all \emph{j}.
Consequently, {[}\emph{B, AB, A}2\emph{B, .., An−}1\emph{B}{]}\emph{Ta}
= 0, i.e., \emph{a ∈} ker Γ\emph{T}. Hence, Im Γ \emph{⊆} Im \emph{W}.
\end{quote}

\emph{a} = \emph{Wx}, and hence Conversely, suppose \emph{a ∈} Im
\emph{W}. Then there is an \emph{x ∈} R\emph{n}such that

\begin{quote}
\emph{a} =∑\emph{AjB}∫ \emph{t}1 \emph{j}!(\emph{t}1 \emph{−
s})\emph{jBT eAT} (\emph{t}1\emph{−s})\emph{x ds,}\\
1

from which we see that \emph{a ∈} Im {[}\emph{B, AB, A}2\emph{B,
A}3\emph{B, ....}{]}. By Remark 3.2.2. this is equivalent to \emph{a ∈}
Im Γ. Hence, Im \emph{W ⊆} Im Γ. □

Remark 3.2.4. Since Im Γ = Im \emph{W}(\emph{t}0\emph{, t}1) for any
interval (\emph{t}0\emph{, t}1), we see that in the time-invariant case
the image of the reachability Gramian is independent of the the interval
(\emph{t}0\emph{, t}1). □

Definition 3.2.5. Let \emph{n} be the dimension of the state space. The
pair (\emph{A, B}) is said to be \emph{completely reachable
(controllable)} if Γ has full rank, i.e.,
\end{quote}

rank Γ = \emph{n.}

\begin{quote}
For an arbitrary system, not necessarily completely reachable, Im Γ is a
subspace of the state space of great importance, and the following
definition will be useful.

Definition 3.2.6. The \emph{reachable subspace} R is defined as
\end{quote}

R ≜ Im {[}\emph{B, AB, A}2\emph{B, .., An−}1\emph{B}{]}

\begin{quote}
We easily see that R is the set of states that can be reached from the
origin. An important property of R is its \emph{A}-invariance.

Lemma 3.2.7. \emph{The reachable subspace} R \emph{is A-invariant, i.e}

\emph{A}R \emph{⊆} R\\
\emph{In particular, eAt}R \emph{⊆} R \emph{for all t ∈} R\emph{n.}

\textbf{Proof:}Since, by the Cayley-Hamilton theorem, \emph{An}is a
linear combination of \emph{Aj}for \emph{j} = 0\emph{,} 1\emph{, .., n
−} 1 it follows that\\
\emph{A}R = Im {[}\emph{AB, A}2\emph{B, .., AnB}{]} \emph{⊆} Im
{[}\emph{B, AB, .., An−}1\emph{B}{]} = R\emph{.}\\
Moreover, by induction we get \emph{Aj}R \emph{⊆} R, which implies that

\emph{eAt}R =∑\emph{j}!\emph{AjR ⊆} R\\
\emph{tj}

□ The preceding lemma is very important in the geometric theory of
linear systems and as an immediate consequence we have the following
theorem.

Theorem 3.2.8. \emph{Let ϵ \textgreater{}} 0\emph{. A time-invariant
system can be transferred from any x}0 \emph{∈} R \emph{to any x}1
\emph{∈} R \emph{in time ϵ.}

3.2. REACHABILITY FOR TIME-INVARIANT SYSTEMS 21

\textbf{Proof:} By Lemma 3.2.7, \emph{x}0 \emph{∈} R implies that
\emph{eAϵx}0 \emph{∈} R. Therefore, if in addition, \emph{x}1 \emph{∈}
R, \emph{d} = \emph{x}1 \emph{− eAϵx}0 \emph{∈ R}, and since \emph{d ∈}
R the state transfer is possible. □ To further clarify the picture we
note that if the state of the system is in R at some instant it is
impossible to steer the state out of R, neither is it possible to enter
R from an initial state not in R.
\end{quote}

allows for a decomposition theorem for time-invariant systems. Example
3.2.9 (A decomposition theorem). The \emph{A−}invariance of R

completely reachable. Let R be the reachable subspace with \emph{d} =
dim R and Consider the system ˙\emph{x} = \emph{Ax} + \emph{Bu,
x}(\emph{t}) \emph{∈} R\emph{n}, with the pair (\emph{A, B}) \emph{not}

\begin{quote}
let \emph{V} be a complement to R in R\emph{n}, i.e.,
\end{quote}

R\emph{n}= R + \emph{V.}

\begin{quote}
Choose a basis for R\emph{n}such that the \emph{d} first vectors span R
and the remaining span \emph{V} and let \emph{x} be the coordinates in
this basis. Partition \emph{x} as \emph{x} = {[}\emph{x′}system has the
structure 1 \emph{x′}2 {]}\emph{′}, with \emph{x}1 \emph{d−}dimensional.
We shall show that in this basis the

{[}˙\emph{x}1 ˙\emph{x}2 {]} = {[} \emph{A}11 0 \emph{A}12 \emph{A}22
{]} \emph{x} + {[} \emph{B}1 0 {]} \emph{u,}

where \emph{B}1 has \emph{d} rows.

Hence, we have to show that the blocks \emph{A}21 and \emph{B}2 are
zero, as indicated. In the chosen basis, the action of \emph{A} on an
arbitrary vector in R can be written as\\
{[} \emph{A}11 \emph{A}21 \emph{A}12 \emph{A}22 {]} {[} \emph{x}1 0 {]}
= {[} \emph{A}11\emph{x}1 \emph{A}21\emph{x}1 {]} \emph{.}

Since R is \emph{A−}invariant it holds that \emph{A}21\emph{x}1 = 0
regardless of \emph{x}1 and we conclude that \emph{A}21 = 0. Moreover,
since Im \emph{B ⊆} R it must hold that only the first \emph{d} rows of
\emph{B} can be nonzero.

It is easily seen that the pair (\emph{A}11\emph{, B}1) is completely
reachable and de-fines a reachable subsystem with state space R.

As a side remark, at this stage, we mention that if the matrix
\emph{A}22 is asymptotically stable ( see Chapter 4) we say that the
pair (\emph{A, B}) is \emph{stabilizable} . This is a natural definition
to make, since \emph{x}2(\emph{t}) is unaffected by the input and tends
asymptotically to zero by the stability of \emph{A}22 and by
reachability of the pair (\emph{A}11\emph{, B}1) the state
\emph{x}1(\emph{t}) can, by choice of suitable input, be steered to the
origin.

We illustrate the decomposition theorem with a numerical example.

Consider the system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
˙\emph{x} =
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
2

\begin{quote}
3\\
2
\end{quote}\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}1 2 \emph{x} + 0
\end{quote}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{−}2 1 \emph{u.}\\
1\strut
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
22 3. REACHABILITY AND OBSERVABILITY

The reachability matrix is

1 1 Γ =\emph{−}2 1 0

2\\
8

4 \emph{,}\\
1 2 4

and the set \emph{\{}

1  0
\end{quote}

1 \emph{,}2

1\emph{\}} is a basis for R. A complement can be chosen as \emph{V} =

\begin{quote}
Im

with inverse \emph{T−}1=

sented by\\
0

0. Let the transformation matrix \emph{T} be defined as \emph{T} = 
0

1\\
\emph{−}0\emph{.}5 0\emph{.}5

0 \emph{−}1 0. In the new basis the system is repre-\\
 0

1\\
2

1\\
0
\end{quote}

0 \emph{,}

\begin{quote}
and \emph{A} = \emph{T−}1\\
2
\end{quote}

2

\begin{quote}
3\\
0
\end{quote}

2

\begin{quote}
0 \emph{−}1\\
0

2 \emph{T} =\\
0
\end{quote}

2

\begin{quote}
0\\
\emph{−}2 4

0 \emph{−}1\\
2
\end{quote}

1

\begin{quote}
It is easy to verify that that the pair (\emph{A}11\emph{, B}1) is
reachable and since \emph{A}22 = \emph{B} = \emph{T−}1\emph{−}2 1
=\emph{−}1 0 \emph{.}\\
1 2

\emph{−}1 the pair (\emph{A, B}) is stabilizable. □
\end{quote}

\textbf{3.3. Reachability for Discrete-Time Systems}

\begin{quote}
For simplicity we shall only study time-invariant systems.

Consider the discrete-time system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(Σ)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t});
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t}0) = \emph{x}0\emph{.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
For an arbitrary input sequence \emph{\{u}(0)\emph{, u}(1)\emph{, ...,
u}(\emph{t})\emph{\}}, we have
\end{quote}

\emph{t}

\emph{x}(\emph{t} + 1) = \emph{At}+1\emph{x}(0)
+∑\emph{At−sBu}(\emph{s})\emph{.}

The question of reachability is whether it is possible to transfer the
system

from the state \emph{x}0 to \emph{x}1 in \emph{T} +1 steps. As in the
continuous-time case we let \emph{d}

be the difference between the desired state and the state of the
uncontrolled

motion, i.e., \emph{d} ≜ \emph{x}(\emph{T} + 1) \emph{−
AT}+1\emph{x}(0). By solving the system equations explicitly, we see
that

\begin{quote}
\emph{d} = {[}\emph{B, AB, A}2\emph{B, ....., ATB}{]}\emph{u}(\emph{T
−} 1) \emph{.} 
\end{quote}

\emph{.}

\emph{u}(\emph{T})

\begin{quote}
 \emph{u}(0) \emph{.} 

3.4. OBSERVABILITY 23
\end{quote}

Hence, a necessary and sufficient condition for the desired state
transfer to

be possible is that \emph{d ∈} Im {[}\emph{B, AB, A}2\emph{B, .....,
ATB}{]}. We observe that since the rank of this matrix might depend on
\emph{T} we have, in contrast to the

continuous-time case, that the state transfer might be possible for some
\emph{T},

\begin{quote}
but not for \emph{T −} 1.

Definition 3.3.1. R\emph{t} ≜ Im {[}\emph{B, AB, A}2\emph{B, .., AtB}{]}

The spaces R\emph{t} forms an increasing sequence of subspaces of the
state
\end{quote}

space R\emph{n}and again by the Cayley-Hamilton theorem we have that
R\emph{t} =

\begin{quote}
R\emph{n−}1 for \emph{t ≥ n}. Hence,
\end{quote}

R0 \emph{⊆} R1 \emph{⊆ .... ⊆} R\emph{n−}1 = R\emph{n} = R\emph{n}+1 =
\emph{..}

\begin{quote}
Consequently, we define the reachable subspace R as
\end{quote}

R ≜ Im {[}\emph{B, AB, A}2\emph{B, .., An−}1\emph{B}{]}

for discrete time systems as well and we say that the system is
\emph{completely}

\begin{quote}
\emph{reachable} if R = R\emph{n}.

We shall now state and prove an intuitively plausible lemma that will be
\end{quote}

a major component in the proof of the pole-placement theorem in Chapter
6.

\begin{quote}
Lemma 3.3.2. \emph{Let} (\emph{A, B}) \emph{be a reachable pair and b ∈}
Im \emph{B.} \emph{Then}

\emph{there is an input sequence \{}˜\emph{u}(1)\emph{,}
˜\emph{u}(2)\emph{, ...,} ˜\emph{u}(\emph{n −} 1)\emph{\} such that the
trajectory \{x}(1)\emph{, x}(2)\emph{, ..., x}(\emph{n})\emph{\} of the
system x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) +
\emph{Bu}(\emph{t})\emph{, x}(1) = \emph{b spans} R\emph{n, i.e.,}
\end{quote}

span \emph{\{x}(1)\emph{, x}(2)\emph{, ..., x}(\emph{n})\emph{\}} =
R\emph{n.}

\textbf{Proof:}Let \emph{\{x}(1)\emph{, x}(2)\emph{, ...,
x}(\emph{k})\emph{\}} be a maximal linearly independent sequence that
can be obtained as a trajectory of the system starting at \emph{x}(1) =
\emph{b}.

\begin{quote}
Let the subspace \emph{V} be defined as \emph{V} ≜ span
\emph{\{x}(1)\emph{, x}(2)\emph{, ..., x}(\emph{k})\emph{\}}. Clearly,
\emph{V ⊆} R. Since we want to show that \emph{V} = R\emph{n}, it is
enough by reachability to show that R \emph{⊆ V} .
\end{quote}

By letting \emph{u}(\emph{k}) = 0 in the relation \emph{x}(\emph{k} + 1)
= \emph{Ax}(\emph{k}) + \emph{Bu}(\emph{k}) we see that Since the
sequence is maximal we have that \emph{x}(\emph{k} +1) \emph{∈ V} for
every \emph{u}(\emph{k}).

\emph{Ax}(\emph{k}) \emph{∈ V} . From \emph{x}(\emph{k} + 1) \emph{−
Ax}(\emph{k}) = \emph{Bu}(\emph{k}) we conclude that Im \emph{B ⊆ V} .
The next step is to note that \emph{V} is \emph{A}-invariant, which
follows from the fact

that for any \emph{j} such that 1 \emph{≤ j \textless{} k} we have that
\emph{Ax}(\emph{j}) = \emph{x}(\emph{j}+1)\emph{−Bu}(\emph{j}) \emph{∈
V} .

\begin{quote}
By applying \emph{A} to the inclusion Im \emph{B ⊆ V} , using the
\emph{A}-invariance of \emph{V} , we get that \emph{A} Im \emph{B ⊆ V} .
Hence, Im \emph{B} + \emph{A} Im \emph{B ⊆ V} , and by induction we
obtain the inclusion R \emph{⊆ V} . □
\end{quote}

\textbf{3.4. Observability}

\begin{quote}
Consider now a system with output:
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(16)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
˙\emph{x} = \emph{A}(\emph{t})\emph{x} + \emph{B}(\emph{t})\emph{u}\\
\emph{y} = \emph{C}(\emph{t})\emph{x} + \emph{D}(\emph{t})\emph{u,}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
24 3. REACHABILITY AND OBSERVABILITY

where \emph{x ∈} R\emph{n}, \emph{u ∈} R\emph{m}, and \emph{y ∈}
R\emph{p}. In this section we shall investigate the problem of
determining the state of a system given the input and the output. More
specifically, can we determine \emph{\{x}(\emph{t}); \emph{t}0 \emph{≤ t
≤ t}1\emph{\}} from \emph{\{y}(\emph{t}); \emph{t}0 \emph{≤ t
≤t}1\emph{\}} and \emph{\{u}(\emph{t}); \emph{t}0 \emph{≤ t ≤
t}1\emph{\}} ? The analysis will be very similar to that of reachability
and we shall later show that reachability and observability are in a
sense dual concepts.

By solving the system equations (14) explicitly we get
\end{quote}

\emph{x}(\emph{t}) = Φ(\emph{t, t}0)\emph{x}0 +∫ \emph{t} Φ(\emph{t,
s})\emph{B}(\emph{s})\emph{u}(\emph{s})\emph{ds.}

\begin{quote}
We notice that in order to determine \emph{x}(\emph{t}) for all \emph{t
∈} {[}\emph{t}0\emph{, t}1{]}, it is enough to determine \emph{x}0. The
output \emph{y} is given by the relation
\end{quote}

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t}) +
\emph{D}(\emph{t})\emph{u}(\emph{t})\emph{.}

\begin{quote}
After rearranging terms we get

(17) \emph{C}(\emph{t})Φ(\emph{t, t}0)\emph{x}0 = \emph{y}(\emph{t})
\emph{− D}(\emph{t})\emph{u}(\emph{t}) \emph{−}∫ \emph{t
C}(\emph{t})Φ(\emph{t, s})\emph{B}(\emph{s})\emph{u}(\emph{s})\emph{ds.}

The right-hand side of (17) is a known quantity and if we call it
\emph{v}(\emph{t}) the problem is to determine \emph{x}0 from the
relation

(18) \emph{C}(\emph{t})Φ(\emph{t, t}0)\emph{x}0 = \emph{v}(\emph{t})
\emph{t ∈} {[}\emph{t}0\emph{, t}1{]}\emph{.}

Now the question is, for a given input \emph{u}, can different initial
states produce the same function \emph{v}(\emph{t})? If not, the mapping
from initials states to functions \emph{v} is injective and from a given
function \emph{v} we can determine \emph{x}0. If so we say the system is
\emph{observable}.

To this end, let \emph{Y} be the space of \emph{m}-dimensional,
square-integrable func-tions on {[}\emph{t}0\emph{, t}1{]} and introduce
the mapping \emph{T} : R\emph{n→ Y} as\\
(19) (\emph{Tx}0)(\emph{t}) ≜ \emph{C}(\emph{t})Φ(\emph{t,
t}0)\emph{x}0\emph{.}

It is easily seen that \emph{T} is a linear mapping, but since \emph{Y}
is not a finite-dimensional space, \emph{T} does not have a
finite-dimensional matrix representa-tion.
\end{quote}

injective if and only if Ω has full column rank, where Ω is a constant
matrix. Recall that in the finite dimensional case, a mapping Ω :
R\emph{n→} R\emph{p}is

\begin{quote}
This implies that Ω must have more rows than columns. On the other hand,
the mapping defined in (19) has the following form:

(20) Ω(\emph{t})\emph{x} = \emph{v}(\emph{t}); \emph{t ∈}
{[}\emph{t}0\emph{, t}1{]}\emph{,} where Ω(\emph{t}) has \emph{p} rows
and \emph{n} columns (\emph{p ≤ n})!

Lemma 3.4.1. \emph{The mapping defined in} (20) \emph{is injective if
and only if the column vectors of} Ω(\emph{t}) \emph{are linearly
independent over} {[}\emph{t}0\emph{, t}1{]}\emph{.}

\textbf{Proof:} Sufficiency: if \emph{x} = \emph{a} and \emph{x} =
\emph{b} produce the same \emph{v}(\emph{t}) on {[}\emph{t}0\emph{,
t}1{]} we have that Ω(\emph{t})(\emph{a−b}) = 0. Since the columns of
Ω(\emph{t}) are linearly independent, \emph{a − b} = 0.

3.5. OBSERVABILITY FOR TIME-INVARIANT SYSTEMS 25

Necessity: injection implies that Ω(\emph{t})\emph{b} = 0 over
{[}\emph{t}0\emph{, t}1{]} if and only if \emph{b} = 0, which implies
the linear independence. □

Definition 3.4.2. The \emph{observability Gramian M}(\emph{t}0\emph{,
t}1) is the matrix ∫ \emph{t}1 Φ\emph{T}(\emph{t,
t}0)\emph{CT}(\emph{t})\emph{C}(\emph{t})Φ(\emph{t, t}0)\emph{dt.}

Theorem 3.4.3. \emph{Consider system} (16)\emph{.}

(1) \emph{The system is observable if and only if the observability
Gramian} \emph{M}(\emph{t}0\emph{, t}1) \emph{is nonsingular.}

(2) \emph{When M}(\emph{t}0\emph{, t}1) \emph{is singular, for a given
input u the initial states x}(\emph{t}0) = \emph{a and x}(\emph{t}0) =
\emph{b produce the same output on} {[}\emph{t}0\emph{, t}1{]} \emph{if
and only if a − b ∈} ker \emph{M}(\emph{t}0\emph{, t}1)\emph{.}

\textbf{Proof:} The first part can be proven by Lemmas 3.1.3 and 3.4.1.

For the second part it is enough to show that
\emph{C}(\emph{t})Φ(\emph{t, t}0)\emph{x} = 0 over {[}\emph{t}0\emph{,
t}1{]} if and only if \emph{M}(\emph{t}0\emph{, t}1)\emph{x} = 0.

Suppose \emph{C}(\emph{t})Φ(\emph{t, t}0)\emph{x} = 0, then we can
easily see \emph{M}(\emph{t}0\emph{, t}1)\emph{x} = 0. On the other
hand, suppose \emph{M}(\emph{t}0\emph{, t}1)\emph{x} = 0. This implies
that \emph{xTM}(\emph{t}0\emph{, t}1)\emph{x} = 0. Using the same
argument we have used before we can draw the conclusion that
\emph{C}(\emph{t})Φ(\emph{t, t}0)\emph{x} = 0 over {[}\emph{t}0\emph{,
t}1{]}. □ Formally, the initial state \emph{x}0 can be determined by
solving the equation
\end{quote}

\emph{M}(\emph{t}0\emph{, t}1)\emph{x}0 =∫ \emph{t}1 Φ\emph{T}(\emph{t,
t}0)\emph{CT}(\emph{t})\emph{v}(\emph{t})\emph{dt,}

\begin{quote}
which has a unique solution if and only if \emph{M}(\emph{t}0\emph{,
t}1) is positive definite. In the time-invariant case though, state
determination is done by means of an \emph{observer} as explained in
Chapter 6.
\end{quote}

\textbf{3.5. Observability for Time-Invariant Systems}

\begin{quote}
In this section we shall specialize to the class of time-invariant
systems and characterize ker \emph{M}(\emph{t}0\emph{, t}1) in terms of
the matrix pair (\emph{C, A}). As a companion to Theorem 3.2.3 we have
the following theorem.

Theorem 3.5.1. \emph{Let A be n × n and C be m × n. Then for all
t}0\emph{, t}1 \emph{such that t}0 \emph{\textless{} t}1 \emph{we
have}\\
 \emph{CA} \emph{.} \\
\emph{C}\\
\\
\emph{CAn−}1 \emph{.}  ker \emph{M}(\emph{t}0\emph{, t}1) = ker
\emph{.}\\
\emph{.}

The proof of this theorem is analogous to that of Theorem 3.2.3.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
26
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
3. REACHABILITY AND OBSERVABILITY

Definition 3.5.2. The observability matrix Ω is defined as  \emph{CA}
\emph{.} \\
\emph{C}

Definition 3.5.3. Let \emph{n} be the dimension of the state space. The
pair\emph{CAn−}1 \emph{.} \\
Ω ≜ \emph{.} \emph{.}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
(\emph{C, A}) is said to be completely observable if Ω has full rank,
i.e.
\end{quote}

rank Ω = \emph{n.}

\begin{quote}
Example 3.5.4. Consider the pair (\emph{C, A}), where \emph{A} =
{[}\emph{−}1 0 \emph{−}1 0
\end{quote}

{]} and

\begin{quote}
servability matrix Ω is {[}1 \emph{−}1{]}\\
\emph{C} = , describing the electrical network in Example 1.3.2. The ob-
\end{quote}

Ω = {[} \emph{CA}

\emph{C}

\begin{quote}
{]} = {[}\emph{−}1 1 \emph{−}1 1 {]} \emph{.}

Since rank Ω = 1 the system is not completely observable and
\end{quote}

ker Ω = \emph{\{λ} {[} 1 {]} ; \emph{λ ∈} R\emph{\}.}

1

\begin{quote}
Can we observe any difference between the initial states {[} 1
\end{quote}

2 {]} and {[}\emph{−}1 0 {]} ? No,

\begin{quote}
since {[}\emph{−}1 0 {]}\emph{−}{[} 1
\end{quote}

2 {]} = {[}\emph{−}2

\emph{−}2 {]}\emph{∈} ker Ω\emph{.}

\begin{quote}
If the initial states are {[} 2 {]} and {[} 8 {]} ? Yes, since
{[}\emph{−}6 {]} \emph{/∈} ker Ω. □

1 2 1

Example 3.5.5. The pair (\emph{C, A}) in Example 1.3.1 is \emph{A} = {[}
0 0 {]} and

0 1

\emph{C} =

so (\emph{C, A}) is completely observable. {[}1 0{]} and the
observability matrix is Ω = {[} 0 1 {]} , which is of full rank,

□\\
1 0

For a time-invariant system, not completely observable, ker Ω
constitutes
\end{quote}

an interesting subspace of the state space, which we will call the
\emph{unobservable}

\begin{quote}
\emph{subspace}. The following important lemma is easy to prove.

Lemma 3.5.6. \emph{The unobservable subspace is A−invariant.}

Remark 3.5.7. By partitioning the state space as R\emph{n}= ker
Ω+\emph{V} , where
\end{quote}

\emph{V} is any complement, we can derive a decomposition theorem
analogous to

that of Example 3.2.9. Moreover, by combining these two decompositions

and partitioning the state space into four particular subspaces we get
the

\begin{quote}
\emph{Kalman decomposition} of Chapter 5. □

3.7. DUALITY 27
\end{quote}

\textbf{3.6. Observability for Discrete-Time Systems}

\begin{quote}
We will only study time-invariant systems. Consider the discrete-time
system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(Σ)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t});
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t}0) = \emph{x}0
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t}) + \emph{Du}(\emph{t})\emph{.}

\begin{quote}
Can we distinguish between a system with \emph{x}(0) = \emph{a} and a
system with \emph{x}(0) = \emph{b} by observing \emph{u} and \emph{y} on
the interval {[}0\emph{, T}{]}? By solving the system equations we get
that
\end{quote}

\emph{x}(0) = \emph{a ⇒ ya}(\emph{t}) = \emph{CAta}
+∑\emph{CAt−s−}1\emph{Bu}(\emph{s}) + \emph{Du}(\emph{t})

\begin{quote}
and
\end{quote}

\emph{x}(0) = \emph{b ⇒ yb}(\emph{t}) = \emph{CAtb}
+∑\emph{CAt−s−}1\emph{Bu}(\emph{s}) + \emph{Du}(\emph{t})\emph{.}

\begin{quote}
The conclusion is now that \emph{ya}(\emph{t}) = \emph{yb}(\emph{t}) for
all \emph{t ∈} {[}0\emph{, T}{]} if and only if \emph{CAt}(\emph{a − b})
= 0 for \emph{t} = 0\emph{,} 1\emph{, ..., T}, i.e.

Consequently, the observability properties of a discrete-time system is
de-  \emph{CAT} \emph{CA} \emph{.} \emph{.}  (\emph{a − b})
= 0\emph{.}\\
\emph{C}

termined by Ω, where Ω is defined as in Definition 3.5.2, and we say
that the system is completely observable if rank Ω = \emph{n}.
\end{quote}

\textbf{3.7. Duality between reachability and observability}

\begin{quote}
As we have seen reachability is determined by the pair (\emph{A, B}) and
ob-servability by the pair (\emph{C, A}). There is an important duality
between reach-ability and observability which amounts to reversing the
direction of time and transposing matrices determining the dynamics. We
shall illustrate this important principle, to which we shall return in
Chapter 9, in the time-invariant case.

Consider the controlled system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(21)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
˙\emph{x} = \emph{Ax} + \emph{Bu};
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(\emph{t}0) = \emph{a}

\emph{z}(\emph{t}1) = \emph{b}
\end{quote}
\end{minipage}} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
and the dual observed system

(22) \{˙\emph{z} = \emph{−ATz};
\end{quote}

\emph{y} = \emph{BTz}
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
The first thing to observe is the following

28 3. REACHABILITY AND OBSERVABILITY

Proposition 3.7.1. \emph{The system} (21) \emph{is completely reachable
if and only}

\emph{if} (22) \emph{is completely observable.}

\textbf{Proof:} This is immediately clear from the fact that

To illustrate the more general principle let us consider the following
rank (\emph{−}1)\emph{−}1\emph{BT}(\emph{AT})\emph{n−}1
\emph{−BTAT} ...   = rank {[}\emph{B} \emph{AB}
\emph{An−}1\emph{B}{]} . □ \emph{BT}
\end{quote}

example. Can we control the system (21) so that it is transferred from
state

\begin{quote}
\emph{x}(\emph{t}0) = \emph{a} at time \emph{t}0 to a state at time
\emph{t}1 such that \emph{bTx}(\emph{t}1) = 0?

To answer this question let us form

\emph{d}\\
\emph{dt}(\emph{zT x}) = \emph{zT} ˙\emph{x} + ˙\emph{zT x}
\end{quote}

= \emph{zTAx} + \emph{zTBu − zTAx}

= \emph{yTu}

\begin{quote}
which integrated between \emph{t}0 and \emph{t}1 yields
\end{quote}

\emph{bTx}(\emph{t}1) \emph{− z}(\emph{t}0)\emph{Ta} =∫ \emph{t}1
\emph{u}(\emph{t})\emph{Ty}(\emph{t})\emph{dt.}

Therefore, (21) can be controlled so that \emph{bTx}(\emph{t}1) = 0 if
and only if there is

\begin{quote}
a control \emph{u} so that
\end{quote}

\emph{aTz}(\emph{t}0) = \emph{−}∫ \emph{t}1
\emph{u}(\emph{t})\emph{Ty}(\emph{t})\emph{dt.}

This can be interpreted as a condition for observing
\emph{aTz}(\emph{t}0), i.e. for ob-

taining a linear functional of the observed function \emph{y}(\emph{t})
so that \emph{aTz}(\emph{t}0) is

\begin{quote}
reconstructed. In this sense, \emph{bTx}(\emph{t}1) = 0 can be reached
if and only if

\emph{aTz}(\emph{t}0) can be observed.

These ideas will be further elaborated upon in Chapter 9.
\end{quote}

CHAPTER 4

\textbf{Stability}

\begin{quote}
Stability is one of the most central concepts in systems and control. In
most engineering projects unstable systems are useless. Therefore in
system analysis and control design stability (or stabilization) is
almost always the first priority to be met.

This chapter deals only with the stability of time-invariant linear
sys-tems, a subject which is drastically simplified by the fact that the
complete set of solutions of the system ˙\emph{x} = \emph{Ax} can be
displayed explicitly by means of the Jordan form. As a consequence, it
is enough to check the eigenvalues of \emph{A} in order to determine the
stability of a system.
\end{quote}

\textbf{4.1. Stability of a dynamical system}

\begin{quote}
Let us study a homogeneous linear system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(23)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
˙\emph{x} = \emph{Ax};
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(0) = \emph{x}0\emph{,}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{A} is \textbf{constant}.

Definition 4.1.1. The system (23) is \emph{stable} if the solution is
bounded on the interval {[}0\emph{, ∞}) for all initial values \emph{x}0
and \emph{asymptotically stable} if \emph{x}(\emph{t}) \emph{→} 0 when
\emph{t → ∞} for all \emph{x}0.

Remark 4.1.2. This definition is equivalent to the so-called Lyapunov
stability for linear systems. However, for nonlinear systems, being
asymp-totically stable requires more conditions than just the
convergence of all solutions.

Theorem 4.1.3.

(1) \emph{The system} (23) \emph{is asymptotically stable if and only if
the real parts of all the eigenvalues of A are negative, i.e., the
eigenvalues are all located in the open left half plane.}

(2) \emph{The system} (23) \emph{is unstable if A has at least one
eigenvalue in the} \emph{open right half plane.}

\textbf{Proof:} In this proof we shall use a fundamental result from
linear algebra, the \emph{Jordan decomposition theorem}. This theorem
guarantees the existence of a basis for R\emph{n}in which the
representation of the linear mapping \emph{A} takes a particularly
simple form.
\end{quote}

29

\begin{quote}
30 4. STABILITY

Transform the matrix \emph{A} to Jordan form \emph{A} = \emph{TJT−}1,
where \emph{J} is a

block-diagonal matrix
\end{quote}

\emph{J} = diag(\emph{J}1\emph{, J}2\emph{, ..., Jr})

\begin{quote}
and each \emph{dν × dν} block \emph{Jν} has the form

\emph{Jν} =  \emph{λν}

0\\
\emph{λν}\\
1

...\\
1

\emph{λν}\\
0

1 \emph{,}

\emph{λν} being an eigenvalue of \emph{A}. Thus,

\emph{eAt}= \emph{T} \emph{eJ}2\emph{t} \\
\emph{eJ}1\emph{t} 0

 0 ...
\end{quote}

\emph{eJrt} \emph{T −}1\emph{,}

\begin{quote}
so it remains to analyze each \emph{eJνt}. But \emph{Jν} has the form
\end{quote}

\emph{Jν} = \emph{λνI} + \emph{Sν}

\begin{quote}
where \emph{Sν} is a shift matrix

\emph{Sν} =\\
 0 1

0 ... \\
0 1 0

0 ... 1
\end{quote}

0

of dimension \emph{dν × dν}, having the property that \emph{Si}= 0 for
\emph{i ≥ dν}. Conse-quently,

\emph{eJνt}= \emph{eλνteSνt}= \emph{eλνt}(\emph{I} + \emph{tS}
+\emph{t}2 2!\emph{S}2 + \emph{...} + (\emph{dν −} 1)!\emph{Sdν−}1)
\emph{tdν−}1

\begin{quote}
and therefore, setting \emph{σν} = Re \emph{λν} and \emph{ων} = Im
\emph{λν},

(24) \emph{eJt}=∑\emph{eσνtPν}(\emph{t}) (cos \emph{ωνt} + \emph{j} sin
\emph{ωνt})\emph{,}
\end{quote}

where \emph{Pν}(\emph{t}) is a matrix-valued polynomial of dimension
\emph{dν −} 1 in \emph{t}. From

this expression it follows that (1) \emph{eAtx}0 \emph{→} 0 for all
\emph{x}0 if and only if \emph{σν} ≜

\begin{quote}
Re \emph{λν \textless{}} 0 for all \emph{ν} and that (2) \emph{eAtx}0
\emph{→ ∞} for at least one \emph{x}0 if some \emph{σν \textgreater{}}
0. □

Corollary 4.1.4. \emph{The system} (23) \emph{is stable if and only if
all eigenvalues}
\end{quote}

\emph{of A are located in the closed left half plane and any eigenvalues
on the}

\begin{quote}
\emph{imaginary axis correspond to one-dimensional Jordan blocks.}

4.2. INPUT-OUTPUT STABILITY 31

\textbf{Proof:} By Theorem 4.1.3 (1) we only need to worry about terms
in (24) for which \emph{σν} = 0 i.e. \emph{eσνt}= 1. These terms will
remain bounded if and only if the degree of \emph{Pν} is zero, i.e.,
\emph{dν} = 1. □

Definition 4.1.5. \emph{A} is a \emph{stable matrix} if Re
\emph{λ}(\emph{A}) \emph{\textless{}} 0.

Example 4.1.6 (Time-varying systems). Consider a time-varying sys-tem
˙\emph{x}(\emph{t}) = \emph{A}(\emph{t})\emph{x}(\emph{t}), is it true
that Re \emph{λ}(\emph{A}(\emph{t})) \emph{\textless{}} 0 for every
\emph{t} implies stability? The answer is no, as can be seen from the
following counterexam-ple.

can be calculated from Let \emph{A}(\emph{t}) = {[}\emph{−}1 0 \emph{−}1
\emph{e}2\emph{t} {]} . The eigenvalues are a priori time varying and

which yields that \emph{λ}(\emph{t}) = \emph{−}1 for all \emph{t}.
det(\emph{λ}(\emph{t})\emph{I − A}(\emph{t})) =���� \emph{λ}(\emph{t}) +
1 0 \emph{λ}(\emph{t}) + 1\emph{−e}2\emph{t} ���� = 0\emph{,}
\end{quote}

commute, so by Remark 2.2.2 the transition matrix is given by The next
step is to calculate \emph{φ}(\emph{t, s}). It easily seen that
\emph{A}(\emph{t}) and∫ \emph{t sA}(\emph{τ})\emph{dτ}

\begin{quote}
\emph{φ}(\emph{t, s}) = \emph{e}∫ \emph{t sA}(\emph{τ})\emph{dτ}= exp(∫
\emph{t−I} + \emph{e}2\emph{τ} {[} 0 0 {]} \emph{dτ})\emph{.} 0 1

The two terms in the exponent commute, so the product rule for matrix
exponentials is applicable and gives
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{φ}(\emph{t, s}) = \emph{es−t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[} 1

0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{e}2\emph{t−e}2\emph{s} 2
\end{quote}

1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{]}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage}} \\
\multicolumn{5}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.8333} + 8\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
Since \emph{φ}12(\emph{t, s}) is unbounded for \emph{t \textgreater{}
s}, the system is unstable.
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\textbf{4.2. Input-output stability}

\begin{quote}
Now we consider a linear control system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
(25)
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
˙\emph{x} = \emph{Ax} + \emph{Bu}\\
\emph{y} = \emph{Cx.}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
We want a bounded input to give a bounded output, which is sometimes
abbreviated as \emph{BIBO-stability}.

Definition 4.2.1. The system (25) is \emph{input-output} stable if there
is a \emph{k}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
such that
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{x}(0) = 0\emph{,}
\end{minipage} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.3750} + 4\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{t ∈} {[}0\emph{, ∞})
\end{quote}
\end{minipage}}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{⇒ ∥y}(\emph{t})\emph{∥ ≤ k,}
\end{quote}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{t ∈} {[}0\emph{, ∞})\emph{.}
\end{quote}
\end{minipage}} \\
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
Since
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{∥u}(\emph{t})\emph{∥ ≤} 1
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{y}(\emph{t}) =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.3750} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{CeA}(\emph{t−s})\emph{Bu}(\emph{s}) \emph{ds,}
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
32 4. STABILITY

defining \emph{G}(\emph{t}) ≜ \emph{CeAtB},

\emph{∥y}(\emph{t})\emph{∥ ≤}∫ \emph{t∥G}(\emph{t − s})\emph{∥
∥u}(\emph{s})\emph{∥ ds} (since \emph{∥u}(\emph{t})\emph{∥ ≤} 1)

(26) \emph{≤}∫ \emph{t∥G}(\emph{σ})\emph{∥ dσ}
\end{quote}

\emph{≤ ∥C∥ ∥B∥}∫ \emph{t∥eAσ∥ dσ,}

\begin{quote}
i.e. a sufficient for condition for input-output stability is that the
integral∫ \emph{∞}0\emph{∥eAt∥ dt} is convergent. Theorem 4.2.2.
\emph{If A is a stable matrix then the time-invariant system} (25)
\emph{is input-output stable. Moreover, if} (\emph{A, B}) \emph{is
reachable and} (\emph{C, A}) \emph{is observable,} (25) \emph{is
input-output stable only if A is stable.}

\textbf{Proof:} If all eigenvalues of \emph{A} have negative real parts
so that all \emph{σν} in (24) are negative then\\
∫ \emph{∞∥eAt∥ dt \textless{} ∞}\\
and hence, in view of (26) the system is input-output stable. The second
assertion can be shown using the Jordan form and is omitted here. □

Remark 4.2.3. Sometimes one prefers to define the norm of a signal as
the energy it contains, i.e., \emph{∥u∥}2 Theorem 4.2.2 still hold if we
replace \emph{∥ · ∥} in Definition 4.2.1 with \emph{∥ · ∥}2. 2:=∫
\emph{∞}0\emph{∥u}(\emph{s})\emph{∥}2\emph{ds.} The conclusions in
\end{quote}

\textbf{4.3. The Lyapunov equation}

\begin{quote}
The \emph{second method of Lyapunov} utilizes a so-called Lyapunov
function to investigate stability of dynamical systems. When
specializing the method to the linear case the so-called \emph{Lyapunov
equation} appears. Lyapunov equations also occur in many other control
problems.

Consider again system (23). Let a quadratic function be defined as
\emph{V} (\emph{x}) = \emph{xTPx}, where \emph{P} is a symmetric matrix.

Definition 4.3.1. \emph{V} (\emph{x}) (correspondingly \emph{P}) is said
to be \emph{positive defi-nite} if \emph{V} (\emph{x})
\emph{\textgreater{}} 0 \emph{∀x ̸}= 0, to be \emph{positive
semi-definite} if \emph{V} (\emph{x}) \emph{≥} 0 \emph{∀x ̸}= 0.

A positive definite \emph{V} can be viewed as an energy function and a
stable system should dissipate energy all the time, namely ˙\emph{V}
should always be negative. Thus for a stable system we expect ˙\emph{V}
= \emph{xT}(\emph{ATP} + \emph{PA})\emph{x} to be negative for all
nonzero \emph{x} with some positive definite \emph{P}. This rational
leads to the following Lyapunov equation

(LE) \emph{ATP} + \emph{PA} + \emph{Q} = 0\emph{,}

where both \emph{P} and \emph{Q} are symmetric. For given matrices
\emph{A} and \emph{Q} this is a linear equation in the unknown \emph{P},
and we have the following lemma.

4.3. THE LYAPUNOV EQUATION 33

Lemma 4.3.2. \emph{Let A be a stable matrix. Then} (LE) \emph{has a
unique solution}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(27)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{P} =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{∞}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{eAT tQeAtdt.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Remark 4.3.3. It is obvious that if \emph{Q} is positive definite
(\emph{Q \textgreater{}} 0), then \emph{P \textgreater{}} 0. Moreover,
even for some positive semi-definite \emph{Q}, \emph{P} can still be
positive definite, as our discussion below will show. On the other hand,
the sum \emph{ATM} + \emph{MA} may be sign indefinite even if \emph{A}
is stable and \emph{M} is a positive definite matrix.

\textbf{Proof:} Since Re \emph{λ}(\emph{A}) \emph{\textless{}} 0, the
integral is convergent, for the same reason as in the proof of Theorem
4.2.2. Now, straight-forward differentiation yields

\emph{d}\\
\emph{dt}(\emph{eAT tQeAt}) = \emph{AT eAT tQeAt} + \emph{eAT tQeAtA.}

Integrating this from 0 to \emph{∞} yields 0\emph{−Q} = \emph{ATP}
+\emph{PA}, i.e. \emph{P} satisfies (LE). It remains to show that there
is only one solution. Define the linear map \emph{L} : R\emph{n× n→}
R\emph{n× n}as\\
\emph{L}(\emph{P}) ≜ \emph{ATP} + \emph{PA}

Since (LE) has a solution for all \emph{Q}, Im \emph{L} = R\emph{n×n},
i.e. \emph{L} has full rank. Hence \emph{L}(\emph{P}) = \emph{−Q} has a
unique solution for each \emph{Q}. By specializing the structure of (LE)
we are able to give the main the-□

orem of this section.

Theorem 4.3.4. \emph{Let} (\emph{C, A}) \emph{be an observable pair and
consider the equa-tion}

(28) \emph{ATP} + \emph{PA} + \emph{CTC} = 0\emph{.}

\emph{Then the following statements are equivalent}

(1) \emph{A is a stable matrix}\\
(2) (28) \emph{has a positive definite solution P.}

\textbf{Proof:} First, suppose \emph{A} is a stable matrix. Then, by
Lemma 4.3.2, the unique solution \emph{P} of (28) is given by
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{P} =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{∞}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{eAT tCTCeAtdt.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
The observability Gramian of the pair (\emph{C, A}) on the interval
{[}0\emph{, t}{]} is
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{M}(0\emph{, t}) =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{eAT sCTCeAsds.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
By observability, the matrix \emph{M}(0\emph{, t}) is positive definite
for all \emph{t \textgreater{}} 0 and we conclude that \emph{P
\textgreater{}} 0.

34 4. STABILITY

Conversely, suppose that \emph{P} is positive definite and form the
Lyapunov function \emph{V} (\emph{x}) ≜ \emph{xTPx}, which is strictly
positive for all \emph{x ̸}= 0. Differentia-tion of \emph{V} along a
solution of ˙\emph{x} = \emph{Ax} yields

\emph{d}\\
\emph{dt}(\emph{xT Px}) = ˙\emph{xT Px} + \emph{xT P} ˙\emph{x} =
\emph{xT} (\emph{AT P} + \emph{PA})\emph{x} = \emph{−xT CT Cx.}

Integrating this from \emph{t}0 to \emph{t} we obtain

(29) \emph{V} (\emph{x}(\emph{t})) \emph{− V} (\emph{x}(\emph{t}0)) =
\emph{−}∫ \emph{t}1
\emph{\textbar Cx}(\emph{s})\emph{\textbar{}}2\emph{ds ≤} 0

and hence,\\
0 \emph{≤ V} (\emph{x}(\emph{t})) \emph{≤ V} (\emph{x}(\emph{t}0))\\
for all \emph{t ≥ t}0. This implies that all solutions of ˙\emph{x} =
\emph{Ax} remain bounded and from Corollary 4.1.4 it follows that
Re\emph{λ}(\emph{A}) \emph{≤} 0 and that imaginary eigenvalues, if any,
correspond to one-dimensional Jordan blocks.

We shall now exclude the possibility of such imaginary eigenvalues. It
is easily verified, using the Jordan form, that the existence of
imaginary eigenvalues is equivalent to the existence of a nontrivial
periodic solution of˙\emph{x} = \emph{Ax}. Hence, it is enough to show
that there is no nontrivial periodic solution.

Suppose \emph{x}(\emph{t}) is a periodic solution such that
\emph{x}(\emph{t}0) = \emph{x}(\emph{t}1). By (29) and the continuity of
\emph{x}(\emph{t}) it now holds that \emph{Cx}(\emph{t}) \emph{≡} 0 on
the interval {[}\emph{t}0\emph{, t}1{]}, and by periodicity it holds for
all \emph{t}. Differentiation of this identity yields

\emph{C} ˙\emph{x}(\emph{t}) \emph{≡ CAx}(\emph{t}) \emph{≡} 0\emph{,}\\
and by induction it follows \emph{CAkx}(\emph{t}) \emph{≡} 0 for all
\emph{k}. Since (\emph{C, A}) is observable, it follows that
\emph{x}(\emph{t}) \emph{≡} 0. We demonstrate that Theorem 4.3.4 leads
to a stability test. □

Corollary 4.3.5. \emph{Let Q be symmetric and positive definite}
(\emph{Q \textgreater{}} 0)\emph{. Then the following statements are
equivalent}

(1) \emph{A is a stable matrix}\\
(2) (LE) \emph{has a positive definite solution P.}

\textbf{Proof:} If \emph{Q \textgreater{}} 0 it can be factored as
\emph{CTC} = \emph{Q}, with \emph{C} invertible and it trivially holds
that (\emph{C, A}) is observable. □ By dualizing we get the following
corollary, important in stochastic sys-tems theory.

Corollary 4.3.6. \emph{Let} (\emph{A, B}) \emph{be a reachable pair and
consider the equa-tion}

(30) \emph{AP} + \emph{PAT}+ \emph{BBT}= 0\emph{.}

\emph{Then the following statements are equivalent}

(1) \emph{A is a stable matrix}\\
(2) (30) \emph{has a positive definite solution P.}

4.4. STABILITY OF DISCRETE-TIME SYSTEMS 35

\textbf{Proof:}Use the facts that (\emph{A, B}) is reachable if and only
if (\emph{BT, AT}) is ob-servable, and that \emph{A} is stable if and
only if \emph{AT}is stable. □

Example 4.3.7.

Is \emph{A} = {[}\emph{−}1 \emph{−}2 {]} a stable matrix?\\
0 1
\end{quote}

Choose \emph{Q} = \emph{I} which is positive definite as required and
solve the Lyapunov

\begin{quote}
equation (LE) for the symmetric matrix \emph{P} = {[} \emph{p}11
\emph{p}12 \emph{p}12 \emph{p}22 {]} yielding

and \emph{A} is a stable matrix. \emph{⇒} \emph{−}2\emph{p}12 + 1
\emph{p}11 \emph{−} 2\emph{p}12 \emph{− p}22 2\emph{p}12 \emph{−}
4\emph{p}22 + 1 = 0
\end{quote}

= 0

\begin{quote}
= 0 \emph{⇒ P} = 1 {[} 3 1 1 1 {]} \emph{\textgreater{}} 0\emph{,}
\end{quote}

□

\textbf{4.4. Stability of discrete-time systems}

\begin{quote}
Here we give the analogous results for discrete-time systems leaving the
proofs as an exercise for the reader. Consider the homogeneous system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(31)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t})
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(0) = \emph{x}0
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{A} is an \emph{n × n} constant matrix.\\
Theorem 4.4.1.

(1) \emph{The system in} (31) \emph{is asymptotically stable if and only
if \textbar λ}(\emph{A})\emph{\textbar{} \textless{}} 1 \emph{i.e. if
and only if all the eigenvalues of A are located strictly inside}
\emph{the unit circle.}

(2) \emph{The system is unstable if there is at least one eigenvalue
outside} \emph{the unit circle.}

Definition 4.4.2. \emph{A} is a stable matrix if
\emph{\textbar λ}(\emph{A})\emph{\textbar{} \textless{}} 1. Consider the
discrete Lyapunov equation

(DLE) \emph{P} = \emph{ATPA} + \emph{Q}

Lemma 4.4.3. \emph{Let A be a stable matrix. solution,}

\emph{Then} (DLE) \emph{has a unique}
\end{quote}

\emph{P} =∑(\emph{AT})\emph{kQAk}

\begin{quote}
Theorem 4.4.4. \emph{Let} (\emph{C, A}) \emph{be observable and let P be
a solution of}

(DLE) \emph{with Q} = \emph{CTC. Then the following statements are
equivalent}

(1) \emph{A is a stable matrix}

(2) \emph{P \textgreater{}} 0\emph{.}

36 4. STABILITY

Example 4.4.5. The discrete Lyapunov equation leads to a stability
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
test.

\emph{A} = {[} 0\emph{−}1 \emph{−}5 {]}\\
1
\end{quote}

Choose \emph{Q} = \emph{I} and solve the equation \emph{P − ATPA} =
\emph{I} for the symmetric matrix \emph{P}.

\begin{quote}
Sylvesters test yields  \\
\emph{p}11 \emph{−}1

\emph{−p}11 +\uline{5} 6\emph{p}12 \emph{−} 5 36\emph{p}22 36\emph{p}22
3\emph{p}12 + \sout{11} 36\emph{p}22 = 1 = 0 = 1 \emph{⇒ P} = {[} 67 21
1 2 5 {]}

67 60\emph{\textgreater{}} 0 det \emph{P} = 111 25\emph{\textgreater{}}
0 \emph{⇒ P \textgreater{}} 0
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
showing that \emph{P} is positive definite, and hence \emph{A} is a
stable matrix.
\end{quote}

CHAPTER 5

\textbf{Realization theory}

\begin{quote}
Finite-dimensional time-invariant linear systems of the form

(32) \{˙\emph{x}(\emph{t}) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t})
\end{quote}

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t}) + \emph{Du}(\emph{t})

\begin{quote}
and\\
\{ \emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t})
\end{quote}

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t}) + \emph{Du}(\emph{t})

\begin{quote}
are of fundamental interest, both in applications and in theory. These
repre-sentations exhibit the state explicitly as a dynamical memory of
the system, making them ideally suited for estimation and control where
efficient algo-rithms can be derived.

The system (32) defines an input/output mapping

(33) \emph{y}(\emph{t}) =∫ \emph{t G}(\emph{t, τ})\emph{u}(\emph{τ})
\emph{dτ} + \emph{D}(\emph{t})\emph{u}(\emph{t})\emph{.}

Conversely, given an input/output description as in (33) we would like
to know if there is a finite-dimensional time-invariant system realizing
the input/-output behavior and such a representation is consequently
called a \emph{realization} of the input/output representation. Clearly,
realizations having as few as possible state variables are of special
interest, both from a compu-tational and a theoretical point of view,
and are called \emph{minimal}.

A major theme in systems theory is that reachability and observability
should be equivalent to minimality. The intuition behind this is the
idea that states that cannot be reached or observed should not affect
the input/output behavior of a system and therefore account for
non-minimality.

Definition 5.0.6. The dimension of a realization (\emph{A, B, C, D}) is
defined as the dimension of \emph{A}.

Definition 5.0.7. The system (32) is a \emph{minimal} of the
input/output representation (33) if there is no other realization of
(33) of lower dimension.

Two major tasks of realization theory is to analyze properties of
re-alizations such as minimality, and to investigate the problem of
realizing input/output behaviors as state space systems.

In this chapter we shall mainly discuss continuous-time systems.
How-ever, many properties are common for continuous- and discrete-time
sys-tems. Moreover, many results will be relevant for \emph{stochastic}
systems as well.
\end{quote}

37

\begin{quote}
38 5. REALIZATION THEORY

As it turns out, the three properties linearity, finite dimensionality
and time invariance together define a nice class of systems which is
sometimes referred to as the class of \emph{rational} systems, a term
explained later in this chapter. The attributes continuous/discrete-time
and deterministic/stochastic are of-ten interchangeable within the class
of rational systems.
\end{quote}

\textbf{5.1. Realizability and rationality}

\begin{quote}
We shall now address the question of existence of a system (32)
realizing a given input/output description. Without loss of generality
we consider the input/output system
\end{quote}

\emph{y}(\emph{t}) =∫ \emph{t G}(\emph{t, τ})\emph{u}(\emph{τ})
\emph{dτ,}

\begin{quote}
and consequently we let \emph{D} = 0 in (32). By solving the equations
(32) we get
\end{quote}

\emph{y}(\emph{t}) = \emph{CeAtx}(0) +∫ \emph{t
CeA}(\emph{t−τ})\emph{Bu}(\emph{τ}) \emph{dτ.}

\begin{quote}
By linearity we must have \emph{CeAtx}(0) = 0 for all \emph{t}, which
can be achieved by letting \emph{x}(0) = 0. A necessary condition for
the system (33) to have a finite-dimensional time-invariant realization
is now seen to be that \emph{G}(\emph{t, τ}) be a function of the
difference \emph{t − τ}.

Example 5.1.1. Consider the simple delay system \emph{y}(\emph{t}) =
\emph{u}(\emph{t−}1), which can be written \emph{y}(\emph{t}) = clearly
a function of \emph{t − τ}, but does the system have a
finite-dimensional realization? ∫ \emph{t−∞δ}(\emph{τ −} (\emph{t −}
1))\emph{u}(\emph{τ}) \emph{dτ}. The weighting function is
\end{quote}

□

dimensional time-invariant realizations we shall employ the Laplace
trans-In order to characterize the weighting functions \emph{G}(\emph{t
− τ}) having finite-

\begin{quote}
form. Consider the system\{˙\emph{x}(\emph{t}) = \emph{Ax}(\emph{t}) +
\emph{Bu}(\emph{t})
\end{quote}

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t}) + \emph{Du}(\emph{t})

\begin{quote}
Suppose the system is at rest at \emph{t} = 0, i.e. \emph{x}(0) = 0 .
Then applying the Laplace transform to the system we obtain\\
\{ \emph{s}˜\emph{x}(\emph{s}) = \emph{A}˜\emph{x}(\emph{s}) +
\emph{B}˜\emph{u}(\emph{s})
\end{quote}

˜\emph{y}(\emph{s}) = \emph{C}˜\emph{x}(\emph{s}) +
\emph{D}˜\emph{u}(\emph{s})\emph{,}

\begin{quote}
and therefore

where the matrix-valued function
\end{quote}

˜\emph{y}(\emph{s}) =

{[} \emph{C}(\emph{sI − A})\emph{−}1\emph{B} + \emph{D}

\begin{quote}
{]}\\
˜\emph{u}(\emph{s}) = \emph{R}(\emph{s})˜\emph{u}(\emph{s})\emph{,}

(34) \emph{R}(\emph{s}) ≜ \emph{C}(\emph{sI − A})\emph{−}1\emph{B} +
\emph{D}
\end{quote}

is called the \emph{transfer function} of the system (32). By Cramer's
rule it is

seen that \emph{R}(\emph{s}) is a matrix of proper rational functions
and that \emph{L\{G}(\emph{t})\emph{\}} =

\includegraphics[width=1.98611in,height=0.38889in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image12.png}\includegraphics[width=1.91667in,height=0.375in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image13.png}

\begin{quote}
5.1. REALIZABILITY AND RATIONALITY 39

\emph{C}(\emph{SI − A})\emph{−}1\emph{B} is a matrix of strictly proper
rational functions (recall that a rational function is called
\emph{proper} if the degree of the numerator is less or equal the degree
of the denominator and \emph{strictly proper} if the degree of the
numerator is strictly less than the degree of the denominator). Hence, a
nec-essary condition for \emph{G}(\emph{t}) to have a realization is
that its Laplace transform is a matrix of strictly proper rational
functions.
\end{quote}

Since the Laplace transform of the weighting function is \emph{e−s},
which is not Example 5.1.2. Consider again the simple delay system
\emph{y}(\emph{t}) = \emph{u}(\emph{t−}1).

\begin{quote}
a rational function, the delay system does not have a finite-dimensional
realization. Interpret this result! □

Hence, the system (32) can then be described by the rational matrix
function \emph{R} and we illustrate this as
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
y
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
This leads to a natural inverse problem, namely the \emph{realization
problem}. Given a proper rational matrix function \emph{R}(\emph{s}),
determine (\emph{A, B, C, D}) such that\\
\emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B} +
\emph{D.}

Without loss of generality we may take \emph{D} = 0, because \emph{D} =
\emph{R}(\emph{∞}) can be subtracted from \emph{R}(\emph{s}) forming a
new \emph{R}(\emph{s}) with rational elements such that the degree of
the numerator is lower than that of the denominator.

Remark 5.1.3. There is a very useful interpretation of
\emph{R}(\emph{s}) = \emph{L\{G}(\emph{t})\emph{\}} (for simplicity,
assume \emph{R}(\emph{∞}) = 0), forming the basis for the frequency
domain approach to linear systems.

Suppose that the input/output system is input/output stable, and for
simplicity suppose that the system is single-input/single-output
(\emph{SISO}). Pass a sine wave with frequency \emph{ω} through the
system and let it go to steady state. More precisely, apply the input
\emph{u}(\emph{t}) = sin(\emph{ωt}) = Im \emph{eiωt}to the system
starting at \emph{t} = \emph{−∞}.

ae ιω~t R (s) y

Then
\end{quote}

\emph{y}(\emph{t}) = Im∫ \emph{t−∞G}(\emph{t − τ})\emph{eiωτdτ} =
\emph{\{t − τ} = \emph{s\}}

= Im∫ \emph{∞eiω}(\emph{t−s})\emph{G}(\emph{s}) \emph{ds} = Im
\emph{eiωtR}(\emph{iω})\emph{.}

\begin{quote}
Hence, since \emph{y}(\emph{t}) = \emph{A} sin(\emph{ωt} + \emph{φ}),
where \emph{A} = \emph{\textbar R}(\emph{iω})\emph{\textbar{}} and
\emph{φ} = arg \emph{R}(\emph{iω}), the function \emph{R}(\emph{s})
tells us how the amplitude and phase of a sinusoid is affected by a
linear time-invariant system. For a multivariable system this
interpretation holds for each input/output pair separately. Moreover, a
direct term, i.e., the case of \emph{R}(\emph{∞}) = \emph{D ̸}= 0, can
easily be incorporated into this interpretation as well.

40 5. REALIZATION THEORY
\end{quote}

ality, but only linearity and time invariance. Furthermore, this also
leads Note that this property of \emph{L\{G}(\emph{t})\emph{\}} does not
depend on finite dimension-

to a procedure for determining ˜\emph{G}(\emph{s}) experimentally. By
varying \emph{ω} we can

determine the value of \emph{R} in a number of points and then estimate
˜\emph{G}(\emph{s}) by

\begin{quote}
extrapolation. □
\end{quote}

discrete time system Remark 5.1.4. In the same way, the
\emph{Z}-transform can be applied to the

\{ \emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t})

\emph{y}(\emph{t}) = \emph{Cx}(\emph{t}) + \emph{Du}(\emph{t})

\begin{quote}
to yield

where \emph{R}(\emph{z}) is a \emph{m × k} matrix of rational functions.
˜\emph{y}(\emph{z}) =
\end{quote}

{[} \emph{C}(\emph{zI − A})\emph{−}1\emph{B} + \emph{D}
{]}˜\emph{u}(\emph{z}) = \emph{R}(\emph{z})˜\emph{u}(\emph{z})\emph{.}

is the transfer function of a system of the form (32) with \emph{k}
inputs and \emph{m} Next we show that any \emph{m×k} matrix
\emph{R}(\emph{s}) of a proper rational functions

\begin{quote}
outputs.

Theorem 5.1.5. \emph{Given a proper rational matrix function
R}(\emph{s}) \emph{there are}

\emph{constant matrices} (\emph{A, B, C, D}) \emph{such that}
\end{quote}

\emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B} + \emph{D}

\begin{quote}
We shall give two constructive proofs for Theorem 5.1.5, each proof
\end{quote}

giving a different solution as well as insight into the problem. Without
loss

\begin{quote}
of generality we can assume that \emph{R}(\emph{s}) is strictly proper,
i.e., \emph{R}(\emph{∞}) = 0.

\textbf{Proof:}\# 1(\emph{the standard reachable realization}) Let

(35) \emph{X}(\emph{s}) = \emph{sr}+ \emph{a}1\emph{sr−}1+ \emph{· · ·}
+ \emph{ar}
\end{quote}

be the least common denominator of the elements of \emph{R}(\emph{s}).
Since \emph{R} is strictly

proper, \emph{X}(\emph{s})\emph{R}(\emph{s}) is then a matrix polynomial
of degree less than or equal

\begin{quote}
to \emph{r −} 1, i.e.
\end{quote}

\emph{X}(\emph{s})\emph{R}(\emph{s}) = \emph{N}0 + \emph{N}1\emph{s} +
\emph{N}2\emph{s}2+ \emph{· · ·} + \emph{Nr−}1\emph{sr−}1\emph{.}

\begin{quote}
Define now (\emph{A, B, C}) as follows
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
 0

0 \emph{Ik}

0 \emph{Ik}
\end{quote}

0

\begin{quote}
\emph{rk×rk}=\emph{−arIk· · ·}0

\emph{−ar−}1\emph{Ik}\\
\emph{· · ·}0

\emph{−ar−}2\emph{Ik}\\
\emph{· · ·}0

\emph{m×rk}= {[}\emph{N}0\emph{, N}1\emph{, N}2\emph{, · · · , Nr−}1{]}
\end{quote}\strut
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{· · ·· · ·· · ·· · ·· · ·}
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\\
\strut
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{rk×k}=
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
0

0

\emph{Ik}

\begin{quote}
0\\
...
\end{quote}\strut
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\\

\end{quote}\strut
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{· · ·}\\
\emph{Ik}
\end{quote}\strut
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{−a}1\emph{Ik}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
5.1. REALIZABILITY AND RATIONALITY
\end{minipage} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
41
\end{minipage}} \\
\midrule()
\endhead
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
where \emph{Ik} is an identity matrix of dimension \emph{k × k}, and let
\emph{X}(\emph{s}) =
\end{quote}
\end{minipage} & \multirow{2}{*}{\emph{X}1(\emph{s})

\emph{X}2(\emph{s})

\emph{Xr}(\emph{s}) ...} &
\multirow{2}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\\

\end{quote}\strut
\end{minipage}} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
be the solution of (\emph{sI − A})\emph{X}(\emph{s}) = \emph{B}. i.e.

\emph{sXi} = \emph{Xi}+1 for \emph{i} = 1\emph{,} 2\emph{, · · · , r −}
1\\
\emph{sXr} + \emph{a}1\emph{Xr} + \emph{a}2\emph{Xr−}1 + \emph{· · ·} +
\emph{arX}1 = \emph{Ik}
\end{quote}\strut
\end{minipage} \\
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{1.0000} + 4\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
From this we readily see that \emph{X}(\emph{s})\emph{X}1 = \emph{Ik}
and consequently

\emph{Xi} =\emph{si−}1 \emph{X}(\emph{s})\emph{Ik} \emph{i} = 1\emph{,}
2\emph{, · · · , r}

Therefore

1\\
\emph{C}(\emph{sI − A})\emph{−}1\emph{B} = \emph{CX} =
\emph{X}(\emph{s}){[}\emph{N}0 + \emph{N}1\emph{s} + \emph{N}2\emph{s}2
+ \emph{· · ·} + \emph{Nr−}1\emph{sr−}1{]} = \emph{R}(\emph{s})

and hence (\emph{A, B, C}) defines a realization as required. □ Note
that this

realization is completely reachable because

has full rank \emph{rk}. We will call this realization the
\emph{standard reachable realiza-} {[}\emph{B, AB, · · · ,
Ark−}1\emph{B}{]} = \\
\emph{I}\\
0
\end{quote}

0

\emph{. . . . . . . . . . . . . . . . . . . . . . . . . . .}

\begin{quote}
0 \emph{I}\\
0
\end{quote}

0

\begin{quote}
\emph{∗}\\
\emph{· · ·}\\
\emph{· · ·}

\emph{· · ·}\\
\emph{· · ·}\\
\emph{I}\\
0

\emph{∗}\\
\emph{∗}\\
\emph{I}
\end{quote}

\emph{∗}

\begin{quote}
\emph{∗}\\
\emph{∗}\\
\emph{∗}\\
\emph{∗}

\emph{∗}\\
\emph{∗}\\
\emph{· · ·}\\
\emph{· · ·}

\emph{· · ·}\\
\emph{· · ·}\\
\emph{∗}\\
\emph{∗}

\emph{∗∗}
\end{quote}\strut
\end{minipage}} \\
\bottomrule()
\end{longtable}

\begin{quote}
\emph{tion}.

\textbf{Proof:}\#2 (\emph{the standard observable realization}) We note
that \emph{R}(\emph{∞}) = 0, which implies that \emph{R}(\emph{s}) is
analytic around \emph{s} = \emph{∞} and thus can be expanded in a
Laurent-series

(36) \emph{R}(\emph{s}) = \emph{R}1\emph{s−}1+ \emph{R}2\emph{s−}2+
\emph{R}3\emph{s−}3+ \emph{. . .}

around infinity. (This expansion holds outside a circle around zero in
the complex plane encircling all the poles of \emph{R}.) Moreover, for
any realization (\emph{A, B, C}) it holds that

\emph{C}(\emph{sI − A})\emph{−}1\emph{B} = \emph{s−}1\emph{C}(\emph{I −
As−}1)\emph{−}1\emph{B}\\
= \emph{s−}1\emph{C}{[}\emph{I} + \emph{As−}1+ \emph{A}2\emph{s−}2+
\emph{A}3\emph{s−}3+ \emph{· · ·} {]}\emph{B} = \emph{CBs−}1+
\emph{CABs−}2+ \emph{CA}2\emph{Bs−}3+ \emph{· · ·}= \emph{R}1\emph{s−}1+
\emph{R}2\emph{s−}2+ \emph{R}3\emph{s−}1+ \emph{· · ·} =
\emph{R}(\emph{s})\emph{.}

Hence, the transfer function is uniquely determined by the sequence

\emph{\{R}1\emph{, R}2\emph{, R}3\emph{, . . .\},}\\
which is called the sequence of \emph{Markov parameters} of the transfer
function.

Before proceeding with the proof we need a lemma, showing that the
Markov parameters of a \emph{rational} are give by finite data.

42 5. REALIZATION THEORY

Lemma 5.1.6. \emph{The matrix coefficients of the Laurent expansion}
(36)

\emph{satisfy the recursion}

(37) \emph{Rr}+\emph{i} = \emph{−a}1\emph{Rr}+\emph{i−}1 \emph{−
a}2\emph{Rr}+\emph{i−}2 \emph{− · · · − arRi for i} = 1\emph{,}
2\emph{,} 3\emph{, ...}

\textbf{Proof:} Multiplying together (35) and (36) yields

\emph{X}(\emph{s})\emph{R}(\emph{s}) = (\emph{sr}+ \emph{a}1\emph{sr−}1+
\emph{a}2\emph{sr−}2+ \emph{· · ·} + \emph{ar})(\emph{R}1\emph{s−}1+
\emph{R}2\emph{s−}2+ \emph{R}3\emph{s−}3+ \emph{...})

= \emph{· · ·} + \emph{s−}1(\emph{Rr}+1 + \emph{a}1\emph{Rr} +
\emph{a}2\emph{Rr−}1 + \emph{· · ·} + \emph{arR}1)
\end{quote}

+ \emph{s−}2(\emph{Rr}+2 + \emph{a}1\emph{Rr}+1 + \emph{a}2\emph{Rr} +
\emph{· · ·} + \emph{arR}2)

+ \emph{s−}3(\emph{Rr}+3 + \emph{a}1\emph{Rr}+2 + \emph{a}2\emph{Rr}+1 +
\emph{· · ·} + \emph{arR}3)

+ \emph{· · ·}

\begin{quote}
But \emph{X}(\emph{s})\emph{R}(\emph{s}) is a matrix polynomial and
consequently the coefficients of the negative powers of \emph{s} are
zero and (37) follows. □ We shall now

continue the proof of the theorem. To this end define

\\
0

0\\
\emph{Im}

0 \emph{Im} 0 \emph{· · ·} \emph{· · ·} 0

0  \emph{R}1

\emph{R}2\emph{rm×rm}=\emph{−arIm} \emph{· · ·}0

\emph{−ar−}1\emph{Im}\\
\emph{· · ·}0

\emph{−ar−}2\emph{Im} \emph{· · ·}0 \emph{· · ·} \emph{· · ·} \emph{· ·
·} \emph{−a}1\emph{Im} \emph{· · ·} \emph{Im} 
\emph{rm×k}=\emph{Rr} ...\emph{m×rm}= {[}\emph{Im,} 0\emph{, · ·
· ,} 0{]}

Then, by Lemma 5.1.6 we have
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule()
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{AB} =
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright

\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{R}2
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\\
 =
\end{quote}\strut
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
 \emph{R}2
\end{quote}

\emph{R}3

\emph{Rr}+1 ...
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\\
 \emph{.}
\end{quote}\strut
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{R}3
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
...
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{−a}1\emph{Rr − a}2\emph{Rr−}1 \emph{− · · · − arR}1
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Similarly, by Lemma 5.1.6, we obtain by induction

\emph{AiB} =  \emph{Ri}+1
\end{quote}

\emph{Ri}+2

\begin{quote}
\emph{Ri}+\emph{r} ...   \emph{i} = 0\emph{,} 1\emph{,} 2\emph{,}
3\emph{, ...} \emph{.}

and hence \emph{Ri} = \emph{CAi−}1\emph{B}, showing that (\emph{A, B,
C}) defines a realization. □

5.1. REALIZABILITY AND RATIONALITY 43

This realization is completely observable, because

 \emph{CA}2 \emph{CA} \emph{C}  = \emph{Im}

\emph{. . . . . . . . . . . . . . . . . . . . . .}\\
0 0\\
\emph{Im}\\
0

0 \emph{Im} 0 0 \emph{· · ·} \emph{· · ·} \emph{· · ·} 0 0 0 

has full rank \emph{rm} and it is therefore named \emph{the standard
observable realiza-} \emph{CArm−}1 ...  \emph{. .
. . . . . . . . . . . . . . . . . . . .} 0 \emph{∗} \emph{∗} 0 \emph{∗}
\emph{∗} 0 \emph{∗} \emph{∗} \emph{· · ·} \emph{· · ·} \emph{· · ·}
\emph{Im} \emph{∗} \emph{∗}\\
\emph{tion.}

Example 5.1.7. Consider a system with two inputs and two outputs
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
and having the transfer function \emph{R}(\emph{s}) = {[} \emph{s}+1
\emph{s}+1 {]} 1 2
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[}\\
\emph{s} + 2
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{]} 2\emph{s} + 4
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\begin{minipage}[t]{\linewidth}\raggedright
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{−}1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} \\
\midrule()
\endhead
\emph{s}2+3\emph{s}+2 & \emph{s}+2 \\
\bottomrule()
\end{longtable}
\end{minipage} & & \emph{s}2+ 3\emph{s} + 2 & \emph{−}1 & \emph{s} + 1
& \\
\multicolumn{6}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{1.0000} + 10\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Then\\
\emph{X}(\emph{s}) = \emph{s}2+ 3\emph{s} + 2 = (\emph{s} + 1)(\emph{s}
+ 2)\emph{,} \emph{r} = 2

\emph{a}1 = 3\emph{, a}2 = 2 \emph{N}0 = {[}\emph{−}1 1 {]} \emph{N}1 =
{[} 0 1 {]} 2 4 1 2

and therefore the standard reachable realization is

To determine the standard observable realization, we calculate the
Laurent- \emph{A} = \emph{−}2\\
0

0 \emph{−}2 0 0 \emph{−}3\\
0

0 \emph{−}3 1 0  \emph{B} =  0 1 0 0 0 1   \emph{C} =
{[}\emph{−}1 2 4 1 1 0 2 1 {]} \emph{.}

0 0 0 0 0 1

expansion around \emph{s} = \emph{∞}

which yields  \\
\emph{r}11(\emph{s})

\emph{r}12(\emph{s})

\emph{r}22(\emph{s})

\emph{r}21(\emph{s})\\
=

=

=

= \emph{s}+1= \emph{s−}1(1 + \emph{s−}1)\emph{−}1 = \emph{s−}1 \emph{−
s−}2 + \emph{s−}3 \emph{− s−}4 + \emph{· · ·} \emph{s}+1= 2\emph{s−}1
\emph{−} 2\emph{s−}2 + 2\emph{s−}3 \emph{−} 2\emph{s−}4 + \emph{· · ·}
\emph{s}+2= \emph{s−}1(1 + 2\emph{s−}1) = \emph{s−}1 \emph{−}
2\emph{s−}2 + 4\emph{s−}3 \emph{−} 8\emph{s−}4 + \emph{· · ·}
\emph{s}+2\emph{−} 1 \emph{s}+1= \emph{−s−}2 + 3\emph{s−}3 \emph{−}
7\emph{s−}4 + \emph{· · · ,}

\emph{R}1 = {[} 1 0 2 1 {]} \emph{R}2 = {[}\emph{−}1 \emph{−}1 \emph{−}2
\emph{−}2 {]} \emph{R}3 = {[} 1 3 2 4 {]} \emph{R}4 = {[}\emph{−}1
\emph{−}7 \emph{−}2 \emph{−}8 {]} so the standard observable realization
is

Are these realizations minimal? The answer is that, in general, the
observ- \emph{A} as above \emph{B} = \emph{−}1 \emph{−}1 0 \emph{−}2
\emph{−}2 1  \emph{C} = {[} 1 0 0 1 0 0 0 0 {]} \emph{.}\\
1 2
\end{quote}\strut
\end{minipage}} \\
\bottomrule()
\end{longtable}

\begin{quote}
able and reachable realizations are not minimal.

44 5. REALIZATION THEORY
\end{quote}

\textbf{5.2. Minimality and McMillan degree}

\begin{quote}
If a realization (\emph{A, B, C, D}) of a given transfer function
\emph{R}(\emph{s}) is not min-imal, there are obviously too many states
in the realization. It is a natural guess that there should be a way of
eliminating some of the states to obtain a minimal realization.

Which states should be eliminated? Since we are only concerned with the
input/output behavior of the realization we should try to eliminate the
states that are not affected by the input and the states that do not
affect the output. Hence, a natural candidate for the part of the state
space that should be \emph{kept} is the part of the reachable subspace
that is not included in the unobservable subspace.

The \emph{Kalman decomposition} is way of partitioning the state space
into four subspaces, of which one is a complement to R \emph{∩} ker Ω in
R.

Theorem 5.2.1 (The Kalman decomposition). \emph{Let} (\emph{A, B, C, D})
\emph{define an n-dimensional realization and let} R \emph{be the
reachable subspace and let} ker Ω \emph{be the unobservable subspace.
Let V}¯\emph{o}¯\emph{r and Vor be complements defined by}

ker Ω = R \emph{∩} ker Ω + \emph{V}¯\emph{o}¯\emph{r and}
\end{quote}

R = R \emph{∩} ker Ω + \emph{Vor.}

\emph{Finally let V}¯\emph{ro be the subspace of states that are neither
in} R \emph{nor in} ker Ω\emph{.}

\begin{quote}
(38) R\emph{n}= R \emph{∩} ker Ω + \emph{Vor} +
\emph{V}¯\emph{o}¯\emph{r} + \emph{V}¯\emph{ro.}

\emph{Then the four subspaces in the decomposition} (38) \emph{are
linearly independent and in a basis corresponding to the decomposition
the matrices} (\emph{A, B, C}) \emph{have the following structure}

\emph{A} =  \emph{A}11 0 0 0 \emph{A}12 \emph{A}22 0 0 \emph{A}13

\emph{A}33\\
0

0 \emph{A}14 \emph{A}24 \emph{A}34 \emph{A}44  \emph{and}

\emph{B} =  \emph{B}1 \emph{B}2 0 0   \emph{and C} = {[}0
\emph{C}2 0 \emph{C}4 {]} \emph{.}

\textbf{Proof:} The proof is similar to that of Example 3.2.9 and is
left for the reader. □

Remark 5.2.2. The realization defined by (\emph{A}22\emph{, B}2\emph{,
C}2) is clearly reach-able and observable. □

5.2. MINIMALITY AND MCMILLAN DEGREE 45
\end{quote}

\emph{realization} (\emph{A, B, C}) \emph{depends only on the reachable
and observable part, i.e.} Corollary 5.2.3. \emph{The transfer function
R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B of a}

\begin{quote}
\emph{in the notation of Theorem 5.2.1 it holds that}
\end{quote}

\emph{C}(\emph{sI − A})\emph{−}1\emph{B} = \emph{C}2(\emph{sI −
A}22)\emph{−}1\emph{B}2\emph{.}

\begin{quote}
\textbf{Proof:} It is easily verified that \emph{CAkB} =
\emph{C}2\emph{Ak} 22\emph{B}2, which proves the corol-lary. □ In
particular we have shown the following.

Corollary 5.2.4. \emph{A minimal realization is reachable and
observable.}

We shall now proceed to show that the converse of Corollary 5.2.4 also
holds.

Definition 5.2.5. The \emph{McMillan degree δ}(\emph{R}) of \emph{R} is
the dimension of a minimal realization (\emph{A, B, C, D}) such that
\end{quote}

\emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B} +
\emph{D.}

\begin{quote}
In the following discussion we assume, without loss of generality, that
\emph{R}(\emph{∞}) = 0. The McMillan degree can be related to the rank
of the block-Hankel matrix\\
\emph{R}1 \emph{R}2 \emph{R}2 \emph{R}3 \emph{R}3 \emph{R}4 \emph{· ·
·} \emph{· · ·} \emph{Rr}+1 \emph{Rr} \\
\emph{Hr} =\\
\emph{· · ·} \emph{R}3

\emph{Rr} \emph{Rr}+1\\
\emph{· · ·}\\
\emph{R}4

\emph{Rr}+2 \emph{· · ·} \emph{R}5 \emph{· · ·} \emph{· · ·} \emph{· ·
·} \emph{R}2\emph{r−}1 \emph{Rr}+2 \emph{· · ·} \\
where, as before, \emph{R}1\emph{, R}2\emph{, R}3\emph{, ...,} are the
matrix coefficients of the Laurent expansion

(39) \emph{R}(\emph{s}) = \emph{R}1\emph{s−}1+ \emph{R}2\emph{s−}2+
\emph{R}3\emph{s−}3+ \emph{...}

around \emph{s} = \emph{∞} and \emph{r} = deg \emph{X}, where \emph{X}
is the least common denominator of the elements of \emph{R}. Let
(\emph{A, B, C}) be an arbitrary realization of \emph{R}(\emph{s}) of
dimension \emph{n}. Then
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(40)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B} =
\emph{s−}1\emph{C}(\emph{I − As−}1)\emph{−}1\emph{B} = \emph{CBs−}1+
\emph{CABs−}2+ \emph{CA}2\emph{Bs−}3+ \emph{...}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
By identifying the coefficients of (39) and (40) we see that (\emph{A,
B, C}) is a realization of \emph{R}(\emph{s}) if and only if

\emph{CAi−}1\emph{B} = \emph{Ri} for every \emph{i} = 1\emph{,}
2\emph{,} 3\emph{, ...}

The fundamental insight is now that given a realization (\emph{A, B,
C}), the Hankel matrix can be written as the product of an observability
matrix and

46 5. REALIZATION THEORY

a reachability matrix, i.e.

(41) \emph{Hν} = \emph{CAν−}1 \emph{CA}

... \\
 {[}\emph{B, AB, · · · , Aν−}1\emph{B}{]}\\
\emph{C}
\end{quote}

for all \emph{ν} = 1\emph{,} 2\emph{,} 3\emph{, . . .}. Moreover, if dim
\emph{A} = \emph{n} then rank \emph{Hν ≤ n} for all

\begin{quote}
\emph{ν ≥} 1.
\end{quote}

of triplets (\emph{A, B, C}) realizing a given transfer function
\emph{R}(\emph{s}). This invariance Hence, the sequence of Hankel
matrices \emph{\{Hν\}} is an invariant for the set

\begin{quote}
property will be exploited heavily in the rest of this chapter.

Theorem 5.2.6. \emph{The realization} (\emph{A, B, C, D}) \emph{of a
matrix proper rational}

\emph{functions is reachable and observable if and only if it is
minimal.}
\end{quote}

\textbf{Proof:} Without loss of generality, assume \emph{D} = 0. By
Corollary 5.2.4 it is

enough to show that reachability and observability together imply
minimal-

\begin{quote}
ity. For any realization (\emph{A, B, C}) we let Γ\emph{ν} ≜ {[}\emph{B,
AB, · · · , Aν−}1\emph{B}{]} and

Suppose now that (\emph{A, B, C}) is a reachable and observable
realization of Ω\emph{ν} ≜ \emph{CAν−}1\\
\emph{CA}

... \\
 \emph{.}\\
\emph{C}

dimension \emph{n}. Then it holds that
\end{quote}

rank \emph{Hn} = rank Ω\emph{n}Γ\emph{n ≤} rank Γ\emph{n} = \emph{n.}

Moreover, since rank Ω\emph{T n}Ω\emph{n}= rank Ω\emph{n}= \emph{n} and
rank Γ\emph{n}Γ\emph{T n}= rank Γ\emph{n}= \emph{n}

\begin{quote}
the matrices Ω\emph{T n}Ω\emph{n}and Γ\emph{n}Γ\emph{T n}are \emph{n ×
n} and nonsingular. Hence,
\end{quote}

\emph{n} = rank Ω\emph{T n}Ω\emph{n}Γ\emph{n}Γ\emph{T n≤} rank
\emph{Hn.}

\begin{quote}
Therefore, rank \emph{Hn} = \emph{n}.

For an arbitrary realization ( ˜\emph{A,}˜\emph{B,}˜\emph{C}) of
dimension ˜\emph{n} it holds that
\end{quote}

\emph{n} = rank \emph{Hn} = rank ˜Ω\emph{n}˜Γ\emph{n ≤} rank ˜Γ\emph{n
≤} ˜\emph{n,}

\begin{quote}
showing that the realization (\emph{A, B, C}) is minimal. □

Remark 5.2.7. Given an arbitrary realization Theorem 5.2.6 provides
\end{quote}

a way of checking whether it is minimal, and in the case of
non-minimality

we can use the Kalman decomposition to obtain a minimal realization. In

\begin{quote}
particular, \emph{δ}(\emph{R}) = dim \emph{Vro}. □

There is a way to compute the McMillan degree directly from
\emph{R}(\emph{s}). We

first need a lemma putting a bound on rank \emph{Hν}.

Lemma 5.2.8. \emph{Let r be the degree of the least common denominator
X.}

\emph{Then} rank \emph{Hν} = rank \emph{Hr for all ν ≥ r.}

5.2. MINIMALITY AND MCMILLAN DEGREE 47

\textbf{Proof:} According to Lemma 5.1.6 we can successively extend the
block-Hankel matrix \emph{Hr} with block-rows without increasing the
rank. Then we can do the same thing with the columns and, by Lemma
5.1.6, this will not increase the rank either. □

Theorem 5.2.9. \emph{Let δ}(\emph{R}) \emph{be the McMillan degree of R,
and let r be the degree of the least common denominator of the elements
of R. Then}
\end{quote}

\emph{δ}(\emph{R}) = rank \emph{Hr.}

\begin{quote}
\textbf{Proof:} Let (\emph{A, B, C}) be a minimal realization of
dimension \emph{n} of the strictly proper part of \emph{R}(\emph{s}). As
shown in the proof of Theorem 5.2.4, \emph{n} = rank \emph{Hn}. Since
\emph{X}(\emph{s}) is a divisor of det(\emph{sI − A}), as can be seen
from Cramer's rule, it holds that \emph{r ≤ n}. Hence, by Lemma 5.2.8
rank \emph{Hr} = rank \emph{Hn} = \emph{δ}(\emph{R}). The following
corollary is a consequence of Theorem 5.2.9. □

Corollary 5.2.10. \emph{Let} (\emph{A, B, C, D}) \emph{be a realization
of R}(\emph{s}) \emph{of dimen-sion n, and let r be the degree of the
least common denominator X}(\emph{s}) \emph{of R}(\emph{s})\emph{. Then
r ≤ n. Furthermore, the realization is minimal if and only if the
matrices}

Ω\emph{r} = \emph{CAr−}1 \emph{CA} \emph{...}   \emph{and}
Γ\emph{r} = {[}\emph{B, AB, · · · , Ar−}1\emph{B}{]}\\
\emph{C}

\emph{have full rank.}

Remark 5.2.11. This result can be interpreted in a commutative dia-gram
as in Section 2.1,Chapter 2

\emph{Hr}\\
\emph{U} \emph{−→} \emph{Y}

Γ \emph{↘} \emph{↗} Ω\\
\emph{X}

The goal is to find a factorization \emph{Hr} = Ω\emph{r}Γ\emph{r} over
a state space \emph{X} = R\emph{n}of minimal dimension \emph{n}. □

Example 5.2.12. Let us apply Theorem 5.2.9 to the system defined in
Example 5.1.7. Since \emph{r} = 2, the corresponding Hankel matrix is

\emph{Hr} = \emph{−}1 \emph{−}1 1 0 \emph{−}2 \emph{−}2 2 1
\emph{−}1 \emph{−}1 1 3 \emph{−}2 \emph{−}2 2 4  \\
and rank \emph{Hr} = 3. Hence the McMillan degree \emph{δ}(\emph{R}) = 3
while the standard observable and reachable realizations both have
dimension 4. □

48 5. REALIZATION THEORY

Suppose that we have a minimal realization of a given transfer function
\emph{R}(\emph{s}). Is the realization unique or are there other minimal
realizations? And if there are, in what way are they related to each
other? The answer is given in the \emph{state-space isomorphism
theorem}.

Theorem 5.2.13 (State-space isomorphism theorem). \emph{Let} (\emph{A,
B, C}) \emph{and} ( ˜\emph{A,}˜\emph{B,}˜\emph{C}) \emph{be two minimal
realizations of a strictly proper transfer function.}

\emph{Then there is a nonsingular matrix T such that}

(42) ( ˜\emph{A,}˜\emph{B,}˜\emph{C}) = (\emph{TAT−}1\emph{, TB,
CT−}1)\emph{.}

\textbf{Proof:} If there is a \emph{T} such that the theorem holds then
we necessarily have that
\end{quote}

˜Γ = \emph{T} Γ\emph{.}

\begin{quote}
In the multivariable case Γ is not square, so we cannot solve for
\emph{T} by em-ploying the inverse of Γ. However, since (\emph{A, B}) is
reachable Γ has full row rank and therefore ΓΓ\emph{T}is invertible.
Hence, as a natural candidate for \emph{T} pick
\end{quote}

\emph{T} ≜ (˜ΓΓ\emph{T})(ΓΓ\emph{T})\emph{−}1\emph{.}

\begin{quote}
We shall now show that this choice of \emph{T} satisfies (42). The
following relations tie the two realizations together,

(43) ΩΓ = ˜Ω˜Γ

and

(44) Ω \emph{A} Γ = ˜Ω ˜\emph{A}˜Γ\emph{.}

By multiplying (43) from the right with Γ\emph{T}(ΓΓ\emph{T})\emph{−}1we
get

(45) Ω = ˜Ω\emph{T}

and in particular \emph{C} = ˜\emph{CT}.

Since ( ˜\emph{C,}˜\emph{A}) is observable, ˜Ω has full column rank and
therefore, ˜Ω\emph{T} ˜Ω is invertible. By multiplying (45) from the
left with (˜Ω\emph{T} ˜Ω)\emph{−}1 ˜Ω\emph{T}it follows that another
expression for \emph{T} is
\end{quote}

(˜Ω\emph{T} ˜Ω)\emph{−}1 ˜Ω\emph{T}Ω = \emph{T,}

\begin{quote}
and by multiplying (43) from the left with the same matrix we get
\end{quote}

\emph{T}Γ = ˜Γ

\begin{quote}
and in particular \emph{TB} = ˜\emph{B}.

Finally, multiplying (44) from the right with
Γ\emph{T}(ΓΓ\emph{T})\emph{−}1and from the left with (˜Ω\emph{T}
˜Ω)\emph{−}1 ˜Ω\emph{T}gives that \emph{TA} = ˜\emph{AT}, i.e. ˜\emph{A}
= \emph{TAT−}1. □

5.3. CHARACTERISTIC POLYNOMIAL AND MINIMAL REALIZATION 49
\end{quote}

\textbf{5.3. Characteristic polynomial and minimal realization}

\begin{quote}
Suppose (\emph{A, B, C, D}) is a state space realization of
\emph{R}(\emph{s}). An interesting question is if we can check the
minimality without computing reachability and observability?

Definition 5.3.1. The characteristic polynomial \emph{ρ}(\emph{s}) of a
proper ratio-nal matrix \emph{R}(\emph{s}) is the least common
denominator of all minors of \emph{R}(\emph{s}). The degree of
\emph{ρ}(\emph{s}) is called the degree of \emph{R}(\emph{s}), and
denoted by \emph{deg R}(\emph{s}).
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Example 5.3.2. Consider
\end{quote}
\end{minipage} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.4286} + 4\tabcolsep}}{%
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[} \emph{a s}+2
\end{quote}

\emph{s}+2
\end{minipage}}} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\emph{R}(\emph{s}) =
\end{minipage} & & & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{s}+2 1
\end{minipage} \\
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & & & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{s}+2
\end{minipage} \\
\midrule()
\endhead
\multicolumn{7}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{1.0000} + 12\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Minors of order one are the entries of \emph{R}(\emph{s}), and there is
only one minor of order 2: (\emph{s}+2)2 . Hence the characteristic
polynomial is (\emph{s} + 2)2 if \emph{a ̸}= 1, and \emph{s} + 2 if
\emph{a} = 1.
\end{quote}
\end{minipage}} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Example 5.3.3. Consider
\end{quote}
\end{minipage} & & 1 &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2857} + 2\tabcolsep}}{%
1} & \multirow{3}{*}{{]}} & \\
\multirow{2}{*}{\emph{R}(\emph{s}) =} & {[} & \emph{s}+1 1 &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2857} + 2\tabcolsep}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{s}+1 1
\end{quote}
\end{minipage}} & & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage} \\
& {[} & \emph{s}+2 &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2857} + 2\tabcolsep}}{%
\emph{s}+2} & & \\
\bottomrule()
\end{longtable}

The only minor of order 2 is 0. The characteristic polynomial is
(\emph{s}+1)(\emph{s}+2).

\begin{quote}
Form these examples we see that the characteristic polynomial is in
general different from the least common denominator of all the entries,
as well as from the denominator of the determinant if \emph{R}(\emph{s})
is square.

We state the following result without giving a proof.

Theorem 5.3.4. \emph{A state space realization} (\emph{A, B, C, D})
\emph{is minimal if and only if}
\end{quote}

\emph{dim A} = \emph{deg R}(\emph{s})\emph{.}

\begin{quote}
This result provides an alternative way to verify if a realization is
min-imal. In the following we provide a different method for computing
the degree of a rational matrix.

Assume \emph{R}(\emph{s}) can be factored as

\emph{R}(\emph{s}) = \emph{Nr}(\emph{s})\emph{Dr}(\emph{s})\emph{−}1=
\emph{D−}1 \emph{l} (\emph{s})\emph{Nl}(\emph{s})\emph{,}

where \emph{Dr}(\emph{s}) and \emph{Nr}(\emph{s}) are right coprime, and
\emph{Dl}(\emph{s}) and \emph{Nl}(\emph{s}) are left coprime.

Then,
\end{quote}

\emph{deg R}(\emph{s}) = \emph{deg det}(\emph{Dr}(\emph{s})) = \emph{deg
det}(\emph{Dl}(\emph{s}))\emph{,}

\begin{quote}
where \emph{det}(\emph{·}) stands for the determinant. In the scalar
case where
\end{quote}

\emph{r}(\emph{s}) =\emph{n}(\emph{s}) \emph{d}(\emph{s})\emph{,}

\begin{quote}
50 5. REALIZATION THEORY

coprimeness just implies that there is no zero-pole cancellation. Now
let
\end{quote}

\emph{r}(\emph{s}) =\emph{cp}+1\emph{sp} + \emph{· · ·} +
\emph{c}2\emph{s} + \emph{c}1 \emph{.}

\begin{quote}
\emph{sn}+ \emph{ansn−}1+ \emph{· · ·} + \emph{a}1\\
If there is no zero-pole cancellation, we can easily derive a minimal
realiza-

tion as follows.

Let us introduce a new variable \emph{z}(\emph{t}) by letting
\emph{z}(\emph{s}) = \emph{d−}1(\emph{s})\emph{u}(\emph{s}). Then
\emph{y}(\emph{s}) = \emph{n}(\emph{s})\emph{z}(\emph{s}). Define state
variables

\emph{x}(\emph{t}) =\emph{x}1(\emph{t}) ...   \emph{z}(\emph{t}) ...


or, \emph{xn}(\emph{t}) =\emph{z}(\emph{n−}1)(\emph{t}) \emph{,}

\emph{x}(\emph{s}) =\emph{x}1(\emph{s}) ...   \emph{z}(\emph{s}) ...


Then we have ˙\emph{xi} = \emph{xi}+1 (\emph{sxi}(\emph{s}) =
\emph{xi}+1(\emph{s}))\emph{, i \textless{} n}, and ˙\emph{xn} =
\emph{−}∑\emph{n} \emph{xn}(\emph{s})
=\emph{sn−}1\emph{z}(\emph{s}) \emph{.} 1\emph{aixi} + \emph{u}.

˙\emph{x} = 0 ... 1 ... 0 ... \emph{· · ·} ... 0 ...  0 ...

This is the so-called \emph{reachable canonical form}. We can derive the
\emph{observable} \emph{y} = {[}\emph{c}1\\
\emph{−a}1 0

\emph{· · ·}\\
\emph{−a}2\\
0

\emph{cp}+1\\
\emph{−a}3\\
0

0 \emph{· · ·} \emph{· · ·} \emph{· · ·} \emph{−an} 0{]}\\
1
\end{quote}

\emph{x.} \emph{x} +0

1 \emph{u}

\begin{quote}
\emph{canonical form} in a similar fashion.
\end{quote}

\textbf{5.4. Ho's algorithm}1

\begin{quote}
1 There is a systematic way to factor \emph{Hr} over a state space
\emph{X} of di-mension \emph{n} = rank \emph{Hr}, which enables us to
determine a \emph{minimal} realization
\end{quote}

directly from the Markov parameters. We first recall a result from
linear

\begin{quote}
algebra.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Remark 5.4.1. Every matrix \emph{A} can be transformed by elementary row

are nonsingular matrices \emph{P} and \emph{Q} such that \emph{PAQ} =

We illustrate this result with a numerical example. Let {[} \emph{Ir} 0
{]} {[} \emph{Ir} 0 {]} .

and column operations to the form where \emph{r} = rank \emph{A}. Hence
there

\emph{A} =\\
\emph{−}1 \emph{−}1\\
1
\end{quote}

0

\begin{quote}
\emph{−}2\\
\emph{−}2\\
2

1 \emph{−}1 \emph{−}1 1

3\\
\emph{−}2\\
\emph{−}2 2

4   \emph{.}

1This section is optional.
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
5.4. HO'S ALGORITHM1 51

By first performing elementary row operations and then elementary column
operations we obtain
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}@{}}
\toprule()
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
0\\
1
\end{quote}

\emph{−}1

\emph{−}1\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}1\emph{−}1 1
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}2\emph{−}2 2
\end{quote}

4 \emph{∼}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
0

\begin{quote}
0\\
1
\end{quote}

0\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}1\emph{−}1 1
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}2\emph{−}2 1

0 \emph{∼}
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
0

\begin{quote}
0\\
1
\end{quote}

0\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
0 0\\
0

0
\end{quote}\strut
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} & & & & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} & & & & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
& \multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{−}2\emph{−}2
\end{minipage}} & & & & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & & & & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
3
\end{minipage} & & & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & & & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
and rank \emph{A} = 3. The matrices \emph{P} and \emph{Q} are determined
in the following way.

\emph{PA} = 1 0 0 2 1 0 \emph{−}1 \emph{−}1 1 \emph{−}2 \emph{−}2 1
1\emph{/}2 1 0 0 1 0 0 0 0 1\emph{/}2 0 0  0 0 0 0 \emph{⇒ P}
= 1 0 1 0 

\emph{PAQ} =  1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0   \emph{⇒ Q} = 
 1 0 0 0 \emph{−}2 1 0 0 \emph{−}1 1 1 0 \emph{−}1

\emph{−}1\\
1

1\\
\\
In particular, if \emph{A ≥} 0 and symmetric then \emph{Q} =
\emph{PT}, i.e. there is a nonsin-gular matrix \emph{P} such that

(46) \emph{PAPT}= {[} \emph{Ir} 0 0 0 {]} \emph{.}
\end{quote}

□

\begin{quote}
We now proceed to give the construction of a minimal realization.
Trans-form \emph{Hr} by nonsingular matrices \emph{P, Q} so that
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(47)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{PHrQ} =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[} \emph{In}

0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
0 {]}

0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{,}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
and define the shift operator \emph{σ} as
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{σi\{Hr\}} =
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+1

\emph{Ri}+2

\emph{Ri}+\emph{r}

\begin{quote}
\emph{· · ·}\\
\emph{Ri}+3
\end{quote}\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+3
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{· · ·· · ·· · ·· · ·· · ·}
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+\emph{r}
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\\

\end{quote}\strut
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+4
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{Ri}+\emph{r}+1
\end{quote}
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{Ri}+5
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{Ri}+\emph{r}+2
\end{quote}
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{· · ·}\\
\emph{Ri}+\emph{r}+1\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{· · ·}\\
\emph{Ri}+\emph{r}+2\strut
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{· · ·}\\
\emph{Ri}+2\emph{r−}1\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
52 5. REALIZATION THEORY

Theorem 5.4.2 (B.L. Ho's algorithm). \emph{Set n} = rank \emph{Hr, and
let P and}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{Q be defined as in} (47)\emph{. Set}

\emph{n×n}= {[}\emph{In\textbar{}}0{]}\emph{Pσ\{Hr\}Q} {[} \emph{In}
\end{quote}

0 {]}

\emph{Ik}

\begin{quote}
0 \emph{R}1
\end{quote}

\emph{R}2

\begin{quote}
\emph{m×n}= {[}\emph{Im,} 0\emph{, · · · ,} 0{]}\emph{HrQ}\\
\emph{n×k}= {[}\emph{In\textbar{}}0{]}\emph{PHr}0 \emph{...} =
{[}\emph{In\textbar{}}0{]}\emph{P}
\end{quote}

{[} \emph{In}

\begin{quote}
0 {]} = {[}\emph{R}1\emph{, R}2\emph{, · · · , Rr}{]}\emph{Q}\\
\emph{Rr} \emph{...}
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[} \emph{In}

0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{]}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\emph{Then} (\emph{A, B, C}) \emph{is a minimal realization.}

For the proof we need a sequence of lemmas.

Lemma 5.4.3. \emph{Let AO and AR be the A-matrices of the observable
and}

\emph{reachable standard realizations respectively. Then}
\end{quote}

\emph{σi\{Hr\}} = \emph{Ai OHr}= \emph{Hr}(\emph{AT R})\emph{i}

\begin{quote}
\emph{for all i} = 1\emph{,} 2\emph{,} 3\emph{, ...}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0714}}@{}}
\toprule()
\multicolumn{14}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{1.0000} + 26\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Proof:} From Lemma 5.1.6 we see that \emph{AOHr} =
\emph{σ\{Hr\}}. Repeated multi-plications with \emph{AO} using Lemma
5.1.6 gives \emph{Ai}

involving \emph{AR} is obtained in the same manner. \emph{OHr} =
\emph{σi\{Hr\}}. The relation□
\end{minipage}} \\
\midrule()
\endhead
Lemma 5.4.4. \emph{Define} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.2143} + 4\tabcolsep}}{%
\multirow{3}{*}{\emph{H}\# \emph{r}= \emph{Q}}} & \multirow{3}{*}{{[}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1429} + 2\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{In}
\end{quote}
\end{minipage}}} & \multirow{3}{*}{0 {]}

0} &
\multicolumn{6}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.4286} + 10\tabcolsep}@{}}{%
\multirow{3}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{P}
\end{quote}
\end{minipage}}} \\
\multirow{2}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
(48)
\end{quote}
\end{minipage}} \\
& & & & &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1429} + 2\tabcolsep}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
0
\end{quote}
\end{minipage}} \\
\multicolumn{14}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{1.0000} + 26\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{Then H}\# \emph{ris a pseudo inverse of Hrin the sense that}
\end{quote}

\emph{HrH}\# \emph{rHr}= \emph{Hr.}
\end{minipage}} \\
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\textbf{Proof:} We have
\end{quote}
\end{minipage} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.2143} + 4\tabcolsep}}{%
\multirow{2}{*}{\emph{Hr} = \emph{P−}1}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1429} + 2\tabcolsep}}{%
\multirow{2}{*}{{[} \emph{In}

0}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1429} + 2\tabcolsep}}{%
\multirow{2}{*}{0 {]}

0}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1429} + 2\tabcolsep}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{Q−}1
\end{quote}
\end{minipage}} & \multirow{5}{*}{{[} \emph{In}

0} & \multirow{5}{*}{0 {]}

0} & \multirow{5}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{Q−}1
\end{quote}
\end{minipage}} & \multirow{5}{*}{□} \\
\multirow{2}{*}{\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
and therefore
\end{quote}
\end{minipage}} & & & & & & & & \multirow{4}{*}{0 {]}

0} & \multirow{4}{*}{\emph{PP−}1} \\
& \multirow{3}{*}{{[} \emph{In}

0

{[} \emph{In}

0} & \multirow{3}{*}{0 {]}

0

0 {]}

0} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.2143} + 4\tabcolsep}}{%
\multirow{2}{*}{\emph{Q−}1\emph{Q}}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1429} + 2\tabcolsep}}{%
\multirow{2}{*}{{[} \emph{In}

0}} \\
\emph{HrH}\# \emph{rHr}= \emph{P −}1 \\
= \emph{P−}1 & & &
\multicolumn{5}{>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.3571} + 8\tabcolsep}}{%
\emph{Q−}1= \emph{Hr}} \\
\multicolumn{14}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{1.0000} + 26\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Lemma 5.4.5. \emph{Let A be defined as in Theorem 5.4.2. Then} (49)
\emph{Ai}= {[}\emph{In\textbar{}}0{]}\emph{Pσi\{Hr\}Q} {[} \emph{In} 0
{]} \emph{for all i} = 1\emph{,} 2\emph{,} 3\emph{, ...}
\end{quote}
\end{minipage}} \\
\bottomrule()
\end{longtable}

\begin{quote}
5.4. HO'S ALGORITHM1 53
\end{quote}

\textbf{Proof:} Use induction. Relation (49) holds trivially for
\emph{i} = 1. Suppose it

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule()
\multicolumn{8}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{1.0000} + 14\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
holds for \emph{i} = \emph{j} and show that it holds for \emph{i} =
\emph{j} + 1.
\end{quote}
\end{minipage}} \\
\midrule()
\endhead
\emph{Aj}+1= \emph{AAj}= {[}\emph{In\textbar{}}0{]}\emph{Pσ\{Hr\}Q} &
{[} \emph{In}

0 & 0 {]}

0 &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.3750} + 4\tabcolsep}}{%
\emph{Pσj\{Hr\}Q}} & {[} \emph{In}

0 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
{]}
\end{quote}
\end{minipage} \\
\multicolumn{8}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{1.0000} + 14\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Then replacing \emph{σ\{Hr\}} by \emph{AOHr} and \emph{σj\{Hr\}} by
\emph{Hr}(\emph{AT} observing (48) we obtain \emph{R})\emph{j} Lemma
5.4.3 and
\end{quote}
\end{minipage}} \\
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.5000} + 6\tabcolsep}}{%
\emph{Aj}+1= {[}\emph{In\textbar{}}0{]}\emph{PAOHrH}\#
\emph{rHr}(\emph{AT R})\emph{jQ}} & {[} \emph{In}

0 &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.3750} + 4\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
{]}
\end{quote}
\end{minipage}} \\
\bottomrule()
\end{longtable}

\begin{quote}
In view of Lemma 5.4.4 and Lemma 5.4.3,
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{1.0000} + 6\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{AOHrH}\# \emph{rHr}(\emph{AT R})\emph{j} = \emph{AOHr}(\emph{AT
R})\emph{j} = \emph{Hr}(\emph{A′R})\emph{j}+1 = \emph{σj}+1\emph{\{Hr\}}

\begin{quote}
and consequently.
\end{quote}
\end{minipage}} \\
\midrule()
\endhead
\emph{Aj}+1= {[}\emph{In\textbar{}}0{]}\emph{Pσj}+1\emph{\{Hr\}Q} & {[}
\emph{In}

0 & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
{]}
\end{quote}
\end{minipage} & □ \\
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{1.0000} + 6\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
Proof of Theorem 5.4.2 Since dim(\emph{A, B, C}) = \emph{δ}(\emph{R}) it
only remains to show

that \emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B}, i.e.
\emph{Ri} = \emph{CAi−}1\emph{B} (\emph{A, B, C}) defined as in the
theorem and \emph{Ai−}1as given in Lemma 5.4.5, we for \emph{i} =
1\emph{,} 2\emph{,} 3\emph{, ...} . With

\begin{quote}
have

\emph{CAi−}1\emph{B} = {[}\emph{Im,} 0\emph{, · · · ,} 0{]}\emph{HrQ}
{[} \emph{In}

0\\
0
\end{quote}

0 {]} \emph{Pσi−}1\emph{\{Hr\}Q} {[} \emph{In}

\begin{quote}
0\\
0
\end{quote}

0 {]} \emph{PHr}\emph{Ik}

0

\begin{quote}
0 ...
\end{quote}

Then, replacing \emph{σi−}1\emph{\{Hr\}} by \emph{Ai−}1 \emph{OHr}
(Lemma 5.4.3), observing the defini-

\begin{quote}
tion (48) of \emph{H}\# \emph{r}, and applying Lemma 5.4.4 we have

\emph{CAi−}1\emph{B} = {[}\emph{Im,} 0\emph{, · · · ,} 0{]}\emph{HrH}\#

= {[}\emph{Im,} 0\emph{, · · · ,} 0{]}\emph{σi−}1\emph{\{Hr\}}\\
\emph{rAi−}1 \emph{OHr}

\emph{Ik}
\end{quote}

0

\begin{quote}
0 ... \\
 = \emph{Ri}\\
  \emph{Ik}
\end{quote}

0

\begin{quote}
0 ...

because, in view of Lemma 5.4.3 and Lemma 5.4.4,
\end{quote}

\emph{HrH}\# \emph{rAi−}1 \emph{OHr} = \emph{HrH}\#\emph{Hr}(\emph{AT
R})\emph{i−}1 = \emph{Hr}(\emph{AT R})\emph{i−}1 =
\emph{σi−}1\emph{\{Hr\}.}

□\strut
\end{minipage}} \\
\bottomrule()
\end{longtable}

\begin{quote}
54 5. REALIZATION THEORY

Example 5.4.6. Apply B.L. Ho's algorithm to \emph{R}(\emph{s}) as
defined in Ex-
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.7500} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
ample 5.1.7. Note that \emph{P} and \emph{Q} are determined in Remark
5.4.1 as

Then, the algorithm of Theorem 5.4.2 yields a minimal realization with
\emph{P} = \\
1\emph{/}2 1 0

1 0 1 0 0\\
0\\
0\\
0 1 1\emph{/}2 0 0

0   \emph{Q} =  1 0 0 0 \emph{−}2 1 0 0 \emph{−}1 1 1 0\\
\emph{−}1

\emph{−}1\\
1

1
\end{quote}\strut
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}3 20
\end{quote}
\end{minipage}} \\
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
(\emph{A, B, C}) given by

\emph{A} =1\emph{/}2 1 0 0 1 0 0 0 0 1\emph{/}2 0 0    \emph{−}1
\emph{−}1 1 3 \emph{−}2 \emph{−}2 2 4 \emph{−}1 \emph{−}7 1 3 \emph{−}2
\emph{−}8 2 4

1    2 1 0 0 0 \emph{−}2 1 0 0 \emph{−}1 1 1 0   =

i.e. we have the following minimal realization \emph{C} =\\
\emph{B} =

{[}\\
1\\
01\emph{/}2

2\\
1\\
1\\
0

\emph{−}1\\
\emph{−}1\\
0 1 0 0\\
0\\
0

\emph{−}2\emph{−}2 {]}\\
1\emph{/}2\\
0\\
0

1 0 0 0\emph{−}2\emph{−}1\emph{−}1

1\\
0\\
0\\
0

\emph{−}1\\
\emph{−}2\\
\emph{−}2

1\\
1\\
0\\
1

 = = {[}\\


1\\
0\\
1\\
0\\
0

0\\
1\\
2\\
1 0

0\\
0 {]}

˙\emph{x}1 = \emph{−x}1 + \emph{u}1 + 2\emph{u}2˙\emph{x}2 =
\emph{−x}1 + \emph{u}2˙\emph{x}3 = \emph{x}1 \emph{− x}2 \emph{−}
3\emph{x}3\\
\emph{y}1 = \emph{x}1 \emph{y}2 = \emph{x}2
\end{quote}\strut
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{−}1

\emph{−}1 1
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\emph{−}1
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\includegraphics[width=1.625in,height=0.36111in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image14.png}\includegraphics[width=2.77778in,height=0.52778in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image15.png}\includegraphics[width=2.33333in,height=0.63889in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image16.png}

CHAPTER 6

\textbf{State Feedback and Observers}

\begin{quote}
Suppose that the time-invariant system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
(Σ)
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
Σ
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
y
\end{quote}
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
\{˙\emph{x} = \emph{Ax} + \emph{Bu}

\begin{quote}
\emph{y} = \emph{Cx}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
is unstable. Is it possible to stabilize the system by linear feedback?
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
v
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
π
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Σ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
y
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
More generally, given any linear system Σ, is it possible to move the
poles of the transfer function to any preassigned location? We shall
allow the feedback function \emph{π} to be either multiplication by a
constant matrix, or, more generally, itself a linear system of type Σ.
\end{quote}

\textbf{6.1. Feedback with Complete State Information}

\begin{quote}
First let us consider the case when the state \emph{x} is available for
observation, i.e., \emph{y} = \emph{x}. Then, it is possible to choose a
feedback of the type
\end{quote}

\emph{u}(\emph{t}) = \emph{Kx}(\emph{t}) + \emph{v}(\emph{t})

\begin{quote}
where \emph{K} is a constant matrix and \emph{v} may be a function of
time \emph{t}.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
v
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Σ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
x
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

K

\begin{quote}
The closed-loop system can be written
\end{quote}

˙\emph{x} = (\emph{A} + \emph{BK})\emph{x} + \emph{Bv,}

\begin{quote}
Hence (\emph{A, B}) is changed to ( ˆ\emph{A, B}) where ˆ\emph{A} =
\emph{A} + \emph{BK}.

\textbf{6.1.1. Invariance of reachability.} The reachability properties
of the system are not changed by such feedback, i.e., the reachable
subspace is in-variant under state feedback. Denoted by R and R\emph{K}
the reachable subspaces
\end{quote}

55

\begin{quote}
56 6. STATE FEEDBACK AND OBSERVERS

of the open-loop and closed-loop systems respectively, i.e.,

R = \emph{⟨A\textbar{}} Im \emph{B⟩} ≜ Im {[}\emph{B, AB, A}2\emph{B, ·
· · , An−}1\emph{B}{]}\\
= Im \emph{B} + \emph{A} Im \emph{B} + \emph{· · ·} + \emph{An−}1Im
\emph{B}\\
R\emph{K} = \emph{⟨A} + \emph{BK\textbar{}} Im \emph{B⟩}
\end{quote}

Remark 6.1.1. If \emph{U} and \emph{V} are subspaces then \emph{U} +
\emph{V} is defined as

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{U} + \emph{V} = \emph{\{u} + \emph{v \textbar{} u ∈ U, v ∈ V \}.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Lemma 6.1.2. \emph{For every K we have}
\end{quote}

R\emph{K} = R\emph{.}

\begin{quote}
\textbf{Proof:} Set ˆ\emph{A} = \emph{A} + \emph{BK}. Then, since
\emph{A}R \emph{⊂} R and Im \emph{B ⊂} R,\\
ˆ\emph{A}R = \emph{A}R + \emph{BK}R \emph{⊂} R\emph{,}\\
i.e., ˆ\emph{A}R \emph{⊂} R and therefore\\
R\emph{K} = Im \emph{B} + Im ˆ\emph{AB} + Im ˆ\emph{A}2\emph{B} +
\emph{· · ·} + Im ˆ\emph{An−}1\emph{B ⊂} R\\
Similarly, noting that \emph{A} = ˆ\emph{A − BK}, we show that R
\emph{⊂} R\emph{K}. Hence R = R\emph{K} as claimed. □

\textbf{6.1.2. Pole-placement.} Let \emph{XA}(\emph{s}) be the
characteristic polynomial
\end{quote}

\emph{XA}(\emph{s}) = det(\emph{sI − A})

\begin{quote}
of the system matrix \emph{A}. By feedback the system matrix is changed
to
\end{quote}

ˆ\emph{A} = \emph{A} + \emph{BK}. The question is now if \emph{K} can be
chosen so that ˆ\emph{A} is a stable

\begin{quote}
matrix, i.e. so that the zeros of the characteristic polynomial
\end{quote}

\emph{XA}+\emph{BK}(\emph{s}) = det(\emph{sI − A − BK})

are all in the open left half plane. The answer is that the zeros of
\emph{XA}(\emph{s}), i.e., the poles of

\begin{quote}
\emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B} +
\emph{D,}\\
can be moved to an arbitrary location in the complex plane, except that
\end{quote}

complex poles have to appear in pairs (i.e. \emph{a ± ib}), if and only
if the system is completely reachable.

\begin{quote}
Theorem 6.1.3. \emph{(Pole-placement Theorem) For any real polynomial}
\end{quote}

\emph{ϕ}(\emph{s}) = \emph{sn}+ \emph{γ}1\emph{sn−}1+ \emph{· · ·} +
\emph{γn}

\begin{quote}
\emph{there is a matrix K such that}
\end{quote}

\emph{XA}+\emph{BK} = \emph{ϕ}

\begin{quote}
\emph{if and only if} (\emph{A, B}) \emph{is completely reachable.}

We first consider the case when \emph{u} is scalar.

6.1. FEEDBACK WITH COMPLETE STATE INFORMATION 57

Lemma 6.1.4. \emph{Let b ∈} R\emph{nand let} (\emph{A, b}) \emph{be
completely reachable, i.e.⟨A\textbar{}} Im \emph{b⟩} = R\emph{n. Then
there is a nonsingular n × n matrix T such that}

\emph{TAT−}1= \emph{F} ≜\emph{. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .} 0 0 1 0 0 1 \emph{. . .} \emph{. . .} 0 0 
\emph{Tb} = \emph{h} ≜ 0 0 \emph{...}  0 1\emph{−an}
0 \emph{−an−}1 0 \emph{−an−}2 0 \emph{. . .}\\
\emph{. . .} \emph{−a}1 1 \emph{where a}1\emph{, a}2\emph{, · · · ,
an are the coefficients of}\\
\emph{XA}(\emph{s}) = \emph{sn}+ \emph{a}1\emph{sn−}1+ \emph{· · ·} +
\emph{an.}

\emph{The transformation T is unique and is given by}

(50) \emph{T} = \emph{cAn−}1 \emph{cA} \emph{...}  \\
\emph{c}

\emph{where c is the unique row vector solution to}

(51) \emph{c}{[}\emph{b, Ab, · · · , An−}1\emph{b}{]} = (0\emph{,}
0\emph{, · · · ,} 0\emph{,} 1)\emph{.}

\textbf{Proof:} Since (\emph{A, b}) is completely reachable, there is a
unique solution to (51). The system (51) can be written

\emph{cAn−}1\\
\emph{cA}\\
\emph{c}

...   \emph{b} =  \\
0

0 1\\
...

\\
0

or equivalently, \emph{Tb} = \emph{h}. To see that \emph{T} is
nonsingular, we show that the rows are linearly independent. To this
end, let \emph{α}1\emph{, α}2\emph{, . . . αn} be real numbers such that

\emph{α}1\emph{c} + \emph{α}2\emph{cA} + \emph{· · ·} + \emph{αncAn−}1=
0\emph{,}\\
multiply from the right by \emph{b} and use (51). This yields \emph{αn}
= 0. Then multiply by \emph{Ab} which yields \emph{αn−}1 = 0 and then by
\emph{A}2\emph{b} which yields \emph{αn−}2 = 0 etc. Since therefore
\emph{α}1 = \emph{α}2 = \emph{...} = \emph{αn} = 0, the rows of \emph{T}
are linearly independent. Set \emph{T−}1= {[}\emph{s}1\emph{, s}2\emph{,
· · · , sn}{]} where \emph{si} are the columns. Then

\emph{TT−}1= \emph{cAn−}1\emph{s}1 \emph{cAs}1 \emph{cs}1
\end{quote}

...

\begin{quote}
\emph{cAn−}1\emph{s}2\\
\emph{cAs}2\\
\emph{cs}2
\end{quote}

...

\begin{quote}
\emph{. . .}\\
\emph{. . .}

\emph{. . .} ... \emph{cAn−}1\emph{sn} \emph{cAsn} \emph{csn}  

58 6. STATE FEEDBACK AND OBSERVERS

and from this it follows that

\emph{TAT−}1= \emph{cAn−}1 \emph{cA}

0\\
\emph{c}

...   \emph{A}(\emph{s}1\emph{, s}2\emph{, · · · , sn}) = 1 0
\emph{. . .} 0  \emph{cAns}1\\
\emph{cA}2\emph{s}1 \emph{cAs}1
\end{quote}

...

\begin{quote}
\emph{cAns}2\\
\emph{cA}2\emph{s}2\\
\emph{cAs}2
\end{quote}

...

\emph{. . .}

\emph{. . .}

\begin{quote}
\emph{. . .}\\
...

\emph{cAnsn}\\
\emph{cA}2\emph{sn cAsn}

= \emph{. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .} 0
0 1 \emph{. . .} 0 
\end{quote}

\emph{.}

\begin{quote}
But, by the Cayley-Hamilton theorem we have\emph{cAns}1 0

\emph{cAns}2\\
0 0

\emph{. . . . . .}\\
\emph{. . .}

\emph{cAnsn} 1 
\end{quote}

\emph{n}

\emph{cAnsi} = \emph{−}∑\emph{ajcAn−jsi} = \emph{−an}+1\emph{−i,}

\begin{quote}
because \emph{cAn−jsi} = 1 if \emph{n − j} = \emph{i −} 1 and 0
otherwise. This completes the proof of the lemma. □

Suppose now that (\emph{A, b}) is completely reachable and that the
system
\end{quote}

˙\emph{x} = \emph{Ax} + \emph{bu}

\begin{quote}
with a scalar input is to be stabilized by the feedback
\end{quote}

\emph{u} = \emph{kx} + \emph{v}

\begin{quote}
where \emph{K} is a constant row vector. Then, the feedback matrix
becomes
\end{quote}

\emph{A} + \emph{bk} = \emph{T−}1\emph{FT} + \emph{T−}1\emph{hk} =
\emph{T−}1{[}\emph{F} + \emph{hg}{]}\emph{T}

\begin{quote}
where
\end{quote}

\emph{g} = (\emph{gn, gn−}1\emph{, · · · , g}1) ≜ \emph{kT−}1

or \emph{k} = \emph{gT}. Note the reversed numbering of the elements in
the row vector

\emph{g}. Then, since similar matrices have the same characteristic
polynomial, we

\begin{quote}
have
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
However,
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{XA}+\emph{bk} = \emph{XF}+\emph{hg}
\end{minipage} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
+
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{. . . . . . . . . . . . . . . . . . . . . . . .} 0 0 0 \emph{. .
.} 0 0 0 0 \emph{. . .} 0

\emph{gn} 0

\emph{gn−}1\\
0

\emph{gn−}2\\
0 \emph{. . .}

\emph{. . .} \emph{g}1\\
0
\end{quote}\strut
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\\

\end{quote}\strut
\end{minipage}} \\
& \multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .} 0
0 1 \emph{. . .} 0  0 1 0 \emph{. . .} 0

\emph{−an}

0\\
0

\emph{−an−}1

1\\
0
\end{quote}

\emph{−an−}2

\begin{quote}
0\\
0

\emph{. . .}\\
\emph{. . .}

\emph{. . .} \emph{−a}1

0 1 \emph{. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .} 0 0 1 \emph{. . .} 0 \emph{−γn} 0

\emph{−γn−}1\\
0

\emph{−γn−}2\\
0 \emph{. . .}

\emph{. . .} \emph{−γ}1 1 
\end{quote}\strut
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\emph{F} + \emph{hg} =
\end{minipage} \\
\begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
6.1. FEEDBACK WITH COMPLETE STATE INFORMATION
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
59
\end{minipage} \\
\midrule()
\endhead
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{1.0000} + 4\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
that is \emph{XF}+\emph{hg} = \emph{ϕ} if
\end{quote}
\end{minipage}} \\
\emph{gi} = \emph{ai − γi,} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{i} = 1\emph{,} 2\emph{, · · · , n.}
\end{quote}
\end{minipage}} \\
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{1.0000} + 4\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
Example 6.1.5. Consider the system
\end{quote}

\{˙\emph{x}1 = \emph{x}1 \emph{−} 3\emph{x}2 + \emph{u}

˙\emph{x}2 = 4\emph{x}1 + 2\emph{x}2 + \emph{u}

\begin{quote}
Then

i.e., the poles are located at \emph{s} =\uline{3}\\
\emph{A} = {[}\\
1

4 \emph{−}3 2 {]} \emph{,} \emph{b} =

2\emph{± i}\\
{[}\\
1
\end{quote}

1 {]} \emph{,}

\emph{√}

2. Is it possible to place the poles

\begin{quote}
47\\
\emph{XA}(\emph{s}) = \emph{s}2\emph{−} 3\emph{s} + 14\emph{,}

at \emph{s} = \emph{−}1 and \emph{s} = \emph{−}2 instead? Yes, because

{[}\emph{b, Ab}{]} = {[}\\
1

1 \emph{−}2 6 {]}

has full rank. The required closed-loop characteristic polynomial
\end{quote}\strut
\end{minipage}} \\
\bottomrule()
\end{longtable}

\emph{ϕ}(\emph{s}) = (\emph{s} + 1)(\emph{s} + 2) = \emph{s}2+ 3\emph{s}
+ 2\emph{,}

\begin{quote}
i.e., \emph{γ}1 = 3\emph{, γ}2 = 2, and therefore we should choose
\emph{g} as
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}@{}}
\toprule()
\multicolumn{11}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.9167} + 20\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{g}1 = \emph{a}1 \emph{− γ}1 = \emph{−}3 \emph{−} 3 = \emph{−}6\\
\emph{g}2 = \emph{a}2 \emph{− γ}2 = 14 \emph{−} 2 = 12

Next, we determine the transformation \emph{T}:

(\emph{c}1\emph{, c}2) {[}\\
1

1 \emph{−}2 6 {]} = (0\emph{,} 1) \emph{⇒}\{ \emph{c}1 + \emph{c}2

\emph{−}2\emph{c}1 + 6\emph{c}2 = 1\emph{⇒ t} = (\emph{−}1 8\emph{,} 1
8)
\end{quote}\strut
\end{minipage}} &
\multirow{7}{*}{\begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage}} \\
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
i.e.

\emph{T} = {[} \emph{cA} {]}\\
\emph{c}

As a check, we compute
\end{quote}\strut
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
= 1 8
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
{[}\emph{−}1 3
\end{minipage} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
5 {]}

1
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{,}
\end{minipage}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{T−}1=
\end{minipage}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
{[}\emph{−}5 3
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
1 {]}

1
\end{quote}
\end{minipage}} \\
& & \multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[}\emph{−}14 0
\end{quote}
\end{minipage}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1667} + 2\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
3 {]}

1
\end{minipage}}} & &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1667} + 2\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{Tb} =
\end{minipage}}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[} 1 {]} 0
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{.}
\end{minipage}} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{TAT−}1=
\end{minipage}} \\
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.2500} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
The required gain is
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[}\emph{−}1 3
\end{minipage}} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.2500} + 4\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
5 {]}

1
\end{minipage}}} &
\multicolumn{4}{>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.3333} + 6\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
= (\emph{−}15 4\emph{, −}9 4)\emph{.}
\end{quote}
\end{minipage}}} \\
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.2500} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{k} = \emph{gT} = (12\emph{, −}6)1
\end{minipage}} \\
\multicolumn{11}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.9167} + 20\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Hence, the feedback\\
\emph{u} = \emph{−}15 4\emph{x}1 \emph{−} 9 4\emph{x}2 + \emph{v}
achieves the desired pole placement.
\end{quote}\strut
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
It remains to prove Theorem 6.1.3 (\emph{Pole-placement Theorem}) for
the case that there are more than one input, as well as the necessity of
the reachability condition. For this we need another lemma.

60 6. STATE FEEDBACK AND OBSERVERS
\end{quote}

\emph{pletely reachable, there is a matrix K such that} (\emph{A} +
\emph{BK, b}) \emph{is completely} Lemma 6.1.6 (Heymann). \emph{Let b ∈}
Im \emph{B and b ̸}= 0\emph{. If} (\emph{A, B}) \emph{is com-}

\begin{quote}
\emph{reachable.}

\textbf{Proof:} We have to show that there is a matrix \emph{K} such
that

(52) rank {[}\emph{b,} (\emph{A} + \emph{BK})\emph{b,} (\emph{A} +
\emph{BK})2\emph{b, ...,} (\emph{A} + \emph{BK})\emph{n−}1\emph{b}{]} =
\emph{n.}

To this end, consider the discrete-time system

(53) \emph{x}(\emph{t} + 1) = \emph{Ax}(\emph{t}) +
\emph{Bu}(\emph{t})\emph{, x}(1) = \emph{b.}

By Lemma 3.3.2 there is an input sequence \emph{\{}˜\emph{u}(1)\emph{,}
˜\emph{u}(2)\emph{, ...,} ˜\emph{u}(\emph{n −} 1)\emph{\}} such that the
corresponding trajectory \emph{\{x}(1)\emph{, x}(2)\emph{, ...,
x}(\emph{n})\emph{\}} of the system spans the state space. The idea is
now to express the input ˜\emph{u}(\emph{t}) as state-feedback

\emph{Kx}(\emph{t}).

Introduce the matrices \emph{Z} ≜ {[}\emph{x}(1)\emph{, x}(2)\emph{,
..., x}(\emph{n}){]}, which is invertible since
\end{quote}

\emph{\{x}(1)\emph{, x}(2)\emph{, ..., x}(\emph{n})\emph{\}} is a basis,
and \emph{U} ≜ {[}˜\emph{u}(1)\emph{,} ˜\emph{u}(2)\emph{, ...,}
˜\emph{u}(\emph{n}){]}, where ˜\emph{u}(\emph{n}) is arbitrary . Then we
have

\begin{quote}
(54) \emph{U} = \emph{UZ−}1\emph{Z} = \emph{KZ,}
\end{quote}

where \emph{K} is defined as \emph{K} ≜ \emph{UZ−}1. From (54) we then
see that ˜\emph{u}(\emph{t}) = \emph{Kx}(\emph{t}),

\begin{quote}
and the solution of (53) with input \emph{\{}˜\emph{u}(1)\emph{,}
˜\emph{u}(2)\emph{, ...,} ˜\emph{u}(\emph{n})\emph{\}} can be obtained
by iterating \emph{x}(\emph{t} + 1) = (\emph{A} +
\emph{BK})\emph{x}(\emph{t})\emph{, x}(1) = \emph{b}. This implies that
\emph{x}(\emph{t}) =

(\emph{A} + \emph{BK})\emph{t−}1\emph{b} and (52) is established. □

Proof of Theorem 6.1.3
\end{quote}

\emph{If}: The scalar-input case has already been proved above.
Therefore, we shall

reduce the general case that \emph{B} have several columns to the
scalar-input case.

Let \emph{b ∈} Im \emph{B} and \emph{b ̸}= 0. Then there is a vector
\emph{u}0 \emph{∈} R\emph{k}such that \emph{b} = \emph{Bu}0. Since
(\emph{A, B}) is completely reachable, there is, according to Lemma
6.1.6, a

\emph{k × n} matrix \emph{K} such that (\emph{A} + \emph{B}ˆ\emph{K, b})
is completely reachable. Then, in view of the scalar-input result, there
is a row vector \emph{k} such that

\emph{XA}+\emph{B} ˆ \emph{K}+\emph{bk}(\emph{s}) = \emph{ϕ}(\emph{s})

But \emph{B}ˆ\emph{K} + \emph{bk} = \emph{B}( ˆ\emph{K} +
\emph{u}0\emph{k}), and consequently the desired pole placement

\begin{quote}
is achieved through a feedback with the gain \emph{K} = ˆ\emph{K} +
\emph{u}0\emph{k}.
\end{quote}

\emph{Only If}: Let \emph{λ}1\emph{, λ}2\emph{, . . . , λn} be different
real numbers such that \emph{λi \textgreater{} ∥A∥} for \emph{i} =
1\emph{,} 2\emph{, . . . , n}, and suppose that \emph{K} is such that

\emph{XA}+\emph{BK}(\emph{s}) = (\emph{s − λ}1)(\emph{s − λ}2) \emph{· ·
·} (\emph{s − λn})\emph{.}

Then, the matrix \emph{A}+\emph{BK} has eigenvalues \emph{λ}1\emph{,
λ}2\emph{, . . . , λn}. Let \emph{\{z}1\emph{, z}2 \emph{. . . , zn\}}
be the corresponding eigenvectors, i.e.

(\emph{A} + \emph{BK})\emph{zi} = \emph{λizi}

The eigenvectors form a basis in \emph{X} = R\emph{n}. However, for each
\emph{i} = 1\emph{,} 2\emph{, . . . , n},

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{zi} = (\emph{λiI − A})\emph{−}1\emph{BKzi} = \emph{λ−}1
\emph{i}(\emph{I − Aλ−}1 \emph{i})\emph{−}1\emph{BKzi} =
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∑
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{λ−j−}1 \emph{i}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{AjBKzi}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
6.2. OBSERVERS 61

where we have used the fact that \emph{∥Aλ−}1 \emph{i∥ \textless{}} 1
when expanding in series. Therefore, in view of Remark 3.2.2, \emph{zi
∈} R for \emph{i} = 1\emph{,} 2\emph{, . . . , n}, where R is the
reachable subspace corresponding to (\emph{A, B}), and consequently
\end{quote}

\emph{X} = span \emph{\{z}1\emph{, z}2\emph{, . . . , zn\} ⊂} R \emph{⊂}
R\emph{n.}

Therefore, since \emph{X} = R\emph{n}, we must have R = R\emph{n}, i.e.
(\emph{A, B}) is completely

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
reachable.
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\textbf{6.2. Observers}

\begin{quote}
Consider a system where only the output is measurable, i.e.

(Σ) \{˙\emph{x} = \emph{Ax} + \emph{Bu}
\end{quote}

\emph{y} = \emph{Cx}

\begin{quote}
where rank \emph{C \textless{} n}. Is it possible to change the poles of

\emph{R}(\emph{s}) = \emph{C}(\emph{sI − A})\emph{−}1\emph{B}\\
by output feedback \emph{u} = \emph{Ky} + \emph{v}? The answer is in
general no.
\end{quote}

\includegraphics[width=2.77778in,height=0.52778in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image17.png}

\begin{quote}
However, if dynamical output feedback is used, namely we feed the
out-put as input to an artificially constructed dynamical system and use
the state of that system for control design, then pole-placement for the
original system is possible when it is viewed as part of this enlarged
system. The key idea is that this artificially constructed system will
be an estimator of the true state \emph{x}(\emph{t}):

\emph{d}ˆ\emph{x}\\
\emph{dt}= \emph{A}ˆ\emph{x} + \emph{Bu} + \emph{L}(\emph{y −
C}ˆ\emph{x})

where \emph{L} is a constant matrix to be chosen so that

\emph{∥x}(\emph{t}) \emph{−} ˆ\emph{x}(\emph{t})\emph{∥ →} 0 as \emph{t
→ ∞}\\
We want ˆ\emph{x} to adjust asymptotically to the state \emph{x}. Set
˜\emph{x} ≜ \emph{x −} ˆ\emph{x}. The estimation error ˜\emph{x}
satisfies the homogeneous system of differential equations

\emph{d}˜\emph{x}\\
\emph{dt}= (\emph{A − LC})˜\emph{x}

Hence ˜\emph{x}(\emph{t}) \emph{→} 0 when \emph{t → ∞} if and only if
(\emph{A − LC}) is a stable matrix, i.e. \emph{XA−LC} has all its zeros
in the (open) left half plane.
\end{quote}

\includegraphics[width=2.34722in,height=0.68056in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image18.png}

\begin{quote}
62 6. STATE FEEDBACK AND OBSERVERS

Theorem 6.2.1. \emph{For any real polynomial}
\end{quote}

\emph{ϕ}(\emph{s}) = \emph{sn}+ \emph{γ}1\emph{sn−}1+ \emph{· · ·} +
\emph{γn}

\begin{quote}
\emph{there is a matrix L such that}
\end{quote}

\emph{XA−LC} = \emph{ϕ}

\begin{quote}
\emph{if and only if} (\emph{C, A}) \emph{is completely observable.}
\end{quote}

\textbf{Proof:} To say that (\emph{C, A}) is completely observable is
equivalent to say

that (\emph{A′, C′}) is completely reachable which, by Theorem 6.1.3
establishes

the existence of a \emph{K} such that \emph{XA′}+\emph{C′K} = \emph{ϕ}
for any \emph{ϕ}, i.e. \emph{XA}+\emph{K′C} = \emph{ϕ}.

\begin{quote}
The theorem follows if we set \emph{L} = \emph{−K′}. □

In particular, we can choose \emph{L} so that \emph{A − LC} is a stable
matrix, i.e.

˜\emph{x}(\emph{t}) \emph{→} 0. Using the feedback law \emph{u} =
\emph{K}ˆ\emph{x} + \emph{v} yields the over-all system

{[} ˙\emph{x} {]} = {[} \emph{LC} \emph{A − LC} + \emph{BK} {]} {[}
ˆ\emph{x} {]} + \emph{Bv} {[} \emph{I} {]} \emph{.}

\emph{A} \emph{BK} \emph{x} \emph{I}

Let us do a linear coordinate change by letting ˜\emph{x} = \emph{x −}
ˆ\emph{x}. Then

{[} ˙\emph{x} {]} = {[}\\
\emph{A} + \emph{BK}

0 \emph{A − LC} \emph{−BK} {]} {[} \emph{x}

˜\emph{x} {]} + \emph{Bv} {[}\\
\emph{I}
\end{quote}

0 {]} \emph{,}

\begin{quote}
which implies
\end{quote}

\emph{XF} = \emph{XA}+\emph{BKXA−LC}

\begin{quote}
since for every matrix \emph{M} which can be partitioned \emph{M} =
{[}\emph{M}11 \emph{M}12 0 \emph{M}22{]} where
\end{quote}

\emph{M}11 and \emph{M}22 are quadratic, it holds that det \emph{M} =
det \emph{M}11 det \emph{M}22. The

over-all system is then stable if and only if both \emph{A} + \emph{BK}
and \emph{A − LC} are stable matrices.

\begin{quote}
The answer to the original question is then: If Σ is a minimal
realization

then the system is stabilizable and it can be stabilized as follows
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
v
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
K
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
x
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Σ
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Σ
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
y
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
The feedback law, defined by \emph{K}, and the observer, defined by
\emph{L}, can
\end{quote}

be determined independently. The dimension of the observer can be
further

reduced to \emph{n −} rank \emph{C} and yet be made stable, but we shall
not pursue this matter here.

\begin{quote}
Remark 6.2.2. This result that the poles of the estimator and the sys-
\end{quote}

tem can be assigned independently is usually called the \emph{separation
princi-}

\emph{ple}. A more general separation theorem is valid in the context of
Optimal

\begin{quote}
(Kalman) filtering and Optimal control. □

6.2. OBSERVERS 63

We note that the overall system is, however, never minimal. This can
\end{quote}

easily be verified by calculating the transfer function from \emph{v} to
\emph{y}. In fact,

\begin{quote}
the overall system is never reachable.

Example 6.2.3. Consider Example 6.1.5 with output \emph{y} = 2\emph{x}1
+ 3\emph{x}2. Is
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
it possible to stabilize the system with an observer? Yes, because

{[} \emph{CA} {]} = {[} 14 0 {]}\\
\emph{C} 2 3
\end{quote}

is full rank, and we can consequently assign the poles arbitrarily (in
pair if

\begin{quote}
they are complex). Construct an observer with poles \emph{s} = \emph{−}3
\emph{± i}, i.e.,

\emph{ϕ}(\emph{s}) = (\emph{s} + 3 + \emph{i})(\emph{s} + 3 \emph{− i})
= \emph{s}2+ 6\emph{s} + 10\emph{,} \emph{γ}1 = 6\emph{,} \emph{γ}2 =
10\emph{.}
\end{quote}

Then, determine a control law \emph{k} = \emph{−LT}as above for a system
for which \emph{AT} and \emph{CT}play roles of \emph{A} and \emph{b}
respectively, i.e.,

\begin{quote}
\emph{A ← AT}= {[}\emph{−}3 2 {]} and \emph{b ← CT}= {[} 3 {]}\\
1 4 2

yielding a characteristic polynomial
\end{quote}\strut
\end{minipage}} \\
\midrule()
\endhead
\begin{minipage}[t]{\linewidth}\raggedright
\emph{XA}(\emph{s}) = \emph{s}2\emph{−} 3\emph{s} + 14

\begin{quote}
which is the same as before. Hence,
\end{quote}

\{ \emph{g}1 = \emph{a}1 \emph{− γ}1 = \emph{−}3 \emph{−} 6 = \emph{−}9

\emph{g}2 = \emph{a}2 \emph{− γ}2 = 14 \emph{−} 10 = 4

\begin{quote}
(\emph{c}1\emph{, c}2) {[}\\
2
\end{quote}

3

14

0 {]} = (0\emph{,} 1) \emph{⇒}\{ 2\emph{c}1 + 3\emph{c}2 = 0

\begin{quote}
14\emph{c}1 = 1 \emph{⇒ c} = ( 1 14\emph{, −} 1 21)
\end{quote}

\emph{T} = 1

\begin{quote}
42 {[}\\
3

9 \emph{−}2 8 {]}

\emph{k} = \emph{gT} = (4\emph{, −}9) 1 42 {[}\\
3

9 \emph{−}2 8 {]} = (\emph{−}69 42\emph{, −}40 21)
\end{quote}

\emph{L} = \emph{−kT}= {[} \uline{69} {]}

\begin{quote}
The dynamical feedback is then



\\
\emph{u} = \emph{−}15

\emph{d}ˆ\emph{x}
\end{quote}

\emph{dt}= {[} 1

\begin{quote}
4\\
4ˆ\emph{x}1 \emph{−} 9
\end{quote}

\emph{−}3

\begin{quote}
2 {]}\\
4ˆ\emph{x}2 + \emph{v}
\end{quote}

ˆ\emph{x} + {[} 1

\begin{quote}
1 {]} \emph{u} + {[}\\
69
\end{quote}

42

40

21 {]} (\emph{y −} 2ˆ\emph{x}1 \emph{−} 3ˆ\emph{x}2)\emph{.}\strut
\end{minipage} & □ \\
\bottomrule()
\end{longtable}

CHAPTER 7

\textbf{Linear-Quadratic Optimal Control}

\begin{quote}
Whenever possible we want to control a system in an optimal way. To do
this there has to be a criterion defining what is best. Sometimes it is
important to minimize the transfer time between two states, sometimes to
minimize the control effort (say the energy used), the deviation from
some nominal trajectory, or a combination of these. The literature on
optimal control is very extensive and this chapter only treats the most
fundamental techniques.
\end{quote}

\textbf{7.1. Linear-Quadratic regulator}

\begin{quote}
This method is widely used and implemented in many working control
systems such as the space shuttle and missiles. It is often relatively
easy to implement and the corresponding theoretical principles are well
understood.

Given a system

(55) \{˙\emph{x}(\emph{t}) = \emph{Ax}(\emph{t}) + \emph{Bu}(\emph{t})
\end{quote}

\emph{x}(\emph{t}0) = \emph{x}0

\begin{quote}
on the interval {[}\emph{t}0\emph{, t}1{]}, the goal is to determine the
control \emph{u} which minimizes the functional

(56) \emph{J}(\emph{u}) =∫ \emph{t}1 {[}
\emph{x}(\emph{t})\emph{TQx}(\emph{t}) +
\emph{u}(\emph{t})\emph{TRu}(\emph{t}) {]} \emph{dt} +
\emph{x}(\emph{t}1)\emph{TSx}(\emph{t}1)\emph{,}

where \emph{Q ≥} 0\emph{, R \textgreater{}} 0\emph{, S ≥} 0 are
symmetric. Preferably, we would like to determine the optimal \emph{u}
as a \emph{control law}, i.e., a feedback function through which the
control signal depends not only on \emph{t} but also on the state
\emph{x}. This is of great importance in stabilizing a system subject to
disturbances. A small perturbation of the state can result in a large
error if the control is not taking into account possible deviations from
a precomputed `optimal trajectory'. A control law which depends on the
state as well of time is less sensitive to disturbances; it is more
robust. As a consequence of the special structure of this problem the
optimal solution ˆ\emph{u} is in fact expressible in feedback form as
ˆ\emph{u} = ˆ\emph{u}(\emph{x, t}). In the following sections we present
two different approaches for deriving the optimal control.
\end{quote}

65

\begin{quote}
66 7. LINEAR-QUADRATIC OPTIMAL CONTROL

\textbf{7.1.1. Completion of the square.} Let \emph{P ≥} 0 be a
differentiable symmetric \emph{n × n} matrix function and put\\
\emph{V} (\emph{x, t}) ≜ \emph{xTP}(\emph{t})\emph{x.}

The function \emph{V} (\emph{x, t}) is to be interpreted as the optimal
cost function giving the optimal value of (56) starting at \emph{x}0 =
\emph{x} and \emph{t}0 = \emph{t}. A rational for choosing a quadratic
function could be that this is the simplest form that gives a
non-negative function. What follows is to show that this choice is well
founded.
\end{quote}

differentiate with respect to \emph{t} Define the function \emph{t → V}
(\emph{x}(\emph{t})\emph{, t}) where \emph{x}(\emph{t}) is the solution
of (55) and

\begin{quote}
\emph{d}\\
\emph{dtV} (\emph{x}(\emph{t})\emph{, t}) = ˙\emph{xT Px} + \emph{xT P}
˙\emph{x} + \emph{xT} ˙\emph{Px}\\
= \emph{xTATPx} + \emph{uTBTPx} + \emph{xTPAx} + \emph{xTPBu} +
\emph{xT} ˙\emph{Px}

Then integrating from \emph{t}0 to \emph{t}1 yields

\emph{V} (\emph{x}(\emph{t}1)\emph{, t}1)\emph{−V} (\emph{x}0\emph{,
t}0) =∫ \emph{t}1 (\emph{uTBTPx}+\emph{xTPBu}) \emph{dt}+∫ \emph{t}1
\emph{xT}(\emph{ATP}+\emph{PA}+ ˙\emph{P})\emph{x dt.}

Add this to the functional (56), subtract \emph{V}
(\emph{x}(\emph{t}1)\emph{, t}1) and complete the square to obtain

\emph{J}(\emph{u}) \emph{− V} (\emph{x}0\emph{, t}0) =∫ \emph{t}1
(\emph{u} + \emph{R−}1\emph{BTPx})\emph{TR}(\emph{u} +
\emph{R−}1\emph{BTPx}) \emph{dt}

+∫ \emph{t}1 \emph{xT}(\emph{ATP} + \emph{PA} + ˙\emph{P} + \emph{Q −
PBR−}1\emph{BTP})\emph{x dt} + \emph{x}(\emph{t}1)\emph{T}{[}\emph{S −
P}(\emph{t}1){]}\emph{x}(\emph{t}1)

This rather complicated expression can be considerably simplified if we
choose \emph{P} to satisfy the matrix valued differential equation

(RE) \{ ˙\emph{P P}(\emph{t}1) = \emph{−ATP − PA} +
\emph{PBR−}1\emph{BTP − Q} = \emph{S}

which is the matrix \emph{Riccati equation} (RE). The following theorem
ensures that there really is such a \emph{P} and that it is unique.

Theorem 7.1.1. (RE) \emph{has a unique solution P on the interval}
{[}\emph{t}0\emph{, t}1{]}\emph{, which is positive semidefinite and
bounded.}

We skip the proof which is based on standard results in the theory of
differential equations. Note, however, that it is essential that
\emph{Q} and \emph{S} are positive semidefinite and that \emph{R} is
positive definite.

Choose therefore \emph{P} to be a solution to (RE), then
\end{quote}

\emph{J}(\emph{u}) = \emph{V} (\emph{x}0\emph{, t}0) +∫ \emph{t}1
(\emph{u} + \emph{R−}1\emph{BTPx})\emph{TR}(\emph{u} +
\emph{R−}1\emph{BTPx}) \emph{dt ≥ V} (\emph{x}0\emph{, t}0)

\begin{quote}
with equality if and only if

(57) \emph{u} = \emph{−R−}1\emph{BTPx}
\end{quote}

\includegraphics[width=3.06944in,height=1.11111in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image19.png}

\begin{quote}
7.1. LINEAR-QUADRATIC REGULATOR 67

The control gain \emph{K} = \emph{R−}1\emph{BTP} is called the
\emph{Kalman gain}. Hence the optimal value of \emph{J}(\emph{u}) is
obtained by applying the control law (57) leading to the feedback
configuration
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
u
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Σ
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
x
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
-1\\
-R B P
\end{quote}

Figure 7.1

\begin{quote}
and \emph{V} (\emph{x, t}) is indeed the optimal cost function. Thus
closing the loop results in the new system

(58) \{˙\emph{x} = (\emph{A} + \emph{BK})\emph{x}
\end{quote}

\emph{x}(\emph{t}0) = \emph{x}0

\begin{quote}
which we may solve for \emph{x} to yield the optimal control

(59) \emph{u}(\emph{t}) =
\emph{−R−}1(\emph{t})\emph{BTP}(\emph{t})Φ\emph{K}(\emph{t,
t}0)\emph{x}0

explicitly as a function of time. {[}Here, of course, Φ\emph{K} is the
transition matrix function of the closed loop system (58).{]} Although
applying the \emph{open-loop control} (59) to the system (55)
theoretically leads to the same results as the feedback (57) implemented
as in Figure 7.1, open-loop and closed-loop control may have drastically
different performance from the point of view of stability and
robustness. The reason for this is better understood by looking at the
optimization problem from the point of view of dynamic programming.

\textbf{7.1.2. Dynamic programming, heuristics.} Dynamic programming
provides a different approach for optimal control, which is based on
Bell-man's principle of optimality:\\
\emph{An optimal control has the property that no matter what the
previous control has been, the remaining control must constitute an
optimal control with regard to the state resulting from the previous
control.}

This heuristics of the dynamic programming procedure is illustrated in
Figure 7.2, where the optimization problem starting from
(\emph{t}0\emph{, x}0) is embed-ded in the class of problems obtained by
varying the initial state and initial time. Thus, let the system start
in \emph{x} at time \emph{t} and control the system with some control
\emph{u}. At \emph{t} + \emph{h} the system is in the corresponding
state \emph{x}(\emph{t} + \emph{h}).
\end{quote}

\includegraphics[width=2.91667in,height=1.91667in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image20.png}

\begin{quote}
68 7. LINEAR-QUADRATIC OPTIMAL CONTROL
\end{quote}

x

x

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
t
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
0
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
t+h
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
1
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

Figure 7.2

\begin{quote}
Define the optimal cost function as

\emph{V} (\emph{a, t}) ≜ min \emph{u} \{ ∫\emph{t}1 (\emph{xTQx} +
\emph{uTRu}) \emph{ds} + \emph{xT}(\emph{t}1)\emph{Sx}(\emph{t}1) \}

where \{˙\emph{x} = \emph{Ax} + \emph{Bu}
\end{quote}

\emph{x}(\emph{t}) = \emph{a}

\begin{quote}
Bellman's principle of optimality yields
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{V} (\emph{x, t}) = min

\emph{u} \{ ∫\emph{t}1 \emph{t}+\emph{h} (\emph{xTQx} + \emph{uTRu})
\emph{ds} + \emph{xT}(\emph{t}1)\emph{Sx}(\emph{t}1) +∫
\emph{t}+\emph{h} (\emph{xTQx} + \emph{uTRu}) \emph{ds} \}

= min

\emph{u} \{ \emph{V} (\emph{x}(\emph{t} + \emph{h})\emph{, t} +
\emph{h}) +∫ \emph{t}+\emph{h} (\emph{xTQx} + \emph{uTRu}) \emph{ds} \}

from which we have

min

\emph{u} \{1 {[} \emph{V} (\emph{x}(\emph{t} + \emph{h})\emph{, t} +
\emph{h}) \emph{− V} (\emph{x}(\emph{t})\emph{, t}) {]} + 1
\end{quote}

\emph{h}∫ \emph{t}+\emph{h} (\emph{xTQx} + \emph{uTRu}) \emph{ds} \} =
0\emph{.}

\begin{quote}
Formally, letting \emph{h →} 0, we obtain the functional equation for
\emph{V} (\emph{x, t})

(60)

\emph{u∈}R\emph{k} min \{ \emph{d dtV} (\emph{x}(\emph{t})\emph{, t}) +
\emph{xT Qx} + \emph{uT Ru} \} = 0\emph{,}

i.e. we get a finite-dimensional optimization problem for each \emph{t}.
Once again
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
we assume
\end{quote}

\emph{V} (\emph{x, t}) = \emph{xTP}(\emph{t})\emph{x}

\begin{quote}
for some \emph{P ≥} 0. Then

\emph{d}\\
\emph{dtV} (\emph{x, t}) = ˙\emph{xT Px} + \emph{xT P} ˙\emph{x} +
\emph{xT} ˙\emph{Px}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5000} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
so (60) may be written
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
= 0\emph{.}
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
min\\
\emph{u∈}R\emph{k}
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\{\\
\emph{xT}(\emph{ATP} + \emph{PA} + ˙\emph{P} + \emph{Q})\emph{x} +
\emph{uTBTPx} + \emph{xTPBu} + \emph{uTRu}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\includegraphics[width=3.38889in,height=0.73611in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image21.png}

\begin{quote}
7.2. SOLVING THE RICCATI EQUATION 69

If \emph{P} satisfies (RE), we get,

\emph{u∈}R\emph{k} \{ \}\\
min (\emph{u} + \emph{R−}1\emph{BTPx})\emph{TR}(\emph{u} +
\emph{R−}1\emph{BTPx}) = 0
\end{quote}

where the minimum is attained for \emph{u} = \emph{−R−}1\emph{BTPx}, the
optimal control (57) determined above.

\begin{quote}
Remark 7.1.2. The equation (60) is a special case of the
Hamilton-Jacobi-Bellman equation of optimal control and the derivation
presented here is by no means mathematically rigorous. The proper
theorem of dy-namic programming is the so called ``verification
theorem'', which will be presented in the course ``Optimal Control''.

Hence, we can regard the feedback control (57) as the optimal control
required at time \emph{t} to minimize \emph{future} cost with the
present state as initial condition, i.e. we apply the best possible
control given \emph{present conditions}.
\end{quote}

\textbf{7.2. Solving the Riccati equation}

\begin{quote}
As the previous discussion has shown, the key for obtaining an optimal
control is to solve the corresponding Riccati equation.

Let \emph{X}(\emph{t}) be the regular matrix solution of the feedback
system \{ ˙\emph{X} = (\emph{A − BR−}1\emph{BT P})\emph{X}

and define the matrix function \emph{Y} as \emph{Y} = \emph{PX}. Then

˙\emph{Y} = ˙\emph{PX} + \emph{P} ˙\emph{X}

= \emph{−ATY − PAX} + \emph{PBR−}1\emph{BTY − QX} + \emph{PAX −
PBR−}1\emph{BTY} = \emph{−ATY − QX}\\
Since \emph{X−}1exists for all \emph{t}\\
\emph{P} = \emph{Y X−}1

and consequently (RE) can be replaced by the so-called \emph{adjoint
system}

(61) \{ ˙\emph{X} = \emph{AX − BR−}1\emph{BT Y} ;˙\emph{Y} = \emph{−QX −
ATY} ; \emph{Y} (\emph{t}1) = \emph{S.} \emph{X}(\emph{t}1) = \emph{I}
\end{quote}

\emph{x}1 be the distance from a nominal trajectory, as depicted in
Figure 7.3. Example 7.2.1. A missile will intercept an aircraft in time
\emph{t}1 \emph{− t}0. Let

x1

Figure 7.3

\begin{quote}
70 7. LINEAR-QUADRATIC OPTIMAL CONTROL

Newton's law describes this motion as ¨\emph{x}1 = \emph{u} where the
control \emph{u} is proportional to the force perpendicular to the
trajectory. Hence, setting
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}2 = ˙\emph{x}1\emph{,} the system may be written ˙\emph{x} = {[}
0 0
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
0 {]}

\begin{quote}
1
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{x} +
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[} 1 {]} 0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
We wish to minimize the square of the distance from the target at the
time \emph{t}1 of interception added to an integral term representing
the fuel burnt.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
min\emph{\{x}1(\emph{t}1)2+
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u}(\emph{t})2\emph{dt\}}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u}2\emph{∼} used fuel
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Setting
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0909}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{A} =
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[} 0 0
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
0 {]}

1
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{B} =
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[} 1 {]} 0
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{Q} = 0
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{R} = 1
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{S} =
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
1
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
0 {]}

0
\end{quote}
\end{minipage}} \\
& & & & & & & & & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
the Riccati equation becomes
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 34\tabcolsep) * \real{0.0556}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[}˙\emph{p}11

˙\emph{p}21
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
˙\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
= \emph{−}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[} 1 0
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
0 0 {]} {[} \emph{p}11 \emph{p}21
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]}\emph{−}{[} \emph{p}11

\emph{p}21
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]} {[}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
1 0 {]} + {[} \emph{p}11 \emph{p}21
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{]} {[} 1 {]} 0
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[}0\emph{,} 1{]}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[} \emph{p}21 \emph{p}11
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]}
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
˙\emph{p}22
\end{minipage} & & & & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
0
\end{minipage} & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} & & & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\emph{P}(\emph{t}1) = \emph{S}\\
which leads to the system of three differential equations

It is not easy to solve this system of nonlinear differential equations
by brute  \\
˙\emph{p}11 = \emph{p}2

˙\emph{p}12 = \emph{−p}11 + \emph{p}12\emph{p}22\\
˙\emph{p}22 = \emph{−}2\emph{p}12 + \emph{p}2\\
12

22\\
\emph{p}11(\emph{t}1) = 1
\end{quote}

\emph{p}12(\emph{t}1) = 0

\emph{p}22(\emph{t}1) = 0

\begin{quote}
force. Instead we solve the linear system (61) of eight differential
equations. {[}˙\emph{x}11 ˙\emph{x}21 ˙\emph{x}12 ˙\emph{x}22 {]} = {[}
0 0 1 0 {]} {[} \emph{x}11 \emph{x}21 \emph{x}12 \emph{x}22
{]}\emph{−}{[} 0 0 0 1 {]} {[} \emph{y}11 \emph{y}21 \emph{y}12
\emph{y}22 {]} \emph{X}(\emph{t}1) = \emph{I}

{[}˙\emph{y}11 ˙\emph{y}21 ˙\emph{y}12 ˙\emph{y}22 {]} = \emph{−}{[} 0 1
0 0 {]} {[} \emph{y}11 \emph{y}21 \emph{y}12 \emph{y}22 {]} \emph{Y}
(\emph{t}1) = {[} 1 0 0 0 {]} i.e.

˙\emph{x}11 = \emph{x}21

˙\emph{x}12 = \emph{x}22\\
\emph{x}11(\emph{t}1) = 1

\emph{x}12(\emph{t}1) = 0 \emph{x}11(\emph{t}) = 1 \emph{−}1
\emph{x}12(\emph{t}) = \emph{−}(\emph{t}1 \emph{− t}) 6(\emph{t}1
\emph{− t})3

˙\emph{x}21 = \emph{−y}21 ˙\emph{x}22 = \emph{−y}22 ˙\emph{y}11 = 0
\emph{x}21(\emph{t}1) = 0
\end{quote}

\emph{x}22(\emph{t}1) = 1

\begin{quote}
\emph{y}11(\emph{t}1) = 1 \emph{⇒}\\
\emph{x}21(\emph{t}) =1
\end{quote}

\emph{x}22(\emph{t}) = 1

\begin{quote}
\emph{y}11(\emph{t}) = 1\\
2(\emph{t}1 \emph{− t})2

˙\emph{y}12 = 0

˙\emph{y}21 = \emph{−y}11\\
\emph{y}12(\emph{t}1) = 0

\emph{y}21(\emph{t}1) = 0\\
\emph{y}12(\emph{t}) = 0

\emph{y}21(\emph{t}) = \emph{t}1 \emph{− t}\\
˙\emph{y}22 = \emph{−y}12 \emph{y}22(\emph{t}1) = 0
\emph{y}22(\emph{t}) = 0
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
7.3. FIXED END-POINT PROBLEMS
\end{minipage} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
71
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Therefore we have

\emph{X}(\emph{t}) = {[} 2(\emph{t}1 \emph{− t})2 6(\emph{t}1 \emph{−
t})3 \emph{−}(\emph{t}1 \emph{− t}) 1 {]}

\emph{Y} (\emph{t}) = {[} \emph{t}1 \emph{− t} 0 {]}\\
1 0

and

\emph{X}(\emph{t})\emph{−}1=

1 +\uline{1} 3(\emph{t}1 \emph{− t})3\\
1

{[}\emph{−}\uline{1} 2(\emph{t}1 \emph{− t})2\\
1

1 \emph{−}\uline{1}\\
\emph{t}1 \emph{− t}
\end{quote}

6(\emph{t}1 \emph{− t})3 {]}

\begin{quote}
Consequently

\emph{P}(\emph{t}) = \emph{Y} (\emph{t})\emph{X}(\emph{t})\emph{−}1=

1 +\uline{1} 3(\emph{t}1 \emph{− t})3\\
1

{[} \emph{t}1 \emph{− t}\\
1

(\emph{t}1 \emph{− t})2 \emph{t}1 \emph{− t} {]}

and hence we obtain the \emph{optimal control} law
\end{quote}\strut
\end{minipage} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u} = \emph{−R−}1\emph{BTPx} = \emph{−p}21\emph{x}1 \emph{−
p}22\emph{x}2 =\emph{−}(\emph{t}1 \emph{\sout{−} t})\emph{x}1
\emph{\sout{−}} (\emph{t}1 \emph{\sout{−} t})2\emph{x}2 3(\emph{t}1
\emph{− t})3

The feedback law is a function of remaining time \emph{t}1 \emph{− t}.
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\textbf{7.3. Fixed end-point problems}

\begin{quote}
Consider the problem of transfering the system (55) from state \emph{x}0
at
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
time \emph{t}0 to state \emph{x}1 at time \emph{t}1 so that the
functional\\
\emph{J}(\emph{u}) =∫ \emph{t}1
{[}\emph{x}(\emph{t})\emph{TQ}(\emph{t})\emph{x}(\emph{t}) +
\emph{u}(\emph{t})\emph{TRu}(\emph{t}){]} \emph{dt}
\end{quote}\strut
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Note that
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
is minimized.
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
The parameter matrices are defined as above.
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
no terminal term \emph{x}(\emph{t}1)\emph{TSx}(\emph{t}1) is present
since the end-point is fixed. By normalization we may set \emph{R} =
\emph{I}. Define U to be the class of all control signals which achieve
the desired transfer. The problem is then to find the \emph{u} in U for
which \emph{J}(\emph{u}) is minimized. The special case when \emph{Q} is
zero was treated in Chapter 3. We shall reduce the present problem to
that form.

To this end, let \emph{P} be the solution to (RE) above. The analysis of
Sec-tion 7.1.1 yields
\end{quote}

\emph{J}(\emph{u}) =∫ \emph{t}1 (\emph{u} +
\emph{BTPx})\emph{T}(\emph{u} + \emph{BTPx}) \emph{dt} + \emph{xT}
0\emph{P}(\emph{t}0)\emph{x}0\emph{− xT}
(\emph{t}1)\emph{P}(\emph{t}1)\emph{x}(\emph{t}1)

where \emph{P}(\emph{t}1) should be set to zero. Thus the problem is to
determine a \emph{u ∈} U such that

˜\emph{J}(\emph{u}) =∫ \emph{t}1\emph{∥u} + \emph{BTPx∥}2\emph{dt}

\begin{quote}
is minimized. If \emph{u} = \emph{−BTPx ∈} U we are done. The problem
then has the same solution as that with free end-point. In general, of
course, this is not true so we set\\
\emph{u} = \emph{−BTPx} + \emph{v}

72 7. LINEAR-QUADRATIC OPTIMAL CONTROL

Consequently, the problem has been reduced to the following: Transfer
the system\\
˙\emph{x} = (\emph{A − BBTP})\emph{x} + \emph{Bv}\\
from state \emph{x}0 at time \emph{t}0 to state \emph{x}1 at time
\emph{t}1 so that\\
∫ \emph{t}1\emph{∥v∥}2\emph{dt}

is minimized. Suppose the system is completely reachable so that the
reach-ability gramian is invertible, i.e. \emph{W}(\emph{t,
t}1)\emph{−}1exists , for all \emph{t \textless{} t}1. Then we know from
Theorem 3.1.6 that the optimal control is

\emph{v}(\emph{t}) = \emph{B}(\emph{t})\emph{T}Φ(\emph{t}1\emph{,
t})\emph{TW}(\emph{t}0\emph{, t}1)\emph{−}1{[}\emph{x}1 \emph{−}
Φ(\emph{t}1\emph{, t}0)\emph{x}0{]}\\
According to the principle of optimality this solution is optimal if,
instead of (\emph{t}0\emph{, x}0), we start in the point (\emph{t,
x}(\emph{t}))\emph{, t \textless{} t}1, and \emph{x}(\emph{t}) lies on
the optimal trajectory. This yields an optimal control law
\end{quote}

\emph{v}(\emph{t}) = \emph{K}(\emph{t})\emph{x}(\emph{t}) +
\emph{w}(\emph{t})

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
where
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{K}(\emph{t}) = \emph{−B}(\emph{t})\emph{T}Φ(\emph{t}1\emph{,
t})\emph{TW}(\emph{t, t}1)\emph{−}1Φ(\emph{t}1\emph{, t}) and
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\emph{w}(\emph{t}) = \emph{B}(\emph{t})\emph{T}Φ(\emph{t}1\emph{,
t})\emph{TW}(\emph{t, t}1)\emph{−}1\emph{x}1

The original problem posed in the beginning of this section then has the

feedback solution

\begin{quote}
\emph{u} = (\emph{K − BTP})\emph{x} + \emph{w.}
\end{quote}

CHAPTER 8

\textbf{LQ Control over Infinite Time Interval and ARE}

\begin{quote}
In this chapter we shall consider the linear-quadratic regulator problem
\end{quote}

for the case that \emph{A}, \emph{B}, \emph{Q} and \emph{R} are constant
and the time interval is infinite.

We shall show that, as \emph{t}1 \emph{→ ∞}, the solution of the matrix
Riccati equation, which is central to the solution of the optimal
control problem, tends under

certain conditions to a limit \emph{P} which is the unique symmetric
positive defi-

nite solution of the \emph{algebraic Riccati equation}, and that the
optimal control

\begin{quote}
law is determined by this \emph{P}.
\end{quote}

\textbf{8.1. Existence of a positive definite solution}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(Σ)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Consider the problem of controlling a time-invariant system \{˙\emph{x}
= \emph{Ax} + \emph{Bu};

\emph{y} = \emph{Cx}\\
\emph{x}(0) = \emph{x}0
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
so that
\end{quote}

\emph{J}(\emph{u}) =∫ \emph{t}1 (\emph{xTQx} + \emph{uTRu})\emph{dt}

is minimized, where \emph{Q} = \emph{CTC}. In the previous chapter we
showed that the

\begin{quote}
optimal solution of this problem is given by the feedback law
\end{quote}

\emph{u}(\emph{t}) = \emph{−R−}1\emph{BTP}(\emph{t})\emph{x}(\emph{t})

\begin{quote}
where \emph{P} is the unique solution of the matrix Riccati differential
equation

(RDE) \{ ˙\emph{P} = \emph{−AT P − PA} + \emph{PBR−}1\emph{BT P − CT C}

Recall that the minimal value is
\end{quote}

min \emph{uJ}(\emph{u}) =: \emph{V} (\emph{x}0\emph{, t}0) = \emph{xT}
0\emph{P}(\emph{t}0)\emph{x}0\emph{.}

\begin{quote}
In this chapter, we consider the situation when the final time \emph{t}1
tends
\end{quote}

to infinity, i.e. we ask what happens if \emph{t}1 \emph{→ ∞}; that is,
we have to find the solution the stationary problem

min

\emph{u∈}U∫ \emph{∞}(\emph{xTQx} + \emph{uTRu})\emph{dt}

\begin{quote}
where U is the class of \emph{u} such that
\end{quote}

is an autonomous differential equation, \emph{P}(\emph{t}) can be
expressed as \emph{P}(\emph{t}1 \emph{− t}) (with an abuse of notation).
First, we need to decide under which conditions∫ \emph{∞}0(\emph{xT
Qx}+\emph{uT Ru})\emph{dt \textless{} ∞}. Since (RDE)

73

\begin{quote}
74 8. LQ CONTROL OVER INFINITE TIME INTERVAL AND ARE

there is a limiting solution \emph{P∞} for \emph{P}(\emph{t}1\emph{−t})
as \emph{t}1\emph{−t} tends to infinity, namely the limiting solution of
\emph{P∞} := \emph{P}(\emph{t}1 \emph{−} 0) as \emph{t}1 tends to
infinity. To this end, we assume that Σ is completely reachable. Then,
there exists a control signal ˆ\emph{u} that transfers the system from
(0\emph{, x}0) to (\emph{T,} 0). Now, let ˆ\emph{u}(\emph{t}) \emph{≡} 0
for \emph{t ≥ T}, which implies that ˆ\emph{y}(\emph{t}) \emph{≡} 0 for
\emph{t ≥ T}.

In order to indicate \emph{V} (\emph{x}0\emph{,} 0) is a function of
\emph{t}1, similarly we slightly abuse the notation by denoting \emph{V}
(\emph{x}0\emph{,} 0) as \emph{V} (\emph{x}0\emph{, t}1). Then we know
that

\emph{V} (\emph{x}0\emph{, t}1) \emph{≤}∫ \emph{∞}(\emph{xTQx} +
\emph{uTRu})\emph{dt} =: \emph{M} for all \emph{t}1 \emph{∈} (0\emph{,
∞})\emph{.}

Moreover, \emph{V} (\emph{x}0\emph{, t}1) is monotonically
non-decreasing, and therefore,

\emph{V} (\emph{x}0\emph{, t}1) \emph{→ V∞}(\emph{x}0) = \emph{xT}
0\emph{P∞x}0\emph{≥} 0\\
for all \emph{x}0 \emph{∈} R\emph{n}, and therefore \emph{P}(\emph{t}1
\emph{−} 0) \emph{→ P∞} as \emph{t}1 \emph{→ ∞}. But then \emph{P∞}must
satisfy the \emph{algebraic Riccati equation}

(ARE) \emph{ATP} + \emph{PA − PBR−}1\emph{BTP} + \emph{Q} = 0\emph{.}

It is evident that \emph{P∞} is real symmetric positive semidefinite. We
note that the ARE can have several solutions That do not need to be
symmetric. \emph{P∞}is always a solution to the ARE, but not all
solutions of the ARE are limiting solutions of the corresponding RDE.

A natural question is now : when is \emph{P∞} also positive definite,
i.e., \emph{P∞ \textgreater{}} 0? Obviously this is true if \emph{Q
\textgreater{}} 0. However in our case \emph{Q} = \emph{CTC} that is in
general only semi-definite. We know that

\emph{xT} 0\emph{P∞x}0\emph{≥ V} (\emph{x}0\emph{, t}1) \emph{≥} 0
\emph{∀t}1 \emph{≥} 0\emph{.}

Thus, if we can prove that, for some \emph{t}1 \emph{\textgreater{}} 0,
\emph{V} (\emph{x}0\emph{, t}1) is positive for all \emph{x}0 \emph{̸}=
0, then \emph{P∞ \textgreater{}} 0. Therefore, we assume that \emph{V}
(\emph{x}0\emph{, t}1) = 0 although \emph{x}0 \emph{̸}= 0. Then, for the
optimal solution, \emph{u}(\emph{t}) \emph{≡} 0, and \emph{y}(\emph{t})
\emph{≡} 0 on (0\emph{, t}1), that is\\
\emph{Cx ≡} 0 \emph{C} ˙\emph{x} = \emph{CAx ≡} 0 \emph{C}¨\emph{x} =
\emph{CA}2\emph{x ≡} 0 ...   =\emph{⇒}   \emph{CA}2 \emph{CA}
\emph{C}

... \\
 \emph{x}(\emph{t}) = 0\\
where \emph{x}(\emph{t}) = \emph{eAtx}0 \emph{̸}= 0. It is clear that
this cannot happen if Σ is completely observable. In fact, the
observability of the system Σ tells us that

(62) \emph{x}0 \emph{̸}= 0 =\emph{⇒ xT} 0\emph{P∞x}0\emph{≥ V}
(\emph{x}0\emph{, t}1) = \emph{xT}
0\emph{P}(\emph{t}1)\emph{x}0\emph{\textgreater{}} 0\emph{,}

i.e., \emph{P∞ \textgreater{}} 0. Therefore, we have proved the
following theorem.

Theorem 8.1.1. \emph{Suppose that} Σ \emph{is a minimal realization.
Then (ARE) has a real symmetric positive definite solution.}

Note that this does not exclude the possibility that (ARE) may have
other solutions which are not positive definite.

8.1. EXISTENCE OF A POSITIVE DEFINITE SOLUTION 75

Corollary 8.1.2. \emph{If} Σ \emph{is a minimal realization,}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{for all u ∈} U\emph{.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{x}(\emph{t}) \emph{→} 0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{as t → ∞}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\textbf{Proof:} Due to time-invariance\\
∫ \emph{ν}+1 (\emph{xTQx} + \emph{uTRu})\emph{dt ≥ V}
(\emph{x}(\emph{ν})\emph{,} 1) =
\emph{x}(\emph{ν})\emph{′P}(1)\emph{x}(\emph{ν}) \emph{≥
λ}min\emph{∥x}(\emph{ν})\emph{∥}2

where \emph{λ}min is the smallest eigenvalue of \emph{P}(1). But
according to (62) above, \emph{P}(1) \emph{\textgreater{}} 0, i.e.
\emph{λ}min \emph{\textgreater{}} 0. Therefore,\\
∑\emph{∥x}(\emph{ν})\emph{∥}2\emph{≤} (\emph{λ}min)\emph{−}1∫
\emph{∞}(\emph{xTQx} + \emph{uTRu})\emph{dt \textless{} ∞}

if \emph{u ∈} U, and consequently, \emph{x}(\emph{ν}) \emph{→} 0 as
\emph{ν → ∞}. Now \emph{x}(\emph{t}) =
\emph{eA}(\emph{t−}{[}\emph{t}{]})\emph{x}({[}\emph{t}{]}) +∫ \emph{t
eA}(\emph{t−τ})\emph{Bu}(\emph{τ})\emph{dτ}
\end{quote}

where {[}\emph{t}{]} is the integer part of \emph{t}. We have just shown
that the first term tends

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.0526}}@{}}
\toprule()
\multicolumn{17}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.8947} + 32\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
to zero as \emph{t → ∞}. To estimate the second term, we apply
Cauchy-Schwartz inequality:
\end{quote}
\end{minipage}} &
\multirow{7}{*}{\begin{minipage}[b]{\linewidth}\raggedright
5 {]}

0
\end{minipage}} &
\multirow{7}{*}{\begin{minipage}[b]{\linewidth}\raggedright
= 0\emph{.}
\end{minipage}} \\
\multicolumn{7}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.3684} + 12\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
because \emph{u ∈} U implies that����∫ \emph{t
eA}(\emph{t−τ})\emph{Bu}(\emph{τ})\emph{dτ}

���� \emph{≤}∫ \emph{t}����\emph{eA}(\emph{t−τ})\emph{B}
\end{quote}

2

\begin{quote}
∫ \emph{ν}+1\emph{∥u∥}2\emph{dτ →} 0 as \emph{ν → ∞}.
\end{quote}
\end{minipage}}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
����\\
2
\end{quote}\strut
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{dτ}
\end{minipage}} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1579} + 4\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage}}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1053} + 2\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{∥u∥}2\emph{dτ →} 0\emph{,}
\end{minipage}}} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1579} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
as \emph{t → ∞,}
\end{quote}
\end{minipage}} \\
& & & & & & & & & & & & & &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1579} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage}} \\
\multicolumn{8}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.4211} + 14\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Example 8.1.3. Control the system

˙\emph{x} = {[} 0 0 {]} \emph{x} +\\
0 1
\end{quote}\strut
\end{minipage}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1053} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
{[} 1 {]} 0
\end{minipage}} &
\multicolumn{7}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.3684} + 12\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u}
\end{quote}
\end{minipage}} \\
\multicolumn{17}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.8947} + 32\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
so that
\end{quote}

∫ \emph{∞}(4\emph{x}2 1+ 5\emph{x}2 2+ \emph{u}2)\emph{dt}

\begin{quote}
is minimized.

Let \emph{A} = {[} 0 0 {]} \emph{B} = {[} 1 {]} , \emph{C} = {[}2
\emph{√}5 {]} , \emph{R} = \emph{I}. Then \emph{CTC} = {[} 0 5 {]} .

0 1 0 0 4 0

Moreover,

that (\emph{A, B, C}) is a minimal realization. Thus, the algebraic
Riccati equation {[}\emph{B} \emph{AB}{]} = {[}\\
0
\end{quote}

1

1

0 {]} and {[} \emph{CA}

\emph{C}

{]} =

\begin{quote}
\\
0
\end{quote}

0

\begin{quote}
0\\
\emph{√}
\end{quote}

2

\begin{quote}
0\\
5

 are of full rank so\\
2 0
\end{quote}\strut
\end{minipage}} \\
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
becomes

\begin{quote}
{[}\\
0
\end{quote}

1

0

0 {]} {[} \emph{p}11

\emph{p}12\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]} +
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{[} \emph{p}11

\emph{p}12
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{]} {[} 0 0
\end{quote}
\end{minipage}} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1579} + 4\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
1 0 {]}\emph{−}{[} \emph{p}11 \emph{p}12
\end{minipage}}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1053} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage}} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1053} + 2\tabcolsep}}{%
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{]} {[} 1 {]} {[}0 0
\end{quote}
\end{minipage}}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
1{]} {[}
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]} + {[} 0

4
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} & & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} & & & & &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 36\tabcolsep) * \real{0.1053} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage}} & & & & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}12
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}22
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
76
\end{minipage} &
\multicolumn{8}{>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.8000} + 14\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
8. LQ CONTROL OVER INFINITE TIME INTERVAL AND ARE
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}

□
\end{quote}
\end{minipage}} \\
\multicolumn{9}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.9000} + 16\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
By solving the resulting algebraic equations

we obtain\\


2\emph{p}12 \emph{− p}2\\
\emph{p}11 \emph{− p}12\emph{p}22 = 0\\
4 \emph{− p}2

22+ 5 = 0\\
12= 0
\end{quote}

\emph{p}12 = \emph{±}2

\begin{quote}
\emph{p}12 = 2 \emph{⇒ p}22 = \emph{±}3
\end{quote}

\emph{p}12 = \emph{−}2 \emph{⇒ p}22 = \emph{±}1

\emph{p}11 = \emph{p}12\emph{p}22\strut
\end{minipage}} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.2000} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Therefore, (ARE) has four solutions:

\emph{P}1 = {[}\\
6
\end{quote}

2

2

\begin{quote}
3 {]} \emph{,} \emph{P}2 = {[}\emph{−}6 2 \emph{−}3 2
\end{quote}

{]} \emph{,}\strut
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{P}3 =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[}\emph{−}2

\emph{−}2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}2 1 {]}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{,}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{P}4 =
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[}\emph{−}2 2
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{−}2\emph{−}1 {]}
\end{quote}
\end{minipage} \\
\multicolumn{9}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.9000} + 16\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
It is not hard to see that \emph{P}1 \emph{\textgreater{}} 0, \emph{P}2
\emph{\textless{}} 0, and \emph{P}3 and \emph{P}4 are indefinite.
\end{quote}
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\textbf{8.2. The optimal control law and the question of uniqueness}

\begin{quote}
We now proceed to determining the optimal control. Let Σ be a
\emph{minimal}

realization, and let \emph{P} be an arbitrary solution of (ARE). Then,

\emph{d}

\emph{dt}(\emph{xT Px}) = ˙\emph{xT Px} + \emph{xT P} ˙\emph{x}
\end{quote}

= \emph{xT}(\emph{ATP} + \emph{PA})\emph{x} + \emph{uTBTPx} +
\emph{xTPBu}

= \emph{∥R} 2 \emph{u} + \emph{R−} \sout{1} 2 \emph{BTPx∥}2\emph{− uTRu
− ∥y∥}2

because \emph{ATP} +\emph{PA} = \emph{PBR−}1\emph{BTP −CTC} since
\emph{P} is the solution of (ARE). Integrating this yields

\begin{quote}
(63)
\end{quote}

\emph{x}(\emph{t})\emph{TPx}(\emph{t}) \emph{− xT} 0\emph{Px}0=∫
\emph{t∥R} 2 \emph{u} + \emph{R−} \sout{1} 2 \emph{BTPx∥}2\emph{ds −}∫
\emph{t} (\emph{∥y∥}2+ \emph{uTRu})\emph{ds}

Suppose now that \emph{u ∈} U. Then, \emph{x}(\emph{t}) \emph{→} 0 as
\emph{t → ∞} (Corollary 8.1.2). Therefore,

∫ \emph{∞}(\emph{∥y∥}2+ \emph{uTRu})\emph{ds} = \emph{xT} 0\emph{Px}0+∫
\emph{∞∥R} 2 \emph{u} + \emph{R−} \sout{1} 2 \emph{BTPx∥}2\emph{ds}

\emph{≥ xT} 0\emph{Px}0\emph{.}

\begin{quote}
Hence, the control law
\end{quote}

\emph{u} = \emph{−R−}1\emph{BTPx}

is optimal \emph{provided} that it defines a control signal \emph{u ∈}
U. The feedback system becomes

˙\emph{x} = (\emph{A − BR−}1\emph{BTP})\emph{x}

\begin{quote}
Setting Γ ≜ \emph{A−BR−}1\emph{BTP} we have \emph{y}(\emph{t}) =
\emph{Ce}Γ\emph{tx}0 and \emph{u}(\emph{t}) =
\emph{−R−}1\emph{BTPe}Γ\emph{tx}0.

If Γ is a stable matrix, it follows that \emph{u ∈} U.

8.2. OPTIMAL CONTROL LAW 77

Lemma 8.2.1. \emph{If} Σ \emph{is a minimal realization and if P is a
real symmetric positive definite solution of} (ARE)\emph{, then}

Γ = \emph{A − BR−}1\emph{BTP}\\
\emph{is a stable matrix.}

\textbf{Proof:} The algebraic Riccati equation can be written as
\end{quote}

Γ\emph{TP} + \emph{P}Γ + \emph{PBR−}1\emph{BTP} + \emph{CTC} = 0\emph{.}

\begin{quote}
We now proceed as in the proof of Theorem 28. Form the Lyapunov function
\emph{V} (\emph{x}) ≜ \emph{xTPx}, which is strictly positive for all
\emph{x ̸}= 0. Differentiation of \emph{V} along a solution of ˙\emph{x}
= Γ\emph{x} yields

\emph{d}\\
\emph{dt}(\emph{xT Px}) = ˙\emph{xT Px} + \emph{xT P} ˙\emph{x} =
\emph{xT} Γ\emph{T Px} + \emph{xT P}Γ\emph{x} =\emph{xT}(Γ\emph{TP} +
\emph{P}Γ)\emph{x} = \emph{−xT}(\emph{PBR−}1\emph{BTP} +
\emph{CTC})\emph{x ≤} 0\emph{.}

Consequently, \emph{V} (\emph{x}(\emph{t})) is non-increasing along
˙\emph{x} = Γ\emph{x}, and it follows that the solutions remain bounded.
Hence, Re \emph{λ}(Γ) \emph{≤} 0. We now show that Γ has no eigenvalue
on the imaginary axis. Recall that Γ has a purely imaginary eigenvalue
if and only if there is a nontrivial periodic solution of ˙\emph{x} =
Γ\emph{x}. Therefore, it suffices to show that there is no nontrivial
periodic solution.

Suppose that \emph{x}(\emph{t}) is a periodic solution such that
\emph{x}(\emph{t}0) = \emph{x}(\emph{t}1). Then \emph{V}
(\emph{x}(\emph{t}0)) = \emph{V} (\emph{x}(\emph{t}1)) and

\emph{d}\\
\emph{dtV} (\emph{x}(\emph{t})) = \emph{\textbar R−} \sout{1} 2
\emph{BTPx}(\emph{t})\emph{\textbar{}}2+
\emph{\textbar Cx}(\emph{t})\emph{\textbar{}}2\emph{≡} 0

on {[}\emph{t}0\emph{, t}1{]}, and by periodicity for all \emph{t}.
Hence, \emph{R−}1\emph{BTPx ≡} 0 and
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
˙\emph{x} = Γ\emph{x} = \emph{Ax − BR−}1\emph{BTPx} = \emph{Ax.}

\begin{quote}
\emph{d}\\
Moreover, \emph{Cx ≡} 0 and \emph{dtCx} = \emph{CAx ≡} 0. Further
differentiation yields \emph{CAkx}(\emph{t}) \emph{≡} 0 for all
\emph{k}. Since (\emph{C, A}) is observable, it follows that
\emph{x}(\emph{t}) \emph{≡} 0.□
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Lemma 8.2.2. \emph{There is at most one real symmetric solution of}
(ARE) \emph{such that}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{is a stable matrix.}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Γ = \emph{A − BR−}1\emph{BTP}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\textbf{Proof:} Assume that there are two such matrices \emph{P}1 and
\emph{P}2. According to (63), we have, for \emph{i} = 1\emph{,} 2,
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
(\emph{∥y∥}2+ \emph{uTRu})\emph{ds} = \emph{xT} 0\emph{Pix}0+
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
∫ \emph{t}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{∥R}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
2 \emph{u} + \emph{R−} \sout{1} 2 \emph{BTPix∥}2\emph{ds −
x}(\emph{t})\emph{TPix}(\emph{t})\emph{.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
78 8. LQ CONTROL OVER INFINITE TIME INTERVAL AND ARE

Set \emph{u} = \emph{−R−}1\emph{BTP}2\emph{x} and let \emph{t → ∞}.
Since \emph{A − BR−}1\emph{BTP}2 is stable, \emph{x}(\emph{t}) \emph{→}
0 and therefore,\\
∫ \emph{∞}(\emph{∥y∥}2+ \emph{uTRu})\emph{ds} = \emph{xT}
0\emph{P}1\emph{x}0+∫ \emph{∞∥R−} \sout{1} 2 \emph{BT}(\emph{P}1 \emph{−
P}2)\emph{x∥}2\emph{ds} = \emph{xT} 0\emph{P}2\emph{x}0
\end{quote}

that is \emph{xT} us that \emph{xT} 0\emph{P}2\emph{x}0 \emph{≥ xT}
0\emph{P}1\emph{x}0 \emph{≥ xT} 0\emph{P}1\emph{x}0. The same argument
with \emph{u} = \emph{−R−}1\emph{BT P}1\emph{x} tells
0\emph{P}2\emph{x}0. Hence, we have shown that

\begin{quote}
\emph{xT} 0\emph{P}1\emph{x}0= \emph{xT} 0\emph{P}2\emph{x}0\emph{,} for
all \emph{x}0 \emph{∈} R\emph{n,}\\
which implies that \emph{P}1 = \emph{P}2. □

Example 8.2.3. Consider Example 8.1.3. Then

The characteristic polynomial of Γ is Γ = \emph{A − BBTP} = {[} 0 0 1 0
{]}\emph{−}{[} 0 1 {]} {[}0 1{]} {[} \emph{p}11 \emph{p}12 \emph{p}12
\emph{p}22 {]} = {[}\emph{−p}12 0 \emph{−p}22 1 {]}
\end{quote}

\emph{χ}Γ(\emph{s}) = \emph{s}2+ \emph{p}22\emph{s} + \emph{p}12

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
which, for each solution of (ARE), takes the form

\emph{P}1 = {[} 2 3 {]} \emph{,} \emph{χ}Γ(\emph{s}) = \emph{s}2+
3\emph{s} + 2 = (\emph{s} + 1)(\emph{s} + 2)

6 2

\emph{P}2 = {[}\emph{−}6 2 \emph{−}3 2

{]} \emph{,} \emph{χ}Γ(\emph{s}) = \emph{s}2\emph{−} 3\emph{s} + 2 =
(\emph{s −} 1)(\emph{s −} 2)

\emph{P}3 = {[}\emph{−}2

\emph{−}2 \emph{−}2 1 {]} \emph{,} \emph{χ}Γ(\emph{s}) = \emph{s}2+
\emph{s −} 2 = (\emph{s −} 1)(\emph{s} + 2)

\emph{P}4 = {[}\emph{−}2 2 \emph{−}2

\emph{−}1 {]} \emph{,} \emph{χ}Γ(\emph{s}) = \emph{s}2\emph{− s −} 2 =
(\emph{s} + 1)(\emph{s −} 2)

Then, the optimal control law is
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
i.e. Γ is stable.

i.e. Γ is unstable.

i.e. Γ is unstable.

i.e. Γ is unstable.
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
\emph{u} = \emph{−BTPx} = \emph{−p}12\emph{x}1 \emph{− p}22\emph{x}2\\
where the real symmetric positive definite solution \emph{P}1 must be
used, i.e.
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
The optimal future cost is
\end{quote}
\end{minipage}} &
\multicolumn{5}{>{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.7143} + 8\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u} = \emph{−}2\emph{x}1 \emph{−} 3\emph{x}2\emph{.}
\end{quote}
\end{minipage}} &
\multirow{4}{*}{\begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage}} \\
& \multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[} 2 6
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]} {[} \emph{x}1

\emph{x}2
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
{]}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
= 6\emph{x}2 1+ 4\emph{x}1\emph{x}2+ 3\emph{x}2 2\emph{.}
\end{quote}
\end{minipage}} \\
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{xTPx} = {[}\emph{x}1\emph{, x}2{]}
\end{minipage}} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
3
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Theorem 8.2.4. \emph{Let} Σ \emph{be a minimal realization. Then} (ARE)
\emph{has a unique real symmetric positive definite solution P.
Moreover,}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u} = \emph{−R−}1\emph{BTPx}\\
\emph{is the unique optimal control law which minimizes J}(\emph{u}) =

\emph{uTRu})\emph{dt and} min \emph{J}(\emph{u}) = \emph{xT}
0\emph{Px}0\emph{.}
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
∫ \emph{∞}0(\emph{∥y∥}2 +
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
8.2. OPTIMAL CONTROL LAW 79
\end{quote}

\textbf{Proof:} By Theorem 8.1.1, (ARE) has a solution \emph{P
\textgreater{}} 0. But Lemma 8.2.1

and Lemma 8.2.2 show that this is the only solution which is real
symmetric

positive definite. Since only such a solution corresponds to a matrix Γ
which

\begin{quote}
is a stable matrix, it follows from the definition above that \emph{u} =
\emph{−R−}1\emph{BTPx} is an optimal control law and that the optimal
value is \emph{xTPx}. □
\end{quote}

The following simple observations from linear algebra have been used
above.

\begin{quote}
Remark 8.2.5. If \emph{xTP}(\emph{t})\emph{x → xTPx} for all \emph{x ∈}
R\emph{n}, then \emph{P}(\emph{t}) \emph{→ P} as

\emph{t → ∞}.
\end{quote}

\textbf{Proof:} First take \emph{x} = \emph{ek}, the unit vector with
zeros in all positions except

position \emph{k} where there is a one. Then, \emph{pkk}(\emph{t})
\emph{→ pkk}, taking care of the

\begin{quote}
diagonal entries. Next take \emph{x} = \emph{ek} + \emph{el}. Then,
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{pkk}(\emph{t}) + 2\emph{pkl}(\emph{t}) + \emph{pll}(\emph{t})
\emph{→ pkk} + 2\emph{pkl} + \emph{pll,} which shows that
\emph{pkl}(\emph{t}) \emph{→ pkl}.
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
□
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Remark 8.2.6. If \emph{Q} is a symmetric matrix, then
\end{quote}

\emph{λ}min\emph{∥x∥}2\emph{≤ xTQx ≤ λ}max\emph{∥x∥}2\emph{,}

where \emph{λ}min and \emph{λ}max are the smallest and the largest
eigenvalues of \emph{Q},

\begin{quote}
respectively.

\textbf{Proof:} If \emph{Q} is symmetric, there is an orthogonal matrix
\emph{S} such that

\emph{SQST}= diag\emph{\{λ}1\emph{, λ}2\emph{, . . . , λn\}}. Then
\end{quote}

\emph{n}

\emph{xTQx} = \emph{xTSTSQSTSx} =∑\emph{λi}(\emph{Sx})2 \emph{i}

\begin{quote}
i.e.
\end{quote}

\emph{λ}min∑(\emph{Sx})2 \emph{i≤ xT Qx ≤ λ}max∑(\emph{Sx})2 \emph{i}

\begin{quote}
Since \emph{STS} = \emph{I}, \emph{xTSTSx} = \emph{∥x∥}2, the above
inequality is the desired one. The proof is complete. □
\end{quote}

CHAPTER 9

\textbf{Kalman Filtering}

\begin{quote}
In 1960 Rudolf E. Kalman published his famous paper describing the
op-timal solution to the discrete-data linear filtering problem. Before
Kalman's solution Norbert Wiener had already described an optimal finite
impulse response filter. However, Wiener filter requires computation of
the impulse response, which is not suitable for on-line implementation.

In Kalman's derivation a recursive approach was proposed using state
space descriptions, which could be easily implemented on-line. The state
space description also enables the filter to be used either as a filter,
smoother or predictor.

The recursive nature of Kalman filter has proven to be very useful.
Kalman filter is perhaps so far the best known result coming from
systems and control since 1960's and it has found a very wide range of
applications, in particular in navigation and tracking.

Let us use a simple example here to illustrate the idea of Kalman
filter.

Example 9.0.7. Two persons make an observation of \emph{x} (say the
height of a building) each.

\emph{•} Person1's observation is \emph{y}1 with confidence (variance)
\emph{σ}2 \emph{y}1.\emph{•} Person2's observation is \emph{y}2 with
confidence (variance) \emph{σ}2 \emph{y}2.

Person 1's observation arrives first so we use
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
ˆ\emph{x}1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{y}1
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\emph{σ}2 1 & = & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{σ}2 \emph{y}1
\end{quote}
\end{minipage} \\
\bottomrule()
\end{longtable}

\begin{quote}
as the best estimation of \emph{x} since no priori information about
\emph{x} is available. We then update once the second observation is
available:
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
(64)\\
Here \emph{K} =
\end{quote}\strut
\end{minipage}} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.7500} + 4\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
ˆ\emph{x}2 = ˆ\emph{x}1 + \emph{K}(\emph{y}2 \emph{−} ˆ\emph{x}1)\emph{,
σ}2

\emph{σ}2 1+\emph{σ}2\emph{y}2is the so-called \emph{Kalman gain}.
Consequently
\end{quote}
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{σ}2 1\emph{σ}2 \emph{y}2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule()
\endhead
& \emph{σ} 2= & \emph{σ}2 1+ \emph{σ}2\emph{y}2 &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage} \\
\bottomrule()
\end{longtable}

\begin{quote}
Or we can directly compute
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(65)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ˆ\emph{x}2 =\emph{σ}2 \emph{y}2\emph{y}1+ \emph{σ}2 \emph{y}1\emph{y}2

\emph{σ}2\emph{y}1 + \emph{σ}2\emph{y}2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

81

\begin{quote}
82 9. KALMAN FILTERING

While (64) and (65) give the same estimation, only the first is
recursive. The latter is an example of the classical Gauss-Markov
parameter estimation.
\end{quote}

\textbf{9.1. The discrete-time filter}

\begin{quote}
Consider a linear system
\end{quote}

\emph{x}(\emph{t} + 1) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{v}(\emph{t})

\begin{quote}
(66) \emph{y}(\emph{t}) = \emph{C}(\emph{t})\emph{x}(\emph{t}) +
\emph{D}(\emph{t})\emph{w}(\emph{t})
\end{quote}

\emph{x}(0) = \emph{x}0 (\emph{unknown})\emph{,}

\begin{quote}
defined on the interval {[}0\emph{, t}1{]}, where the input signals
\emph{v}(\emph{t}) and \emph{w}(\emph{t}) are taken to be random
processes. More specifically, we assume that \emph{v} and \emph{w} are
uncorrelated white noises, i.e.,
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{E v}(\emph{t})\emph{v}(\emph{s})\emph{T}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{Qδts}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\emph{E w}(\emph{t})\emph{w}(\emph{s})\emph{T} & = &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{Rδts}
\end{quote}
\end{minipage} \\
\emph{E v}(\emph{t})\emph{w}(\emph{s})\emph{T} & = &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
0\emph{,}
\end{quote}
\end{minipage} \\
\bottomrule()
\end{longtable}

\begin{quote}
and that the initial state \emph{x}0 is a stochastic vector, with
covariance \emph{P}0 = \emph{E x}0\emph{xT} 0\emph{,}

uncorrelated with both \emph{v} and \emph{w}, i.e., \emph{E
x}0\emph{v}(\emph{t})\emph{T}= \emph{E x}0\emph{w}(\emph{t})\emph{T}= 0.
Moreover, we assume that \emph{x}0\emph{, v} and \emph{w} are
\emph{centered}, i.e., \emph{Ex}0 = \emph{Ev}(\emph{t}) =
\emph{Ew}(\emph{t}) = 0.

The problem is now to estimate the state \emph{x}(\emph{t}) in the best
possible way using the observations \emph{\{y}(0)\emph{, y}(1)\emph{,
..., y}(\emph{t −} 1)\emph{\}}. By best possible we mean that the
estimate shall be optimal in the sense of \emph{least-squares}, i.e.,
the state estimate ˆ\emph{x}(\emph{t}) should be chosen so that the
variance of the error

(67) \emph{E}{[}\emph{xi}(\emph{t}) \emph{−} ˆ\emph{xi}(\emph{t}){]}2\\
is minimized for each \emph{i} = 1\emph{,} 2\emph{, . . . , n} and each
time \emph{t}.

We would like the estimator to be practically implementable in an
ef-ficient manner. As time proceeds the acquired amount of data
\emph{\{y}(0)\emph{, y}(1)\emph{, . . . , y}(\emph{t −} 1)\emph{\}}
grows and can become very large. plementable with bounded memory, where
the bound is independent of the We seek a solution im-

length of the interval {[}0\emph{, t}1{]}. Moreover, we would like the
computations to be recursive in order to be efficient.

Finally, we shall restrict the estimate ˆ\emph{xi}(\emph{t}) to be a
\emph{linear} function of the observed data

(68) \emph{\{y}1(0)\emph{, . . . , ym}(0)\emph{, y}1(1)\emph{, . . . ,
ym}(1)\emph{, . . . , y}1(\emph{t −} 1)\emph{, . . . , ym}(\emph{t −}
1)\emph{\},} which minimizes the least-square criterion (67). This
raises the question whether there really is such an estimate and, if so,
whether it is unique.

Before turning to this question, however, let us comment on the
assump-tion of linearity. It is apparently a restriction to require that
ˆ\emph{xi}(\emph{t}) should be a linear function of the data (68),
rather than allowing nonlinear functions as well. However there is an
important case for which \emph{the optimal least-squares estimate in the
larger class of not necessarily linear functions of the data is}

9.1. THE DISCRETE-TIME FILTER 83

\emph{in fact linear, namely when x}0\emph{, v, w are jointly} Gaussian.
In this case all stochastic variables defined by the system (66) become
Gaussian, because the Gaussian property is preserved under linear
transformations.

In the following we shall show how the estimate ˆ\emph{x}(\emph{t}) can
be delivered by a time-varying finite-dimensional linear system driven
by the observations \emph{y}(\emph{t}). Such a system is called a
\emph{Kalman filter} and meets with the specifica-tions of bounded
memory and recursive computations.

Remark 9.1.1. A useful interpretation of the system in (66) is as
follows. The state process \emph{x}(\emph{t}) describes an airplane with
dynamics described by the matrices \emph{A} and \emph{B}. The actual
control action taken by the pilot is to us unknown and is modeled as a
random process \emph{v} with some known statistics. Moreover, we perform
observations of the state as \emph{y}(\emph{t}) =
\emph{Cx}(\emph{t})+\emph{Dw}(\emph{t}). The observations are of reduced
dimension, and corrupted by the measurement noise \emph{w}. □

\textbf{9.1.1. Linear least-squares estimation and orthogonal
projec-tions.} The family of all stochastic variables with finite second
order mo-ment generates a vector space \emph{H} whose elements are
precisely all linear combinations of these generating stochastic
variables. The space \emph{H} is an inner-product space (Hilbert space)
with \emph{inner product}

(\emph{ξ, η}) = \emph{E\{ξη\}}\\
and norm \emph{∥ξ∥} = (\emph{ξ, ξ})1\emph{/}2. When \emph{H} is
generated by stochastic variables in (68), it is finite dimensional. In
this \emph{H} there is a nested family of subspaces

\emph{H}0(\emph{y}) \emph{⊂ H}1(\emph{y}) \emph{⊂ H}2(\emph{y}) \emph{⊂
. . . ⊂ Ht}1(\emph{y}) where \emph{Ht}(\emph{y}) is the space of all
linear combinations of
\end{quote}

\emph{\{y}1(0)\emph{, . . . , ym}(0)\emph{, y}1(1)\emph{, . . . ,
ym}(1)\emph{, . . . , y}1(\emph{t})\emph{, . . . ,
ym}(\emph{t})\emph{\}.}

\begin{quote}
Since \emph{E}{[}\emph{xi}(\emph{t}) \emph{−} ˆ\emph{xi}(\emph{t}){]}2=
\emph{∥xi}(\emph{t}) \emph{−} ˆ\emph{xi}(\emph{t})\emph{∥}2the linear
least-squares problem is thus reduced to determining an
ˆ\emph{xi}(\emph{t}) in \emph{Ht}(\emph{y}) which minimizes
\emph{∥xi}(\emph{t})\emph{−}ˆ\emph{xi}(\emph{t})\emph{∥}for each
\emph{i} = 1\emph{,} 2\emph{, . . . , n}. Kalman filter on the other
hand will give a recursive procedure for determining the optimal
solution in \emph{Ht}(\emph{y})based on the optimal solution in
\emph{Ht−}1(\emph{y}).

The existence of such a minimizer as well as equations for determining
it is given by the following \emph{projection theorem}.

Lemma 9.1.2. \emph{Let M be a subspace of the finite-dimensional
inner-product space H. Let h ∈ H be arbitrary. Then there is a unique}
ˆ\emph{m ∈ M such that the distance \textbar\textbar h −
m\textbar\textbar{} is minimized. The minimizer} ˆ\emph{m is
charac-terized by the condition that the error}˜\emph{h} ≜ \emph{h −}
ˆ\emph{m is orthogonal to M, i.e.,} (\emph{h −} ˆ\emph{m, m}) = 0
\emph{for all m ∈ M .}
\end{quote}

called the \emph{normal equations}. Remark 9.1.3. The condition that
(\emph{h −} ˆ\emph{m, m}) = 0 for all \emph{m ∈ M} is□

\includegraphics[width=2.76389in,height=1.84722in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image22.png}

\begin{quote}
84 9. KALMAN FILTERING

Remark 9.1.4. The linear space \emph{Ht−}1(\emph{y}) is formed by taking
\emph{linear} combinations of the observed random variables. By
projecting onto this space we get the best linear estimate of
\emph{xi}(\emph{t}). We can also imagine estimators being e.g.
polynomials of some degree \emph{q}. The set of such polynomials,
\emph{Qt−}1(\emph{y}) say, is also a finite-dimensional linear space (
not a subspace of \emph{H}, though) and the best estimator in
\emph{Qt−}1(\emph{y}) would also be given by orthogonal projection.
Hence, the functional form of the estimate is determined by the
structure of the subspace onto which we project. □

Consequently, we may regard the least-squares estimate as the orthogonal
projection of \emph{h} onto the subspace \emph{M} as depicted in Figure
9.3.
\end{quote}

h

\begin{quote}
h\\
M
\end{quote}

Figure 9.3

of \emph{h} onto \emph{M}, which we denote ˆ\emph{m} = \emph{EMh}. The
mapping \emph{h →} ˆ\emph{m} is well defined and called the orthogonal
projection

\begin{quote}
Lemma 9.1.5. \emph{The mapping EM}: \emph{H → M is linear, i.e., for any
h}1\emph{, h}2 \emph{∈H and α, β ∈} R \emph{it holds that}\\
\emph{EM}(\emph{αh}1 + \emph{βh}2) = \emph{αEMh}1 + \emph{βEMh}2\emph{.}

\textbf{9.1.2. Orthogonal projection onto the space generated by a
random vector.} Let \emph{y} = {[}\emph{y}1\emph{, ..., ym}{]}\emph{T}be
a vector of random variables and let \emph{x} be a random variable.
Moreover, let {[}\emph{y}{]} be the space spanned by the components of
\emph{y}, i.e.,\\
{[}\emph{y}{]} ≜ span\emph{\{y}1\emph{, ..., ym\}.}

We shall now show how to compute the projection ˆ\emph{x} =
\emph{E}{[}\emph{y}{]}\emph{x}. An element matrix notation the normal
equations become 1\emph{kiyi} = \emph{ky}, where \emph{k} =
{[}\emph{k}1\emph{, ..., km}{]} and therefore we of {[}\emph{y}{]} has
the form∑\emph{m} seek a \emph{k} satisfying the normal equations
(\emph{x − ky, yj}) = 0 for \emph{j} = 1\emph{, ..., m}. In

\emph{k}\emph{Eymy}1 \emph{Ey}1\emph{y}1 \emph{·} \emph{Eymy}2
\emph{Ey}1\emph{y}2 \emph{·} \emph{·} \emph{·} \emph{·} \emph{Eymym}
\emph{Ey}1\emph{ym} \emph{·}  = {[}\emph{Exy}1\emph{, . . . ,
Exym}{]}\emph{.}

9.1. THE DISCRETE-TIME FILTER 85

It follows from the projection theorem that the system \emph{kEyyT}=
\emph{ExyT} always has a solution, and if \emph{EyyT}is invertible,
which happens if and only the random variables \emph{\{y}1\emph{, ...,
ym\}} are linearly independent, then\\
\emph{k} = \emph{ExyT}(\emph{EyyT})\emph{−}1and ˆ\emph{x} =
\emph{E}{[}\emph{y}{]}\emph{x} =
\emph{ExyT}(\emph{EyyT})\emph{−}1\emph{y.}
\end{quote}

If the random variables \emph{\{y}1\emph{, ..., ym\}} are linearly
dependent then ˆ\emph{x} is still unique but \emph{k} is not.

\begin{quote}
When deriving the Kalman filter we shall need to project vectors of
random variables component wise onto linear spaces, i.e., if \emph{x} =
{[}\emph{x}1\emph{, ..., xn}{]}\emph{T} we define \emph{EMx} as
\emph{EMx} ≜ {[}\emph{EMx}1\emph{, ..., EMxn}{]}\emph{T}. Moreover, we
say that \emph{x ∈ M} if \emph{xi ∈ M} for \emph{i} = 1\emph{, ...n} and
\emph{x ⊥ M} if the components of \emph{x} are orthogonal to \emph{M}.

When projecting the vector \emph{x} onto the space {[}\emph{y}{]} the
estimate has the form ˆ\emph{x} = \emph{E}{[}\emph{y}{]}\emph{x} =
\emph{Ky} where \emph{K} is a matrix. The normal equations can now be
written in matrix form as \emph{E}(\emph{x − Ky})\emph{yT}= 0 and if the
random variables \emph{\{y}1\emph{, ..., ym\}} are linearly independent
then \emph{K} = \emph{ExyT}{[}\emph{EyyT}{]}\emph{−}1. We state the
result as a lemma.

Lemma 9.1.6. \emph{Let x, y be random vectors and suppose that the
compo-nents of y are linearly independent. The linear least-squares
estimate} ˆ\emph{x of}
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x given y is}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
ˆ\emph{x} = \emph{ExyT}(\emph{EyyT})\emph{−}1\emph{y.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
The following lemmas are needed in the derivation of the Kalman filter.

Lemma 9.1.7. \emph{Let M be a subspace of the finite-dimensional
inner-product space H. Let x be a vector of elements xi ∈ H For any
matrix A such that the product Ax is well-defined it holds that}
\end{quote}

\emph{EMAx} = \emph{AEMx.}

\begin{quote}
Lemma 9.1.8. \emph{Let M, N be subspaces of the finite-dimensional
inner-product space H. If M ⊥ N then}\\
\emph{EM⊕N}= \emph{EM}+ \emph{EN.}

Remark 9.1.9. Can you find an example showing that if \emph{M} and
\emph{N} are not orthogonal then the preceding lemma is false? □

fundamental idea in the area of random processes is to extract the
genuinely \textbf{9.1.3. The innovation process.} Recall that
\emph{Ht−}1(\emph{y}) \emph{⊆ Ht}(\emph{y}). A

new information, with respect to \emph{Ht−}1(\emph{y}), contained in
\emph{y}(\emph{t}). To this end, let\\
˜\emph{y}(\emph{t}) ≜ \emph{y}(\emph{t}) \emph{−
EHt−}1(\emph{y})\emph{y}(\emph{t})\emph{.}

The vector ˜\emph{y}(\emph{t}) is by construction orthogonal to the
space \emph{Ht−}1(\emph{y}) and therefore we can perform an orthogonal
decomposition of \emph{Ht}(\emph{y}) as
\end{quote}

\emph{Ht}(\emph{y}) = \emph{Ht−}1(\emph{y}) \emph{⊕}
{[}˜\emph{y}(\emph{t}){]}\emph{.}

\begin{quote}
The stochastic vector ˜\emph{y}(\emph{t}) represents the new information
about the process \emph{y} obtained at time \emph{t}, and ˜\emph{y} is
therefore called the \emph{innovation process}

86 9. KALMAN FILTERING

of \emph{y}. The innovation process is by construction a white noise and
\emph{Ht}(\emph{y}) = \emph{Ht}(˜\emph{y}) (verify this). Defining the
covariance matrix
\end{quote}

\emph{R}(\emph{t}) ≜
\emph{E}˜\emph{y}(\emph{t})˜\emph{y}(\emph{t})\emph{T}

\begin{quote}
we see that if \emph{R}(\emph{t}) is positive definite for all \emph{t},
each component of \emph{y}(\emph{t}) carries new information in each
step, in that no linear combination of \emph{y}1(\emph{t})\emph{,
y}2(\emph{t})\emph{, . . . , ym}(\emph{t}) lies in the subspace
\emph{Ht−}1(\emph{y}) generated by previous observations. We say that
\emph{y} is \emph{purely nondeterministic} if it has this property.

\textbf{9.1.4. The Kalman recursions.} Returning to the original
problem, the linear-least squares estimate of \emph{x}(\emph{t}) based
on the observations \emph{Ht−}1(\emph{y}) can be written as
ˆ\emph{x}(\emph{t}) = \emph{EHt−}1(\emph{y})\emph{x}(\emph{t}). We now
derive explicit formulas for the estimator. In the following it will be
important to observe the various orthogonality relations that hold, e.g.
\emph{v}(\emph{t}) \emph{⊥ Ht}(\emph{y}) and \emph{w}(\emph{t}) \emph{⊥
Ht−}1(\emph{y}) (why?).

In order to simplify the notation we suppress the time dependence of the
matrices \emph{A, B, C, D} in the following computations.

The estimate ˆ\emph{x}(\emph{t} + 1) satisfies

ˆ\emph{x}(\emph{t} + 1) = \emph{EHt}(\emph{y})\emph{x}(\emph{t} + 1) =
\emph{EHt}(\emph{y}){[}\emph{Ax}(\emph{t}) + \emph{Bv}(\emph{t}){]} =
\emph{\{}Lemma 9.1.7\emph{\}} = =
\emph{AEHt}(\emph{y})\emph{x}(\emph{t}) +
\emph{BEHt}(\emph{y})\emph{v}(\emph{t}) = \emph{\{v}(\emph{t}) \emph{⊥
Ht}(\emph{y})\emph{\}} = \emph{AEHt}(\emph{y})\emph{x}(\emph{t})\emph{.}

Decompose \emph{Ht}(\emph{y}) as\\
\emph{Ht}(\emph{y}) = \emph{Ht−}1(\emph{y}) \emph{⊕}
{[}˜\emph{y}(\emph{t}){]}\\
and define ˆ\emph{xt}(\emph{t}) =
\emph{EHt}(\emph{y})\emph{x}(\emph{t}). Using Lemma 9.1.8 we have
\end{quote}

ˆ\emph{xt}(\emph{t}) = \emph{EHt−}1(\emph{y})\emph{x}(\emph{t}) +
\emph{E}{[}˜\emph{y}(\emph{t}){]}\emph{x}(\emph{t}) =
ˆ\emph{x}(\emph{t}) +
\emph{E}{[}˜\emph{y}(\emph{t}){]}\emph{x}(\emph{t}) =
ˆ\emph{x}(\emph{t}) + \emph{K}(\emph{t})˜\emph{y}(\emph{t})\emph{.}

\begin{quote}
where the time-varying matrix function \emph{K}(\emph{t}), called the
\emph{Kalman gain}, re-mains to be determined. Consequently,
\end{quote}

ˆ\emph{x}(\emph{t} + 1) = \emph{A}ˆ\emph{x}(\emph{t}) + \emph{A
E}{[}˜\emph{y}(\emph{t}){]}\emph{x}(\emph{t}) =
\emph{A}ˆ\emph{x}(\emph{t}) +
\emph{AK}(\emph{t})˜\emph{y}(\emph{t})\emph{.}

At time \emph{t}, the estimate ˆ\emph{x}(\emph{t}) and
\emph{y}(\emph{t}) are available and the innovation ˜\emph{y}(\emph{t})

\begin{quote}
is given as

˜\emph{y}(\emph{t}) = \emph{y}(\emph{t}) \emph{−
EHt−}1(\emph{y})\emph{y}(\emph{t})\\
= \emph{y}(\emph{t}) \emph{− CEHt−}1(\emph{y})\emph{x}(\emph{t}) \emph{−
DEHt−}1(\emph{y})\emph{w}(\emph{t})\\
= \emph{y}(\emph{t}) \emph{− C}ˆ\emph{x}(\emph{t})\emph{,}\\
where the last equality follows from the fact that \emph{w}(\emph{t})
\emph{⊥ Ht−}1(\emph{y}). Define the estimation error ˜\emph{x}(\emph{t})
≜ \emph{x}(\emph{t})\emph{−} ˆ\emph{x}(\emph{t}) and let
\emph{P}(\emph{t}) be the covariance matrix of ˜\emph{x}(\emph{t}),
i.e.,\\
\emph{P}(\emph{t}) ≜ \emph{E}
˜\emph{x}(\emph{t})˜\emph{x}(\emph{t})\emph{T.}

The innovation can now be written

(69) ˜\emph{y}(\emph{t}) = \emph{C}˜\emph{x}(\emph{t}) +
\emph{Dw}(\emph{t})\emph{.}

Note that the two terms in the right-hand side of (69) are orthogonal.

9.1. THE DISCRETE-TIME FILTER 87

We now determine the Kalman gain. Under the assumption that \emph{y} is
purely nondeterministic, Lemma 9.1.6 yields
\end{quote}

\emph{E}{[}˜\emph{y}(\emph{t}){]}\emph{x}(\emph{t}) = \emph{E
x}(\emph{t})˜\emph{y}(\emph{t})\emph{T}{[}\emph{E}
˜\emph{y}(\emph{t})˜\emph{y}(\emph{t})\emph{T}{]}\emph{−}1˜\emph{y}(\emph{t})\emph{.}

\begin{quote}
Using(69) we get
\end{quote}

\emph{R}(\emph{t}) =
\emph{E}˜\emph{y}(\emph{t})˜\emph{y}(\emph{t})\emph{T}=
\emph{CP}(\emph{t})\emph{CT}+ \emph{DRDT.}

\begin{quote}
Note that \emph{DRDT\textgreater{}} 0 is a \emph{sufficient} condition
for \emph{y} to be purely nondeter-ministic.

Moreover, we compute \emph{Ex}(\emph{t})˜\emph{y}(\emph{t})\emph{T}as

\emph{Ex}(\emph{t})˜\emph{y}(\emph{t})\emph{T}=
\emph{Ex}(\emph{t}){[}\emph{C}˜\emph{x}(\emph{t}) +
\emph{Dw}(\emph{t}){]}\emph{T}= \emph{\{w}(\emph{t}) \emph{⊥
x}(\emph{t})\emph{\}} = =
\emph{Ex}(\emph{t})˜\emph{x}(\emph{t})\emph{TCT}=
\emph{\{}ˆ\emph{x}(\emph{t}) \emph{⊥} ˜\emph{x}(\emph{t})\emph{\}} =
\emph{E}˜\emph{x}(\emph{t})˜\emph{x}(\emph{t})\emph{TCT}=
\emph{P}(\emph{t})\emph{CT.}

Hence, the Kalman gain is

(70) \emph{K}(\emph{t}) =
\emph{P}(\emph{t})\emph{CT}{[}\emph{CP}(\emph{t})\emph{CT}+
\emph{DRDT}{]}\emph{−}1\emph{.}

Let \emph{Pt}(\emph{t}) = \emph{E}(\emph{x}(\emph{t}) \emph{−}
ˆ\emph{xt}(\emph{t}))(\emph{x}(\emph{t}) \emph{−}
ˆ\emph{xt}(\emph{t}))\emph{T}. Then\\
\emph{Pt}(\emph{t}) = \emph{P}(\emph{t}) \emph{−
K}(\emph{t})\emph{CP}(\emph{t})\emph{,}\\
which implies that the covariance is always decreasing after a
measurement update.

Finally the state estimate ˆ\emph{x} is given by the recursive algorithm
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
(71)
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
ˆ\emph{x}(\emph{t} + 1)
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{A}ˆ\emph{x}(\emph{t}) + \emph{AK}(\emph{t})˜\emph{y}(\emph{t})
\end{quote}
\end{minipage} \\
& & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{A}ˆ\emph{x}(\emph{t}) + \emph{AK}(\emph{t}){[}\emph{y}(\emph{t})
\emph{− C}ˆ\emph{x}(\emph{t}){]}\emph{.}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
The recursion for ˆ\emph{x}(\emph{t}) can also be written

(72) ˆ\emph{x}(\emph{t} + 1) = {[}\emph{A −
AK}(\emph{t})\emph{C}{]}ˆ\emph{x}(\emph{t}) +
\emph{AK}(\emph{t})\emph{y}(\emph{t})\\
displaying the filter as a dynamical system driven by the observations
\emph{y}(\emph{t}). Note that in this form, the Kalman filter has the
same structure as the observer in Chapter 6.
\end{quote}

observations available the best estimate of \emph{x}(0) is the mean.
Letting \emph{H−}1(\emph{y}) ≜ \emph{\{}0\emph{\}} gives the initial
value ˆ\emph{x}(0) = 0, i.e., with no

\begin{quote}
Finally, in order to compute the Kalman gain \emph{K}(\emph{t}) we need
\emph{P}(\emph{t}). For-tunately, the sequence of matrices
\emph{P}(\emph{t}) can be recursively computed by a matrix-valued
difference equation. Subtracting (71) from the first of equa-tions (66)
and applying (69) we obtain
\end{quote}

˜\emph{x}(\emph{t} + 1) = {[}\emph{A −
AK}(\emph{t})\emph{C}{]}˜\emph{x}(\emph{t}) \emph{−
AK}(\emph{t})\emph{Dw}(\emph{t}) + \emph{Bv}(\emph{t})\emph{.}

\begin{quote}
The three terms at the right-hand side of the previous equation are
mutually orthogonal and it follows that\\
(73)\\
\emph{P}(\emph{t}+1) =
{[}\emph{A−AK}(\emph{t})\emph{C}{]}\emph{P}(\emph{t}){[}\emph{A−AK}(\emph{t})\emph{C}{]}\emph{T}+\emph{AK}(\emph{t})\emph{DRDTK}(\emph{t})\emph{TAT}+\emph{BQBT}

88 9. KALMAN FILTERING

which together with (70), after some manipulations, gives

(74)
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{P}(\emph{t} + 1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{AP}(\emph{t})\emph{AT−
AP}(\emph{t})\emph{CT}{[}\emph{CP}(\emph{t})\emph{CT}+
\emph{DRDT}{]}\emph{−}1\emph{CP}(\emph{t})\emph{AT}+ \emph{BQBT
P}0\emph{.}
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\emph{P}(0)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Despite the fact that this equation is not quadratic it is called the
\emph{discrete-time matrix Riccati equation} for reasons that will be
explained below. We summarize the results of this section in the
following theorem.

Theorem 9.1.10 (The Kalman filter). \emph{Given a linear stochastic
sys-tem} (66) \emph{having a purely nondeterministic output process y,
the linear least-squares estimate} ˆ\emph{x}(\emph{t}) \emph{of the
state x}(\emph{t}) \emph{given the observations \{y}(0)\emph{,
y}(1)\emph{, .., y}(\emph{t −} 1)\emph{\} is generated by the Kalman
filter} (72) \emph{where the gain K is deter-mined from} (70) \emph{and
the discrete matrix Riccati equation} (74)\emph{.}

Remark 9.1.11. We have so far shown that Kalman filter minimizes each
\emph{E∥xi}(\emph{t}) \emph{−} ˆ\emph{xi}(\emph{t})\emph{∥}2. We note
that Kalman filter even gives the minimum mean square error (MMSE)
solution, and also minimizes the covariance matrix \emph{Pt}(\emph{t}).

Let us assume that we use a general filter

ˆ\emph{xt}(\emph{t}) = ˆ\emph{x}(\emph{t}) \emph{−
L}(\emph{t})(\emph{y}(\emph{t}) \emph{− C}ˆ\emph{x}(\emph{t}))\\
= ˆ\emph{x}(\emph{t}) \emph{−} (\emph{K}(\emph{t}) +
˜\emph{K}(\emph{t}))(\emph{y}(\emph{t}) \emph{−
C}ˆ\emph{x}(\emph{t}))\emph{.}

Then,
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\emph{Pt}(\emph{t})
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{P}(\emph{t}) \emph{−
P}(\emph{t})\emph{CT}(\emph{CP}(\emph{t})\emph{CT}+
\emph{DRDT})\emph{−}1\emph{CP}(\emph{t})˜\emph{K}(\emph{CP}(\emph{t})\emph{CT}+
\emph{DRDT}) ˜\emph{KT.}
\end{quote}
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
+
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Clearly ˜\emph{K} = 0 gives the minimal covariance matrix. Since
\emph{E∥x}(\emph{t})\emph{−}ˆ\emph{xt}(\emph{t})\emph{∥}2=
\emph{tr}(\emph{Pt}(\emph{t})), it is also minimized.

Example 9.1.12 (Noisy measurements). Let \emph{Z} be an unknown scalar
quantity, modeled as a random variable with mean \emph{E Z} = \emph{−}2
and variance Var\emph{Z} = 0\emph{.}5.

Suppose we are repeatedly performing noisy measurements of \emph{Z} of
the form
\end{quote}

\emph{z}(\emph{t}) = \emph{Z} + \emph{σw}(\emph{t})\emph{,}

\begin{quote}
where \emph{\{w}(\emph{t})\emph{\}} is a scalar normalized white-noise
sequence. At each instant we would like to make the linear least-squares
estimate of \emph{Z} based on the measurements made so far.

This problem can be embedded in the Kalman filtering set-up in the
following way. First define the dynamical system \emph{x}(\emph{t} + 1)
= \emph{x}(\emph{t})\emph{, x}(0) = \emph{Z} + 2 in order to get a zero
mean random process to be estimated. This gives \emph{E x}(0) = 0 and
\emph{E x}(0)2= 0\emph{.}5. Then define the observation process as
\emph{y}(\emph{t}) = \emph{x}(\emph{t}) + \emph{σw}(\emph{t}).
Practically, \emph{y}(\emph{t}) is obtained as \emph{y}(\emph{t}) =
\emph{z}(\emph{t}) \emph{−} 2.

9.1. THE DISCRETE-TIME FILTER 89

We now have the standard setting as in (66), with \emph{A} = 1, \emph{B}
= 0, \emph{C} = 1 and \emph{D} = \emph{σ}. The best estimate of
\emph{x}(\emph{t}) based on \emph{\{y}(0)\emph{, y}(1)\emph{, ...,
y}(\emph{t −} 1)\emph{\}} is delivered by the dynamical system
\end{quote}

ˆ\emph{x}(\emph{t} + 1) = ˆ\emph{x}(\emph{t}) +
\emph{k}(\emph{t}){[}\emph{y}(\emph{t}) \emph{−}
ˆ\emph{x}(\emph{t}){]}\emph{,} ˆ\emph{x}(0) = 0\emph{.}

The Riccati equation for \emph{p}(\emph{t}), which in this example is a
scalar quantity, can

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
be written
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{p}(\emph{t} + 1) =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{p}(\emph{t})\emph{σ}2\\
\emph{p}(\emph{t}) + \emph{σ}2 \emph{, p}(0) = 0\emph{.}5
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
and the Kalman gain \emph{k}(\emph{t}) is given by
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{k}(\emph{t}) =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{p}(\emph{t})\\
\emph{p}(\emph{t}) + \emph{σ}2 \emph{.}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
The asymptotic behavior of \emph{p}(\emph{t}) can easily be analyzed. We
first look for equilibrium solutions to the Riccati equation
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{p} =
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{pσ}2\\
\emph{p} + \emph{σ}2 \emph{,}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

with the only solution \emph{p} = 0. Convergence can be established by
differenti-

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{1.0000} + 4\tabcolsep}@{}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
ating the function
\end{quote}
\end{minipage}} \\
\midrule()
\endhead
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6667} + 2\tabcolsep}}{%
\emph{f}(\emph{p}) ≜} & \begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
\emph{pσ}2\\
\emph{p} + \emph{σ}2 \emph{.}
\end{quote}\strut
\end{minipage} \\
\multirow{2}{*}{The derivative is} & \emph{f′}(\emph{p}) = &
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
1
\end{quote}
\end{minipage} \\
& & (1 +\emph{p σ}2 )2 \\
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{1.0000} + 4\tabcolsep}@{}}{%
and satisfies \emph{\textbar f′}(\emph{p})\emph{\textbar{} \textless{}}
1 for \emph{p ≥} 0. Consequently, the gain \emph{k}(\emph{t}) tends to
zero, which is in line with the intuition that further measurements
should be less} \\
\bottomrule()
\end{longtable}

\begin{quote}
taken into account as time proceeds.

Example 9.1.13 (A scalar problem). Consider the system \{
\emph{x}(\emph{t} + 1) =\uline{1}

\emph{y}(\emph{t}) = \emph{x}(\emph{t}) + \emph{d w}(\emph{t})\emph{,}\\
2\emph{x}(\emph{t}) + \emph{b v}(\emph{t})

where \emph{x}(\emph{t})\emph{, v}(\emph{t})\emph{, w}(\emph{t}) and
\emph{y}(\emph{t}) are scalars. Moreover, let \emph{E x}(0) = 0 and
\emph{E x}(0)2= \emph{σ}2.

The Riccati equation is, by (74),

\emph{p}(\emph{t} + 1) = 1 4\emph{p}(\emph{t}) \emph{−} 1
4\emph{p}(\emph{t})2(\emph{p}(\emph{t}) + \emph{d}2)\emph{−}1 +
\emph{b}2\emph{.} By simplification we get
\end{quote}

\emph{p}(\emph{t} + 1) = 1 4 \emph{p}(\emph{t}) + \emph{d}2 +
\emph{b}2\emph{. p}(\emph{t})\emph{d}2

In order to study the asymptotic behavior of the Riccati equation we
define

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
the function
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\emph{f}(\emph{p}) ≜1 4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{pd}2\\
\emph{p} + \emph{d}2 + \emph{b}2
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
90 9. KALMAN FILTERING

and look for solutions to \emph{f}(\emph{p}) = \emph{p}. quadratic
equation

After rearranging terms we get the
\end{quote}

\emph{p}2+ (3 4\emph{d}2 \emph{− b}2)\emph{p −} (\emph{bd})2 = 0\emph{.}

\begin{quote}
Since \emph{−}(\emph{bd})2\emph{\textless{}} 0, we conclude that the
roots are real and have opposite signs. Hence, there is a strictly
positive stationary solution \emph{p} to the Riccati equation. The
question of convergence can be settled by differentiating
\emph{f}(\emph{p}),
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{f′}(\emph{p}) = 1\\
4
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
1\\
(1 +\emph{p d}2 )2 \emph{.}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
Since \emph{\textbar f′}(\emph{p})\emph{\textbar{} \textless{}} 1 for
\emph{p ≥} 0, we conclude that the Riccati equation con-verges to the
positive positive stationary solution. Moreover, the iteration

converges to the positive stationary solution for all initial values
\emph{p}(0) \emph{≥} 0, indicating that sometimes in Kalman filtering
problems exact knowledge of} \\
\bottomrule()
\end{longtable}

\begin{quote}
\emph{E x}(0)\emph{x}(0)\emph{T}might not be so important.

Example 9.1.14 (A tracking problem). An important area of applica-tion
for Kalman filtering techniques is that of tracking systems, such as
systems for air traffic control.

As a simple example, we shall consider the problem of tracking a
par-ticle performing one-dimensional motion, i.e. ¨\emph{y}(\emph{t}) =
\emph{f}(\emph{t}), where \emph{y}(\emph{t}) is the position of the
particle and \emph{f}(\emph{t}) is the applied force. The tracking
system performs noisy measurements of \emph{y}(\emph{t}), but
\emph{f}(\emph{t}) is unknown to the system and is modeled as a random
process.

Suppose that the tracking system is working in discrete time with time
step \emph{h}. If \emph{h} is very small compared to the time constants
of the objects being tracked it is reasonable, from the tracking
system's point of view, to assume that the input \emph{f}(\emph{t}) is a
piecewise constant signal \emph{u}(\emph{k}), i.e. \emph{f}(\emph{t}) =
\emph{u}(\emph{k}) if \emph{t ∈} {[}\emph{kh, kh} + \emph{h}). Hence, by
sampling a state space representation of ¨\emph{y}(\emph{t}) =
\emph{f}(\emph{t}), as in Example 2.3.1, we get the discrete-time system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 16\tabcolsep) * \real{0.1111}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(75)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[} \emph{x}1(\emph{k} + 1)

\emph{x}2(\emph{k} + 1) {]}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
{[} 0 1
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{h} 1 {]} {[} \emph{x}1(\emph{k}) \emph{x}2(\emph{k}) {]}
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
+
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{[} \emph{\uline{h}}2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
{]}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u}(\emph{k})\emph{,}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{x}1(\emph{k}) is the position of the particle at \emph{kh}
and \emph{x}2(\emph{k}) its velocity.

The unknown input \emph{u}(\emph{k}) is now modeled as a white noise
sequence with zero mean and variance \emph{σ}2, where \emph{σ}2is chosen
to reflect the physical performance of the objects being tracked.
Moreover, we assume that the tracking system performs observations of
the position of the particle at the instants \emph{kh} corrupted by
additive noise, i.e.

where \emph{w}(\emph{t}) is a normalized white noise sequence. {[}1
0{]}\\
\emph{y}(\emph{k}) = \emph{x}(\emph{k}) + \emph{d w}(\emph{t})\emph{,}

9.1. THE DISCRETE-TIME FILTER 91

We now have the setting as in (66) with

If we let \emph{h} = 0\emph{.}5, \emph{σ} = 1 and \emph{d} = 0\emph{.}1
and iterate the Riccati equation \emph{A} = {[} 0 1 {]} \emph{, B} = {[}
\emph{h}2 {]} \emph{σ, C} = {[}1 0{]} and \emph{D} = \emph{d.}\\
1 \emph{h}

with the initial value \emph{P}(0) = \emph{I}, we reach the stationary
value

\emph{P∞} = {[} 0\emph{.}2020 0\emph{.}2756 {]} \emph{\textgreater{}}
0\\
0\emph{.}1532 0\emph{.}2020

after a few iterations.

Actually, the general theory of the discrete-time Riccati equation
gives, under some natural conditions, that the Riccati equation
converges to a stationary value \emph{P∞ \textgreater{}} 0 independent
of the positive semi-definite initial value \emph{P}(0). Moreover, the
corresponding Kalman gain \emph{K∞} stabilizes the estimator, i.e. the
matrix \emph{A − AK∞C} has its eigenvalues in the open unit disc.

We now verify stability of the estimator in our numerical example. In
stationarity we have

\emph{AK∞} = {[} 1\emph{.}2378 {]}\\
1\emph{.}5576

and the closed-loop system matrix \emph{A − AK∞C} has eigenvalues
\emph{\{}0\emph{.}22 + \emph{i} 0\emph{.}11\emph{,} 0\emph{.}22 \emph{−
i} 0\emph{.}11\emph{\}} with magnitude 0\emph{.}25 so the estimator\\
ˆ\emph{x}(\emph{k} + 1) = (\emph{A − AK∞C})ˆ\emph{x}(\emph{k}) +
\emph{AK∞y}(\emph{k})\\
is indeed an input/output stable system.

If we reduce the observation noise by letting \emph{d} = 0\emph{.}05 the
stationary Kalman gain increases to

\emph{AK∞} = {[} 1\emph{.}2974 {]} \emph{.}\\
1\emph{.}6326
\end{quote}

This is in accordance with the intuition that the innovations
\emph{y}(\emph{k}) \emph{− C}ˆ\emph{x}(\emph{k}) should be trusted more
when there is less observation noise.

\begin{quote}
\textbf{9.1.5. Kalman filter and classical parameter estimation.}
Con-sider the problem of estimating parameter \emph{x} from the
observations
\end{quote}

\emph{y} = \emph{Cx} + \emph{v,}

\begin{quote}
where \emph{E\{vvT\}} = \emph{V} . We wish to find the \emph{linear,
unbiased, minimum variance} estimator ˆ\emph{x∗}. Namely, in the class
of ˆ\emph{x} = \emph{Ky,} and \emph{E\{}ˆ\emph{x\}} = \emph{E\{x\}}, we
have \emph{E\{}(\emph{x −} ˆ\emph{x∗})\emph{T}(\emph{x −}
ˆ\emph{x∗})\emph{\} →} min \emph{.}

The classical Gauss-Markov theorem tells us

ˆ\emph{x∗}= \emph{I−}1\emph{CTV−}1\emph{y,}\\
where \emph{I} = \emph{CTV−}1\emph{C} is called the \emph{information
matrix}. Now an interesting question is how this compares with Kalman
filter:
\end{quote}

ˆ\emph{x}(\emph{t} + 1) = ˆ\emph{x}(\emph{t}) +
\emph{P}(\emph{t})\emph{CT}(\emph{CP}(\emph{t})\emph{CT}+ \emph{V}
)\emph{−}1(\emph{y − C}ˆ\emph{x}(\emph{t}))\emph{.}

\begin{quote}
92 9. KALMAN FILTERING
\end{quote}

We can view ˆ\emph{x}(\emph{t}) and \emph{P}(\emph{t}) as the priori
information we have on \emph{x}. Rewrite

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
ˆ\emph{x}(\emph{t} + 1)
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{P}(\emph{t})\emph{CT}(\emph{CP}(\emph{t})\emph{CT}+ \emph{V}
)\emph{−}1\emph{y} + {[}\emph{I −
P}(\emph{t})\emph{CT}(\emph{CP}(\emph{t})\emph{CT}+ \emph{V}
)\emph{−}1\emph{C}{]}ˆ\emph{x}(\emph{t}) {[}\emph{P}(\emph{t})\emph{−}1+
\emph{I}{]}\emph{−}1\emph{CTV−}1\emph{y} +
{[}\emph{P}(\emph{t})\emph{−}1+
\emph{I}{]}\emph{−}1\emph{P}(\emph{t})\emph{−}1ˆ\emph{x}(\emph{t})\emph{.}
\end{quote}
\end{minipage}} \\
& \begin{minipage}[b]{\linewidth}\raggedright
=
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Here we have used the equalities
\end{quote}

\emph{PCT}{[}\emph{CPCT}+ \emph{V} {]}\emph{−}1= {[}\emph{I} +
\emph{PV−}1\emph{C}{]}\emph{−}1\emph{PCTV−}1

\begin{quote}
and\\
\emph{I − PCT}(\emph{CPCT}+ \emph{V} )\emph{−}1\emph{C} = {[}\emph{I} +
\emph{PCTV−}1\emph{C}{]}\emph{−}1\emph{.}

\textbf{Conclusion:} When \emph{P−}1 0 = 0, Kalman filter is the same as
Gauss-Markov estimation!

\textbf{9.1.6. Duality between estimation and control.} Just as there is
a duality between reachability and observability (see Section 3.7,
Chapter 3), there is a duality between control and estimation. This
should not be sur-prising since in fact estimation is a problem of
observation. We now proceed to demonstrate this principle of duality for
the Kalman filter.

To this end, form the adjoint
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(76)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{z}(\emph{t}) = \emph{A}(\emph{t})\emph{Tz}(\emph{t} + 1) +
\emph{C}(\emph{t})\emph{Tu}(\emph{t} + 1);
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{z}(\emph{T}) = \emph{a}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
of the system (66), formed as usual by reversing time and transposing
sys-tems matrices. In analogy to the corresponding construction in
Chapter 3, we form the scalar product \emph{zTx} and take differences,
i.e.

\emph{z}(\emph{t} + 1)\emph{Tx}(\emph{t} + 1) \emph{−
z}(\emph{t})\emph{Tx}(\emph{t})\\
=\emph{z}(\emph{t} + 1)\emph{TA}(\emph{t})\emph{x}(\emph{t}) +
\emph{z}(\emph{t} + 1)\emph{TB}(\emph{t})\emph{v}(\emph{t}) \emph{−
z}(\emph{t} + 1)\emph{TA}(\emph{t})\emph{x}(\emph{t}) \emph{−
u}(\emph{t} + 1)\emph{TC}(\emph{t})\emph{x}(\emph{t}) =\emph{z}(\emph{t}
+ 1)\emph{TB}(\emph{t})\emph{v}(\emph{t}) + \emph{u}(\emph{t} +
1)\emph{TD}(\emph{t})\emph{w}(\emph{t}) \emph{− u}(\emph{t} +
1)\emph{Ty}(\emph{t})\\
where we have also used the second of equations (66). Summing this from
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\multicolumn{3}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.7500} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{t} = 0 to \emph{t} = \emph{T −} 1, we obtain
\end{quote}
\end{minipage}} &
\multirow{3}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{u}(\emph{t}+1)\emph{Ty}(\emph{t})
\end{quote}
\end{minipage}} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5000} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{aTx}(\emph{T})\emph{−z}(0)\emph{Tx}0 = and consequently
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{T−}1
\emph{T−}1∑{[}\emph{z}(\emph{t}+1)\emph{TB}(\emph{t})\emph{v}(\emph{t})+\emph{u}(\emph{t}+1)\emph{TD}(\emph{t})\emph{w}(\emph{t}){]}\emph{−}∑
\end{quote}
\end{minipage} \\
\begin{minipage}[b]{\linewidth}\raggedright
(77)

where

(78)
\end{minipage} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.5000} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\emph{T−}1

\begin{quote}
\emph{E\{}{[}\emph{aTx}(\emph{T}) +∑\emph{u}(\emph{t} +
1)\emph{Ty}(\emph{t}){]}2\emph{\}}
\end{quote}

\emph{T}

\begin{quote}
=\emph{z}(0)\emph{TP}0\emph{z}(0)
+∑{[}\emph{z}(\emph{s})\emph{TQ}(\emph{s})\emph{z}(\emph{s}) +
\emph{u}(\emph{s})\emph{TR}(\emph{s})\emph{u}(\emph{s}){]}

\{ \emph{Q}(\emph{t}) = \emph{B}(\emph{t −} 1)\emph{B}(\emph{t −}
1)\emph{T}
\end{quote}

\emph{R}(\emph{t}) = \emph{D}(\emph{t −} 1)\emph{D}(\emph{t −}
1)\emph{T}
\end{minipage}} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
9.2. THE CONTINUOUS-TIME KALMAN FILTER 93

Recall now that finding a vector sequence \emph{\{u}(1)\emph{,
u}(2)\emph{, . . . , u}(\emph{T})\emph{\}} so that (77) is minimized
amounts to determining the least squares estimate of
\emph{aTx}(\emph{T}) given the data
\end{quote}

\emph{\{y}1(0)\emph{, . . . , ym}(0)\emph{, y}1(1)\emph{, . . . ,
ym}(1)\emph{, . . . , y}1(\emph{T −} 1)\emph{, . . . , ym}(\emph{T −}
1)\emph{\}.}

\begin{quote}
In other words,\\
\emph{T−}1\\
\emph{aT}ˆ\emph{x}(\emph{T}) = \emph{−}∑\emph{u∗}(\emph{t} +
1)\emph{Ty}(\emph{t})

where \emph{u∗}is the optimal choice of \emph{u}.

On the other hand, \emph{u∗}is also the optimal control minimizing the
qua-dratic cost criterion in the second member of (77) under the
dynamics of system (76). This is a \emph{linear-quadratic regulator
problem} of the type dis-cussed in Chapter 7 in continuous time, one
difference being that time has been reversed and matrices transposed. It
can be shown that this control problems has an optimal feedback solution
\end{quote}

\emph{u}(\emph{t}) = \emph{K}(\emph{t})\emph{Tx}(\emph{t})

\begin{quote}
just as in continuous time, and that \emph{K} is precisely the Kalman
gain. Con-sequently this linear-quadratic regulator problem and the
Kalman filtering problem have dual discrete-time matrix Riccati
equations. Since we have developed linear-quadratic optimal control in
continuous time in Chapter 7 and 8, we shall pursuit this line of
investigation further in the context of continuous-time Kalman
filtering.
\end{quote}

\textbf{9.2. The continuous-time Kalman filter}

\begin{quote}
The continuous-time counterpart of the linear stochastic system (66)
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
may be written
\end{minipage} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\{˙\emph{x}(\emph{t}) = \emph{A}(\emph{t})\emph{x}(\emph{t}) +
\emph{B}(\emph{t})\emph{v}(\emph{t});

\begin{quote}
\emph{y}(\emph{t}) = \emph{C}(\emph{t})\emph{x}(\emph{t}) +
\emph{D}(\emph{t})\emph{w}(\emph{t})
\end{quote}
\end{minipage}} &
\multirow{2}{*}{\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{x}(0) = \emph{x}0
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
(79)
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{x}0, \emph{v} and \emph{w} are centered and pairwise
uncorrelated, and where \emph{v} and \emph{w} are white noise processes,
i.e.

where \emph{δ} is the \emph{Dirac delta function} having the property
\emph{E} \{{[} \emph{v}(\emph{t}) {]}{[}
\emph{v}(\emph{s})\emph{T,w}(\emph{s})\emph{T} {]}\} = {[} \emph{Q} 0
{]} \emph{δ}(\emph{t − s})
\end{quote}

∫ \emph{∞−∞f}(\emph{t})\emph{δ}(\emph{t})\emph{dt} = \emph{f}(0)\emph{.}

\begin{quote}
As should be well-known this is not a function but a generalized
function, and therefore we enter into a somewhat grey area when it comes
to con-sidering the stochastic system (79). In fact, \emph{v}, \emph{w}
and \emph{y} must be regarded as \emph{generalized stochastic processes}
for which there is a literature. However, have no fear! As long as we
handle the Dirac function appropriately (as we have been taught) and
stay out of nonlinear filtering (as we shall) we are
\end{quote}

\includegraphics[width=2.05556in,height=0.36111in]{vertopal_af54be8d710748c99ad64e50c092cb81/media/image23.png}

\begin{quote}
94 9. KALMAN FILTERING

OK. Furthermore, we set \emph{P}0 ≜ \emph{E\{x}0\emph{xT} all \emph{t}.
0\emph{\}} and assume that \emph{DRDT \textgreater{}} 0 for

The problem at hand, now as before, is to find a linear state estimator
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
y
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Filter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
x
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
i.e., in external systems description,
\end{quote}

ˆ\emph{x}(\emph{t}) =∫ \emph{t G}(\emph{t,
s})\emph{y}(\emph{s})\emph{ds,}

\begin{quote}
where \emph{G} takes values in R\emph{n×m}, such that

\emph{E}{[}(\emph{x}(\emph{t}) \emph{−}
ˆ\emph{x}(\emph{t}){]})(\emph{x}(\emph{t}) \emph{−}
ˆ\emph{x}(\emph{t}){]})\emph{T}{]}\\
is minimized. The most natural way of derivation is perhaps to extend
the discrete-time case by formally letting the sampling time tend to
zero. We will also discuss an alternative approach by proceeding along
the lines of Section 9.1.6.

\textbf{9.2.1. Continuous time Kalman filter derived from the discrete
case.} A key issue in the extension to the continuous case is how to
han-dle the noise. Heuristically (we do not intend to be very
rigorous!), we can understand the white noises as the derivatives of
some \emph{Brownian mo-tion} (although this derivative does not exist in
a conventional sense). For example,\\
∫ \emph{t}\\
\emph{w}(\emph{r})\emph{dr} = \emph{β}(\emph{t}) \emph{−
β}(\emph{s})\emph{,} where
\end{quote}

\emph{E}(\emph{β}(\emph{t}) \emph{− β}(\emph{s})) = 0\emph{,
E}(\emph{β}(\emph{t}) \emph{− β}(\emph{s}))(\emph{β}(\emph{t}) \emph{−
β}(\emph{s}))\emph{T}= \emph{R}(\emph{t − s})\emph{, t \textgreater{}
s.}

\begin{quote}
Thus,\\
∫ \emph{t Rdr} = \emph{E}∫ \emph{t w}(\emph{r})\emph{dr}∫ \emph{t
wT}(\emph{τ})\emph{dτ.}

Then,\\
∫ \emph{t} (∫ \emph{t E\{w}(\emph{r})\emph{wT}(\emph{τ})\emph{dτ −
R\}})\emph{dr} = 0\emph{.}

Since this is true for any interval, we have\\
∫ \emph{t E\{w}(\emph{r})\emph{wT}(\emph{τ})\emph{\}dτ} = \emph{R, ∀r ∈}
{[}\emph{t, s}{]}\emph{.}

Thus,\\
\emph{E\{w}(\emph{r})\emph{wT}(\emph{τ})\emph{\}} = \emph{Rδ}(\emph{r −
τ})\emph{.}

We can derive \emph{E\{w}(\emph{t})\emph{\}} = 0 similarly. Now we use
the discrete Kalman filter to derive the continuous one by letting
∆\emph{t →} 0.

9.2. THE CONTINUOUS-TIME KALMAN FILTER 95

Let \emph{x}(\emph{t} + 1) = \emph{x}(\emph{t} + ∆\emph{t}), when
∆\emph{t} is very small, we have (``\emph{≈}'' means equal up to
\emph{O}(∆\emph{t}2))\\
\emph{Ad} = \emph{eA}∆\emph{t≈ I} + \emph{A}∆\emph{t, Cd} = \emph{C.}

Since \emph{vd}(\emph{t}) =∫ \emph{t}+∆\emph{t
eA}(\emph{t}+∆\emph{t−s})\emph{v}(\emph{s})\emph{ds ≈ β}(\emph{t} +
∆\emph{t}) \emph{− β}(\emph{t}), \emph{Qd ≈ Q}∆\emph{t, Rd ≈
R/}∆\emph{t.}\\
Then, \emph{Kd}(\emph{t}) \emph{≈
P}(\emph{t})\emph{CT}(\emph{DRDT})\emph{−}1∆\emph{t}. We have\\
ˆ\emph{x}(\emph{t} + 1) \emph{≈} (\emph{I} +
\emph{A}∆\emph{t})(ˆ\emph{x}(\emph{t}) +
\emph{Kd}(\emph{t})(\emph{y}(\emph{t}) \emph{−
C}ˆ\emph{x}(\emph{t}))\emph{,} or,

ˆ\emph{x}(\emph{t} + 1) \emph{−} ˆ\emph{x}(\emph{t}) \emph{≈
A}ˆ\emph{x}(\emph{t})∆\emph{t} +
\emph{P}(\emph{t})\emph{CT}(\emph{DRDT})\emph{−}1(\emph{y}(\emph{t})
\emph{− C}ˆ\emph{x}(\emph{t}))∆\emph{t.} Thus, by dividing both sides
with ∆\emph{t} and taking the limit, we have

˙ˆ\emph{x}(\emph{t}) = \emph{A}ˆ\emph{x}(\emph{t}) +
\emph{K}(\emph{t})(\emph{y}(\emph{t}) \emph{−
C}ˆ\emph{x}(\emph{t}))\emph{,} where \emph{K}(\emph{t}) =
\emph{P}(\emph{t})\emph{CT}(\emph{DRDT})\emph{−}1.

Since
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\emph{≈}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{P}(\emph{t} + ∆\emph{t}) \emph{≈} (\emph{I} +
\emph{A}∆\emph{t})(\emph{I − KdC})\emph{P}(\emph{t})(\emph{I} +
\emph{A}∆\emph{t})\emph{T}+ \emph{BQ}∆\emph{tBT}\\
\emph{P}(\emph{t}) + (\emph{AP}(\emph{t}) +
\emph{P}(\emph{t})\emph{AT})∆\emph{t −
P}(\emph{t})\emph{CT}(\emph{DRDT})\emph{−}1\emph{CP}(\emph{t})∆\emph{t}
+ \emph{BQBT}∆\emph{t}
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
Similarly, we obtain

˙\emph{P}(\emph{t}) = \emph{AP}(\emph{t}) + \emph{P}(\emph{t})\emph{AT−
P}(\emph{t})\emph{CT}(\emph{DRDT})\emph{−}1\emph{CP}(\emph{t}) +
\emph{BQBT,} and we assume \emph{P}(0) = \emph{P}0 is known.

The Riccati equation looks very similar to that we studied in Chapter 7,
thus it is interesting to find out the duality between optimal control
and optimal filtering.

\textbf{9.2.2. The dual control problem.} Define the dual control system

(80) ˙\emph{z}(\emph{t}) = \emph{−A}(\emph{t})\emph{Tz}(\emph{t}) +
\emph{C}(\emph{t})\emph{Tu}(\emph{t}); \emph{z}(\emph{T}) = \emph{a}

evolving backward in time on the interval {[}0\emph{, T}{]} and form as
before

\emph{d}\\
\emph{dt}(\emph{zT x}) = \emph{zT} ˙\emph{x} + ˙\emph{zT x}\\
= \emph{zTAx} + \emph{zTBv − zTAx} + \emph{uTCx}\\
= \emph{zTBv − uTDw} + \emph{uTy}\\
where (79) has been needed. Integrating this over the interval
{[}0\emph{, T}{]}, we obtain

\emph{aTx}(\emph{T}) \emph{− z}(0)\emph{Tx}0 =∫ \emph{T} (\emph{zTBv −
uTDw})\emph{dt} +∫ \emph{T} \emph{uTydt}

and consequently

(81) \emph{E\{}{[}\emph{aTx}(\emph{T}) \emph{−}∫ \emph{T}
\emph{uTydt}{]}2= \emph{z}(0)\emph{TP}0\emph{z}(0) +∫ \emph{T}
(\emph{zT} ˜\emph{Qz} + \emph{uT} ˜\emph{Ru})\emph{dt}

96 9. KALMAN FILTERING

where ˜\emph{Q} ≜ \emph{BQBT}and ˜\emph{R} ≜ \emph{DRDT\textgreater{}}
0. Therefore the linear least-squares problem is equivalent to
minimizing the quadratic cost criterion in the second member of (81)
subject to the constraint of the systems equations (80).

Modulo time reversal, this is a linear-quadratic regulator problem of
the type discussed in Chapter 7, and it has a feedback solution

(82) \emph{u}(\emph{t}) = \emph{K}(\emph{t})\emph{Tz}(\emph{t})

where

(83) \emph{K} = \emph{PCT} ˜\emph{R−}1

where \emph{P} is the solution of the matrix Riccati equation

(84) \{ ˙\emph{P} = \emph{AP} + \emph{PAT − PCT} ˜\emph{R−}1\emph{CP} +
˜\emph{Q}

To see this, apply the time reversal operation \emph{t → T −t} to the
present linear-quadratic problem (changing the sign of all derivatives)
and then apply the results of Chapter 7. Then reverse time again to
obtain (82)-(84).

The optimal control law (82) yields the closed-loop system
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(85)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
˙\emph{z}(\emph{t}) = \emph{−}{[}\emph{A}(\emph{t}) \emph{−
K}(\emph{t})\emph{C}(\emph{t}){]}\emph{Tz}(\emph{t});
\end{quote}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{z}(\emph{T}) = \emph{a}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
i.e., as follows from property (3) in Chapter 2, \emph{z}(\emph{t}) =
Ψ(\emph{T, s})\emph{Ta}, where Ψ is the transition matrix
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(86)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{∂}Ψ\\
\emph{∂t}(\emph{t, s}) = {[}\emph{A}(\emph{t}) \emph{−
K}(\emph{t})\emph{C}(\emph{t}){]}Ψ(\emph{t, s});
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Ψ(\emph{s, s}) = \emph{I}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
and consequently the optimal open-loop control is

(87) \emph{u∗}(\emph{t}) = \emph{K}(\emph{t})\emph{T}Ψ(\emph{T,
t})\emph{Ta}

This will be used to derive the Kalman filter.

\textbf{9.2.3. The Kalman filter revisited.} The optimal control
\emph{u∗}of the linear-quadratic control problem (80)-(81) also provides
us with the best linear least-squares estimates\\
∫ \emph{T} \emph{u∗}(\emph{s})\emph{Ty}(\emph{s})\emph{ds}

of \emph{aTx}(\emph{T}) given the data \emph{\{y}(\emph{t}); \emph{t ∈}
{[}0\emph{, T}{]}\emph{\}}. Inserting (87), we obtain ∫ \emph{T}
\emph{u∗}(\emph{s})\emph{Ty}(\emph{s})\emph{ds} = \emph{aT}∫ \emph{T}
Ψ(\emph{T, s})ˆ\emph{y}(\emph{s})\emph{ds}
\end{quote}

Since this holds for an arbitrary \emph{a ∈} R\emph{n}and an arbitrary
\emph{T \textgreater{}} 0, we must have

ˆ\emph{x}(\emph{t}) =∫ \emph{t} Ψ(\emph{t,
s})ˆ\emph{y}(\emph{s})\emph{ds}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
9.2. THE CONTINUOUS-TIME KALMAN FILTER
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
97
\end{minipage} \\
\midrule()
\endhead
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{1.0000} + 2\tabcolsep}@{}}{%
\begin{minipage}[t]{\linewidth}\raggedright
\begin{quote}
which, in view of (86), satisfies

(88)
\end{quote}

Another way of writing this, which exhibits the way in which the state

ˆ\emph{x}(0) = 0

\emph{dt}= {[}\emph{A}(\emph{t}) \emph{−
K}(\emph{t})\emph{C}(\emph{t}){]}ˆ\emph{x}(\emph{t}) +
\emph{K}(\emph{t})\emph{y}(\emph{t})

\emph{d}ˆ\emph{x}

\begin{quote}
estimate ˆ\emph{x} is updated, is as follows.

(89)

In fact, just as in discrete time, we can show that
\end{quote}

ˆ\emph{x}(0) = 0

\emph{dt}= \emph{A}(\emph{t})ˆ\emph{x}(\emph{t}) +
\emph{K}(\emph{t}){[}\emph{y}(\emph{t}) \emph{−
C}(\emph{t})ˆ\emph{x}(\emph{t}){]}

\emph{d}ˆ\emph{x}
\end{minipage}} \\
\bottomrule()
\end{longtable}

\begin{quote}
˜\emph{y}(\emph{t}) = \emph{y}(\emph{t}) \emph{−
C}(\emph{t})ˆ\emph{x}(\emph{t})\\
is a white noise process, and it is called the innovation process. The
filter (89) together with the matrix Riccati equation (84) determining
the gain (83) is known as the \emph{Kalman} (or the \emph{Kalman-Bucy})
\emph{filter}.

Another way to introduce the Kalman filter is as follows. Suppose that
we define the process ˆ\emph{x} by means of (88) without connecting it
to the optimal estimation problem and then form

\emph{d}\\
\emph{dt}(\emph{zT} ˆ\emph{x}) = \emph{zT} ˙ˆ\emph{x} + ˙\emph{zT}
ˆ\emph{x}\\
= \emph{zT}(\emph{A − KC})ˆ\emph{x} + \emph{zTKy − zT}(\emph{A −
KC})ˆ\emph{x} = \emph{uTy}

where we have used (85) and (82). Integrating this we obtain
\end{quote}

\emph{aT}ˆ\emph{x}(\emph{t}) =∫ \emph{t
u}(\emph{s})\emph{Ty}(\emph{s})\emph{ds}

\begin{quote}
which is the Kalman estimate if \emph{K}, and hence \emph{u}, is chosen
in an optimal fashion.

\textbf{9.2.4. The steady-state Kalman filter.} Consider a
time-invariant
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
stochastic system
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\{˙\emph{x} = \emph{Ax} + \emph{Bv}

\emph{y} = \emph{Cx} + \emph{Dw}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{A}, \emph{B}, \emph{C} and \emph{D} are constant, and
suppose that (\emph{A, B}) is completely reachable and (\emph{C, A})
completely observable. Let us consider what happens with the Kalman
filter when the interval of observation becomes large.

It follows from the theory developed in Chapter 8 that
\emph{P}(\emph{t}) tends to a limit \emph{P} as \emph{t → ∞}, where
\emph{P} is the unique positive definite symmetric solution of the
algebraic Riccati equation

(90) \emph{AP} + \emph{PAT− PCT}(\emph{DRDT})\emph{−}1\emph{CP} +
\emph{BQBT}= 0\emph{,}

98 9. KALMAN FILTERING

and consequently the Kalman gain tends to

(91) \emph{K} = \emph{PCT}(\emph{DRDT})\emph{−}1

Therefore, for all practical purpose, we may as soon as the observation
interval is sufficiently large to take care of the transient behavior of
the Kalman filter use the steady-state Kalman filter
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
(92)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{d}ˆ\emph{x}\\
\emph{dt}= \emph{A}ˆ\emph{x} + \emph{K}(\emph{y − C}ˆ\emph{x})
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
where \emph{K} is constant and given by (90)-(91).
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule()
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.6667} + 6\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Example 9.2.1. Determine the steady state Kalman filter for

Σ  ˙\emph{x}1 = 2\emph{v}
\end{quote}

˙\emph{x}2 = \emph{x}1

\emph{y} = \emph{x}2 + \emph{w}

\begin{quote}
where \emph{E\{v}(\emph{t})\emph{v}(\emph{s})\emph{\}} =
\emph{E\{w}(\emph{t})\emph{w}(\emph{s})\emph{\}} = \emph{Iδ}(\emph{t −
s}) and \emph{E\{v}(\emph{t})\emph{w}(\emph{s})\emph{\}} = 0. Since
\emph{A} = , \emph{B} = , \emph{C} = {[}0\emph{,} 1{]} and \emph{D} = 1,
(\emph{A, B}) is completely 

reachable and (\emph{C, A}) is completely observable (check yourself!)
and conse-

quently the algebraic Riccati equation (90) has a unique real positive
definite {[} 0 0 {]} {[} 2 {]}
\end{quote}
\end{minipage}} &
\multirow{6}{*}{\begin{minipage}[b]{\linewidth}\raggedright
0 {]}

0
\end{minipage}} &
\multirow{6}{*}{\begin{minipage}[b]{\linewidth}\raggedright
= 0\emph{.}
\end{minipage}} \\
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.6667} + 6\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
symmetric solution\\
{[} from which we have the three nonlinear equations 0 1 0 0 {]} {[}
\emph{p}11 \emph{p}12 \emph{p}12 \emph{p}22 {]} + {[} \emph{p}11
\emph{p}12 \emph{p}12 \emph{p}22 {]} {[} 0 0 1 0 {]}\emph{−}{[}
\emph{p}11 \emph{p}12 \emph{p}12 \emph{p}22 {]} {[} 0 1 {]} {[}0 1{]}
{[} \emph{p}11 \emph{p}12 \emph{p}12 \emph{p}22 {]} + {[} 4 0

The first equation yields \emph{p}12 = \emph{±}2. From the third
equation we see that\emph{−p}2 \emph{p}11 \emph{− p}12\emph{p}22 = 0
2\emph{p}12 \emph{− p}2 12+ 4 = 0
\end{quote}

22= 0\emph{.}

\begin{quote}
only \emph{p}12 = 2 yields real solutions \emph{p}22 = \emph{±}2. Hence,
we have two solutions

\emph{P}1 = {[} 4 2 2 2 {]} \emph{,} \emph{P}2 = {[}\emph{−}4 2
\emph{−}2 2 {]} \emph{.}
\end{quote}\strut
\end{minipage}} \\
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.6667} + 6\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Here \emph{P}1 is the required positive definite solution whereas
\emph{P}2 is negative
\end{quote}
\end{minipage}} \\
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
definite. Thus the optimal gain is\\
\emph{K} =
\end{quote}\strut
\end{minipage} &
\multicolumn{3}{>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.5000} + 4\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
{[} 2 {]}\\
2\strut
\end{minipage}} \\
\multicolumn{4}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.6667} + 6\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
so the steady-state Kalman filter becomes\\
\{ ˙ˆ\emph{x}1 = 2(\emph{y −} ˆ\emph{x}2)˙ˆ\emph{x}2 = ˆ\emph{x}1 +
2(\emph{y −} ˆ\emph{x}2)\emph{.}
\end{quote}\strut
\end{minipage}} \\
\multicolumn{2}{@{}>{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3333} + 2\tabcolsep}}{%
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Writing the filtering equations on the form \emph{d}ˆ\emph{x} \emph{dt}=
{[} 0 1 \emph{−}2 \emph{−}2 {]} ˆ\emph{x} +
\end{quote}
\end{minipage}} & \begin{minipage}[b]{\linewidth}\raggedright
{[} 2 {]} 2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
\emph{y}
\end{quote}
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
9.2. THE CONTINUOUS-TIME KALMAN FILTER 99

the theory of Chapter 8, its eigenvalues being \emph{−}1 \emph{± i}. {[}
0 \emph{−}2 {]}\\
we see that the coefficient matrix is a stable matrix as required by
\end{quote}

\textbf{Index}

\begin{quote}
B.L. Ho's algorithm, 51 discrete-time, 82 BIBO-stability, 31 kernel, 6

causal, 1 Lyapunov equation centered, 82 continuous, 32 characteristic
polynomial, 56, 58, 63 discrete, 35\\
closed-loop, 59\\
control\\
Markov parameters, 41 closed-loop, 67 matrix\\
open-loop, 67 Hankel, 45\\
Cramer, 47 stability\\
discrete, 35
\end{quote}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
Dirac delta function, 93\\
duality\\
estimation-control, 92\\
reachability-observability, 27

estimator, 94

feedback, 55

Gaussian, 83\\
generalized function, 93\\
generalized stochastic processes, 93

image, 6\\
impulse response, 2\\
injective, 6\\
inner product, 83\\
innovation process, 85\\
internal, 3

Jordan, 30
\end{quote}\strut
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\begin{quote}
transition, 9\\
matrix exponential, 12\\
memoryless, 1\\
minimal, 6\\
realization, 37\\
model\\
internal, 3\\
linear, 1

normal equations, 83

observability Gramian, 25 observers, 61\\
optimal control\\
linear-quadratic, 65\\
minimum energy, 18\\
open-loop, 96

pole-placement, 56\\
projection theorem, 83
\end{quote}\strut
\end{minipage} \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{quote}
proper, 39\\
Kalman purely nondeterministic, 86 experiment, 5\\
gain, 86 range space, 6\\
Kalman decomposition, 44 reachability, 15\\
Kalman filter, 81 discrete, 22\\
Kalman gain, 67 time invariant, 19 Kalman filter reachability Gramian,
17
\end{quote}

continuous-time, 93

\begin{quote}
101

102 INDEX

reachable subspace, 20\\
realization, 37\\
standard observable, 43\\
standard reachable, 41\\
realization theory, 37\\
Riccati equation\\
algebraic, 74\\
differential, 73\\
discrete-time\\
matrix, 88\\
matrix, 66

separation principle, 62\\
SISO system, 39\\
stability, 29\\
asymptotically, 29\\
discrete, 35\\
continuous-time, 31\\
input-output, 31\\
matrix\\
discrete, 35\\
stabilizable pair, 21\\
stable\\
matrix\\
continuous-time, 31\\
state, 3\\
state space, 3\\
state-space isomorphism, 48\\
surjective, 6\\
system\\
closed-loop, 55, 56, 67, 96\\
open-loop, 56\\
time-invariant, 2
\end{quote}

\end{document}
